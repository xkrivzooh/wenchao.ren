<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>被遗忘的博客</title>
    <link>https://wenchao.ren/</link>
    <description>Recent content on 被遗忘的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 27 May 2021 13:15:30 +0000</lastBuildDate><atom:link href="https://wenchao.ren/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>践行者的法则</title>
      <link>https://wenchao.ren/posts/%E8%B7%B5%E8%A1%8C%E8%80%85%E7%9A%84%E6%B3%95%E5%88%99/</link>
      <pubDate>Thu, 27 May 2021 13:15:30 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E8%B7%B5%E8%A1%8C%E8%80%85%E7%9A%84%E6%B3%95%E5%88%99/</guid>
      <description>践行者的法则，其实就是是法则践行者所践行的法则。「法则践行者」这个词语也是我从理查德·泰普勒《极简工作法则: 如何成为领先的少数人》学习到的，本篇文章主要是整理一下相关的法则。
我们每个人其实都做着两项工作，但大多数人只意识到其中一项，即你的手头工作：完成销售目标、减少故障时长、加快完善台账，诸如此类。与之相比，另一项工作则更笼统、更模糊：让组织有效运作。当大家觉得你能够掌控大局，解决关键症结，而不是只关注自己的一亩三分地时，你就已经脱颖而出了。但是要怎么做到呢，那就是践行这些法则。
法则 法则1 知行合一、言行合一 法则2 认真思考自己在未来的职位上可能会遇到的问题，随时做好晋升的准备。 如果你没有为晋升做好准备，那么凭什么你会获得晋升的机会？即便给了你晋升的机会，那么你如何保证自己一定会晋升呢？
同时你要自觉地思考当前工作的方方面面并做出改变。你需要改善以下两点：
 你做事的方式； 别人对你工作的认知  法则3 学会把事情做对，但是更要弄清楚哪些是对的事情。 首先需要明确，「做对的事情」比「把事情做对」要重要的更多。「把事情做对」这是一个能力问题，而能力问题是可以通过时间、学习来提升的。而「做对的事情」更多的是关于行动的目标、努力的方向是否是正确的，在做事情之前先确认它是一个对的事情。
此法则需要跳出组织的局限，放眼全局，要了解外面世界的需求，了解它们的变化，认真思考组织该如何运作，才能继续生存繁荣。
所以什么是对的事情呢？我觉的可以从下面2个方向来考虑：
 我们每个人其实都做着两项工作，但大多数人只意识到其中一项，即你的手头工作：完成销售目标、减少故障时长、加快完善台账，诸如此类。与之相比，另一项工作则更笼统、更模糊：让组织有效运作。当大家觉得你能够掌控大局，解决关键症结，而不是只关注自己的一亩三分地时，你就已经脱颖而出了。
 法则4 你的一切细节都会被审视。如果你想往上走，那你就必须「看上去就是对的那一型」 单单工作优秀、经验丰富远远不够，事实上你的一切细节都会被审视，无论是你的衣着、谈吐、举止、怎样和同事交谈、怎样在会议上发言都需要有所讲究。
法则5 先人一步 先人一步就是要提前做好准备，比别人多想一步，比别人多准备一些等。
法则6 别让人家看到你的努力 </description>
    </item>
    
    <item>
      <title>关于Spring-statemachine的一些看法</title>
      <link>https://wenchao.ren/posts/%E5%85%B3%E4%BA%8Espring-statemachine%E7%9A%84%E4%B8%80%E4%BA%9B%E7%9C%8B%E6%B3%95/</link>
      <pubDate>Tue, 11 May 2021 14:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%85%B3%E4%BA%8Espring-statemachine%E7%9A%84%E4%B8%80%E4%BA%9B%E7%9C%8B%E6%B3%95/</guid>
      <description> 我们需要有状态的状态机么？
 昨天和今天再一次翻看了一下Spring-Statemachine项目的最新进展, 上一次看这个项目的文档还是几年前。 之所以之前关注这个项目主要有2个原因吧，第一个就是spring的project一般比较有质量保证，第二个是状态机本身是一个用途非常广泛的架构设计。所以这个项目一直让我 惦记着。
然后昨天翻看完了一下Spring-Statemachine的文档，同时今天写了一个demo完整的模拟了一下在交易系统平台的业务背景下使用它来实现状态机。然后给我的感触就是这玩意真的不接地气。主要有下面几个原因:
 Spring-Statemachine仅仅支持「有状态的状态机」，而不支持「无状态的状态机」。  这一点是我放弃将Spring-Statemachine引入我们现有的一个工程来重构代码的最主要原因。表面上来看，状态机理所当然是应该维持状态的。但是深入想一下，这种状态性并不是必须的，因为有状态，状态机的实例就不是线程安全的，而我们的应用服务器是分布式多线程的，所以在每一次状态机在接受请求的时候，都不得不重新 build 一个新的状态机实例。这种new instance per request 的做法，倘若状态机的构建很复杂，QPS 又很高的话，肯定会遇到性能问题。所以使得我产生了一个疑问，我们真的需要有状态的状态机么？   Spring-Statemachine本身支持的功能很多，但是这些工程我们基本不会用到。  支持的功能多，很多时候并不是一个优势。因为功能越多，使得框架本身的复杂度，学习成本也会上升。同时如果这些功能我们在未来可能会用到也还好，问题是用不到，比如它支持的UML Eclipse Papyrus modeling， Distributed state machine based on a Zookeeper等。感觉这类开源项目为了功能完备而把UML State Machine 上罗列的功能点都实现了。但是问题是很多都用不到。尤其是在互联网业务下，绝大多数时候状态的并行（parallel，fork，join）、子状态机都用不到，或者一般也会采样其他的方式来实现。   Spring-Statemachine的API逐渐往reactive方向靠拢，之前的传统API都开始标记过期，使得组内同学的学习成本会进一步提升。  基于这3个原因，我最终放弃了对Spring-Statemachine的关注。我们自己实现了一个功能简单但是完备的状态机。
参考  给 DSL 开个脑洞：无状态的状态机 高德打车通用可编排订单状态机引擎设计 基于有限状态机与消息队列的三方支付系统补单实践 状态机在马蜂窝机票订单交易系统中的应用与优化实践  </description>
    </item>
    
    <item>
      <title>状态机中的状态变更和消息通知的一致性常见解决办法</title>
      <link>https://wenchao.ren/posts/%E7%8A%B6%E6%80%81%E6%9C%BA%E4%B8%AD%E7%9A%84%E7%8A%B6%E6%80%81%E5%8F%98%E6%9B%B4%E5%92%8C%E6%B6%88%E6%81%AF%E9%80%9A%E7%9F%A5%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E5%B8%B8%E8%A7%81%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</link>
      <pubDate>Mon, 10 May 2021 14:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/%E7%8A%B6%E6%80%81%E6%9C%BA%E4%B8%AD%E7%9A%84%E7%8A%B6%E6%80%81%E5%8F%98%E6%9B%B4%E5%92%8C%E6%B6%88%E6%81%AF%E9%80%9A%E7%9F%A5%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E5%B8%B8%E8%A7%81%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</guid>
      <description>一般在订单或者交易系统中，经常会使用到状态机来解决单据的状态流转问题。同时会设计到数据库的状态变更以及事件通知功能。此时就需要有一种机制来保证状态变更的行为和事件通知行为的一致性，也就是状态发生变更，事件必须通知出去。针对这种场景，一般我们有如下的几种做法:
借助MQ的事务消息 使用rocketmq等支持的两阶段式消息提交方式  先向消息服务器发送一条预处理消息 当本地数据库变更提交之后、再向消息服务器发送一条确认发送的消息 如果本地数据库变更失败、则向消息服务器发送一条取消发送的消息 如果长时间没有向消息服务器发生确认发送的消息，消息系统则会回调一个提前约定的接口、来查看本地业务是否成功，以此决定是否真正发生消息  使用数据库事务方案保证 之所以把这种方式也归类到借助MQ的事务消息里面，主要是因为不少MQ也是基于这种方式实现的事务消息。比如Qunar公司之前开源的QMQ。他的原理就是：
 在业务数据库中创建一个message表，然后利用「对于MySQL中同一个实例里面的db，如果共享相同的Connection的话是可以在同一个事务里的」这一机制，在同一个事务内，执行业务操作，并将消息插入message表然后进行事务提交 向消息服务器发送消息 发送成功则删除掉当前表记录 对于没有发送成功的消息（也就是表里面没有被删除的记录），再由定时任务来轮询发送  使用数据库事务+定时任务轮询  创建一个消息发送表，将要发送的消息插入到该表中，同本地业务在一个数据库事务中进行提交 之后在由一个定时任务来轮询发送、直到发送成功后在删除当前表记录  数据对账 数据对账发现不一致时进行补偿处理、以此保证数据的最终一致。其实不管使用哪种方案来保证数据库状态变更和消息的一致，数据对账的方案都是&amp;quot;必须&amp;quot;要有的一种兜底方案。</description>
    </item>
    
    <item>
      <title>AviatorScript ClassDefiner源码解析</title>
      <link>https://wenchao.ren/posts/aviatorscript-classdefiner%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Sat, 08 May 2021 14:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/aviatorscript-classdefiner%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</guid>
      <description>在AviatorScript的内部实现机制中，需要将AviatorScript脚本翻译为可执行的java代码，所以会设计到「类的动态生成」和「类的加载」这2个功能。 而其中关于「类的加载」这部分的功能是在com.googlecode.aviator.code.asm.ClassDefiner中实现的, 在ASMCodeGenerator中通过下面的方式来使用：
Class&amp;lt;?&amp;gt; defineClass = ClassDefiner.defineClass(this.className, Expression.class, bytes, this.classLoader);  而这个defineClass方法的定义如下：
public static final Class&amp;lt;?&amp;gt; defineClass(final String className, final Class&amp;lt;?&amp;gt; clazz, final byte[] bytes, final AviatorClassLoader classLoader) throws NoSuchFieldException, IllegalAccessException { if (!preferClassLoader &amp;amp;&amp;amp; DEFINE_CLASS_HANDLE != null) { try { Class&amp;lt;?&amp;gt; defineClass = (Class&amp;lt;?&amp;gt;) DEFINE_CLASS_HANDLE.invokeExact(clazz, bytes, EMPTY_OBJS); return defineClass; } catch (Throwable e) { // fallback to class loader mode. if (errorTimes++ &amp;gt; 10000) { preferClassLoader = true; } return defineClassByClassLoader(className, bytes, classLoader); } } else { return defineClassByClassLoader(className, bytes, classLoader); } }  因为上下文中preferClassLoader的定义是：</description>
    </item>
    
    <item>
      <title>AviatorScript编译执行流程</title>
      <link>https://wenchao.ren/posts/aviatorscript%E7%BC%96%E8%AF%91%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/</link>
      <pubDate>Fri, 07 May 2021 14:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/aviatorscript%E7%BC%96%E8%AF%91%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/</guid>
      <description>本篇文章通过AviatorScript工程自带的一个示例，来简单说明一下AviatorScript的执行流程:
 初始化Aviator的核心数据结构 读取AviatorScript脚本内容，做语法树解析，并通过ASM翻译为java字节码，然后通过classLoader做类加载，构建Expression实例。 通过触发Exception#execute方法来触发脚本执行。  示例程序 本部分继续以下面的示例来说明，这个实例在AviatorScript的工程中可以找到:
public class RunScriptExample { public static void main(final String[] args) throws Exception { // Enable java method invocation by reflection. AviatorEvaluator.getInstance() .setFunctionMissing(JavaMethodReflectionFunctionMissing.getInstance()); // You can trry to test every script in examples folder by changing the file name. Expression exp = AviatorEvaluator.getInstance().compileScript(&amp;quot;examples/hello.av&amp;quot;); exp.execute(); } }  在这个实例程序中，AviatorEvaluator.getInstance()是单例模式的一种实现，用来获取AviatorEvaluatorInstance实例。
AviatorEvaluatorInstance初始化流程 在AviatorEvaluator中通过单例模式创建了AviatorEvaluatorInstance的实例。
public static AviatorEvaluatorInstance newInstance() { return new AviatorEvaluatorInstance(); } private static class StaticHolder { private static AviatorEvaluatorInstance INSTANCE = new AviatorEvaluatorInstance(); }  下面是AviatorEvaluatorInstance的构造函数：</description>
    </item>
    
    <item>
      <title>hugo新写的文章展示不出来</title>
      <link>https://wenchao.ren/posts/hugo%E6%96%B0%E5%86%99%E7%9A%84%E6%96%87%E7%AB%A0%E5%B1%95%E7%A4%BA%E4%B8%8D%E5%87%BA%E6%9D%A5/</link>
      <pubDate>Fri, 23 Apr 2021 14:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/hugo%E6%96%B0%E5%86%99%E7%9A%84%E6%96%87%E7%AB%A0%E5%B1%95%E7%A4%BA%E4%B8%8D%E5%87%BA%E6%9D%A5/</guid>
      <description>最近因为在公司的笔记本上写博文，所以并不打算采用在terminal中使用hugo new的方式来生成新的博文模板，而是自己在vs code中 编写好之后然后手动在github的网页新增文件触发github actions的执行然后部署到vps上。
遇到的问题 然后发现自己新写的文章居然展示不出来。排查了一下博文前面的头信息：
--- title: &amp;quot;hugo新写的文章展示不出来&amp;quot; date: 2021-04-23T11:11:09+08:00 draft: false tags: [&#39;杂谈&#39;] ---  发现这些头信息的格式写的并没有问题，即便这个时间也没有任何的问题。
解决过程 后来在网上搜到了这个文章：Hugo Post Missing (Hugo 博客文章缺失问题)。文章中提到了：
 Hugo 是否会渲染一篇博文依赖该文章的发布时间。如果一个博文的发布时间比 Hugo 构建当前站点的时间还要晚，也就是 Hugo 认为博文的发布时间在未来，就不会渲染该篇博文。前面没有写时区的博文，就是被 Hugo 认为发布时间还未到，所以没有渲染出来。
 解决办法  第一种最简单的办法是修改文章头信息中的date时间为过去的时间 第二种就是强制Hugo渲染发布时间在未来的博文，这有两种办法：  第一个是在config.toml中加入以下设置：buildFuture = true 第二个是在hugo build博客的时候，加上 --buildFuture 选项    </description>
    </item>
    
    <item>
      <title>DMAIC模型</title>
      <link>https://wenchao.ren/posts/dmaic%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Fri, 23 Apr 2021 11:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/dmaic%E6%A8%A1%E5%9E%8B/</guid>
      <description>这篇文章主要说一下DMAIC模型。
参考资料  MBA智库-DMAIC模型  什么是DMAIC模型？ DMAIC模型是实施6sigma的一套操作方法，通用电气公司总结了众多公司实施6sigma的经验,系统地提出了实施6sigma的DMAIC模型。DMAIC模型现在被广泛认可,认为这是实施6sigma更具操作性的模式，主要侧重在已有流程的质量改善方面。它的模型图如下：
需要强调的是：DMAIC模型的应用是一个循环过程。关于DMAIC模型的实际应用，在上面推荐的资料中，有对应的案例。
DMAIC模型每一项的说明 D(Define)——界定 界定是识别客户要求，确定影响客户满意度的关键因素，找准要解决的问题，比如：
 你们正在做什么？ 为什么要解决这个特别的问题？ 你们的顾客是谁？ 你们的顾客需求是什么？ 你们过去是怎样做这项工作的？ 现在改进这些工作将获得什么益处？ 为什么选择这个案例？ 要特别解决的问题是什么？ 解决这个问题的限制条件是什么？ 解决这个问题涉及的范围有多大？ 团队成员及其职责是什么？ DMAIC的各阶段的时间安排。  M(Measure) ——测量 量测是校准Y的测量系统，收集整理数据，为量化分析做好准备。通过量测使得量化管理成为可能，有了量测才使统计技术与方法的应用成为可能。为了获取真实、准确、可靠的数据，需要对量测的系统进行校准。数据收集还要求掌握一些数据收集的方法，如抽样技术、检查单检查表方法等。
典型量测举例
 如果客户对供货时间不满意，你就需要收集过去若干次收到订单到发出货物的天数。 如一个大型生产制造商过去一个月在订单处理方面的DPMO是253 000 如一个冰箱制造商过去一年的包装清单的DPMO 是85 000 如一个大型商场过去一个月开发票的DPMO是67 000 如一个餐馆过去一个月账单的DPMO是57 000  A(Analyze)——分析 分析是运用多种统计技术方法找出存在问题的根本原因。影响产品质量和顾客满意度的因素很多，运用统计方法可找出影响顾客满意度的主要原因。
常用统计分析工具：
 直方图 排列图 鱼骨图 散点图 控制图  I(Improve)——改进 改进是实现目标的关键步骤。
改进是确定影响y的主要原因x，寻求x与y的关系，建立x的允许变动范围。结果与原因呈现出一个类似函数的模型，即：y=f(x1,x2,&amp;hellip;&amp;hellip;xp)+ε。其中y是因变量，x是自变量向量，是p个原因，称为自变量。ε是随机干扰项，也正是由于有这一项，此模型才真切地刻画出y与x有着密切的关联，但y又不能由x完全确定的这种奇特关系。模型就是一个系统，y是不可控制的随机变量，由系统产出的，也称为内生变量。x是一些可控制的确定性变量，也称为外生变量。ε是不可控的随机变量。如果x与y描述广告投入与销售量的关系，显然销售量y是不可控的，广告费用x是可控制的，对销售量y有影响的一些其它因素就是ε。
相关分析、回归分析、试验设计、方差分析等都是改进步骤中的统计工具。当用统计方法找到了要改进的环节和方案之后，重要的是去实施它。这一过程中的困难往往是员工长期的习惯不会轻易转变。假如公司欲在各部门之间和部门内部跟踪节约资金，这时就要将实际花费与预算联系起来，或跟踪净节约资金、项目范围、项目结束时间等变量。通过正确跟踪数据，建立起回归模型，用回归模型进行预测和控制，使公司收益和顾客满意度达到最大。 改进这一步骤是实现目标的关键，它类似于六步法中的“优化你的工作流程”。
C(Control)——控制 控制是将主要变量的偏差控制在许可范围。
对流程进行一定的改进之后，下来的问题就是坚持避免“突然”回到旧的习惯和流程是控制的主要目的。6σ项目的成功依赖于那些始终坚持如一的人，控制过程中，流程中的每个环节的每个人都必须要有工作描述。没有工作描述和过程程序,就谈不上控制，任何流程的初期阶段都是至关重要的。在生产周期中纠正缺陷的成本:
 在初期发现一个缺陷，花费1美元 在设计中发现一个缺陷，花费10美元 在实验中发现一个缺陷，花费100美元 在现场发现一个缺陷，花费1000美元  </description>
    </item>
    
    <item>
      <title>java描述符</title>
      <link>https://wenchao.ren/posts/java%E6%8F%8F%E8%BF%B0%E7%AC%A6/</link>
      <pubDate>Thu, 22 Apr 2021 14:12:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/java%E6%8F%8F%E8%BF%B0%E7%AC%A6/</guid>
      <description>类型描述符 基元类型的描述符是单个字符:Z 表示 boolean，C 表示 char，B 表示 byte，S 表示 short， I 表示 int，F 表示 float，J 表示 long，D 表示 double。一个类类型的描述符是这个类的 内部名，前面加上字符 L，后面跟有一个分号。例如，String 的类型描述符为 Ljava/lang/String;。而一个数组类型的描述符是一个方括号后面跟有该数组元素类型的描述符。
   Java类型 类型描述符     boolean Z   char C   byte B   short S   int I   float F   long J   double D   Object Ljava/lang/Object;   int[] [I   Object[][] [[Ljava/lang/Object;   String Ljava/lang/String;    方法描述符 方法描述符是一个类型描述符列表，它用一个字符串描述一个方法的参数类型和返回类型。 方法描述符以左括号开头，然后是每个形参的类型描述符，然后是一个右括号，接下来是返回类 型的类型描述符，如果该方法返回 void，则是 V(方法描述符中不包含方法的名字或参数名)。</description>
    </item>
    
    <item>
      <title>SWOT分析模型</title>
      <link>https://wenchao.ren/posts/swot%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Thu, 22 Apr 2021 14:12:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/swot%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/</guid>
      <description>参考资料  MBA智库-SWOT分析模型 MBA智库-高级SWOT分析法  SWOT分析模型简介 SWOT分析法（也称TOWS分析法、道斯矩阵）即态势分析法，20世纪80年代初由美国旧金山大学的管理学教授韦里克提出，经常被用于企业战略制定、竞争对手分析等场合。 主要包括分析企业的优势（Strengths）、劣势（Weaknesses）、机会（Opportunities）和威胁（Threats）。因此，SWOT分析实际上是将对企业内外部条件各方面内容进行综合和概括，进而分析组织的优劣势、面临的机会和威胁的一种方法。通过SWOT分析，可以帮助企业把资源和行动聚集在自己的强项和有最多机会的地方；并让企业的战略变得明朗。
SWOT模型含义介绍 优劣势分析主要是着眼于企业自身的实力及其与竞争对手的比较，而机会和威胁分析将注意力放在外部环境的变化及对企业的可能影响上 。在分析时，应把所有的内部因素（即优劣势）集中在一起，然后用外部的力量来对这些因素进行评估。
使用实例 举一个科尔尼SWOT分析得出战略的例子：
SWOT模型的局限性 与很多其他的战略模型一样，SWOT模型已由麦肯锡提出很久了，带有时代的局限性。以前的企业可能比较关注成本、质量，现在的企业可能更强调组织流程。例如以前的电动打字机被印表机取代，该怎么转型？是应该做印表机还是其他与机电有关的产品？从SWOT分析来看，电动打字机厂商优势在机电，但是发展印表机又显得比较有机会。结果有的朝印表机发展，死得很惨；有的朝剃须刀生产发展很成功。这就要看，你要的是以机会为主的成长策略，还是要以能力为主的成长策略。SWOT没有考虑到企业改变现状的主动性，企业是可以通过寻找新的资源来创造企业所需要的优势，从而达到过去无法达成的战略目标。
因此又有了更高级的POWER SWOT分析法。
POWER SWOT分析法 POWER是个人经验（Personal experience）、规则（Order）、比重（Weighting）、重视细节（Emphasize detail）、权重排列（Rank and prioritize）的首字母缩写，这就是所谓的高级SWOT分析法。
个人经验(Personal experience) 作为市场营销经理，你是如何运用SWOT分析法的呢?无非是将你的经验、技巧、知识、态度与信念结合起来。你的洞察力与自觉力将会对SWOT分析法产生影响。
规则(Order) 优势或劣势，机会或威胁，市场营销经理经常会不由自主地把机会与优势、劣势与威胁的顺序搞混。这是因为内在优势与劣势和外在机会与威胁之间的分界线很难鉴定。举个例子，就说全球气温变暖与气温变化好了，人们会错将环境保护主义当作一种威胁而非潜在的机会。
加权(weighting) 通常人们不会将SWOT分析法所包含的各种要素进行加权。某些要素肯定会比其他的要素更具争议性，因此你需要将所有的要素进行加权从而辨别出轻重缓急。你可以采用百分比的方法，比如威胁A=10％，威胁B=70％，威胁C=20％(总威胁为100％)。
重视细节(Emphasize detail) SWOT分析法通常会忽略细节、推理和判断。人们想要寻找的往往是分析列表里面的几个单词而已，重视细节将极大地帮你决定如何最佳地评价与比较各种要素。
等级与优先(Rank and prioritize) 一旦添加了细节并评价了要素，你便能够进入下一个步骤，即给SWOT分析法一些战略意义，例如你可以开始选择那些能够对你的营销策略产生最重要影响的要素。你将它们按照从高到低的词序进行排列，然后优先考虑那些排名最靠前的要素。比如说机会C=60％，机会A=25％，机会B=10‰那么你的营销计划就得首先着眼于机会C，然后是机会A，最后才是机会B。
POWER STOP分析示例 举一例说明高级SWOT分析是如何进行的。某公司原来是做餐饮经营的，现在发现保健品市场是个很好投资机会，打算代理某一知名保健品在公司所在大区的独家代理。该公司做出了如下所示的较详细的SWOT分析。
每个被分析项的分值和权重是如何确定的呢？总分又是怎样确定的呢？其实，确定每个被评判内容的分值和四大分析项目（SWOT）的分值并不是完全凭空想象出来的。这主要基于该行业竞争对手长期从事该行业经营和管理而形成的每项评判内容的重要性而设定的。但是，值得说明的是，每项分析内容的权重和分数是不同的。</description>
    </item>
    
    <item>
      <title>重视日报和周报</title>
      <link>https://wenchao.ren/posts/%E9%87%8D%E8%A7%86%E6%97%A5%E6%8A%A5%E5%92%8C%E5%91%A8%E6%8A%A5/</link>
      <pubDate>Thu, 22 Apr 2021 14:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/%E9%87%8D%E8%A7%86%E6%97%A5%E6%8A%A5%E5%92%8C%E5%91%A8%E6%8A%A5/</guid>
      <description>如果你们公司、项目组要求你们写日报或者周报，请不要抱怨，而且说实话大多数时候你抱怨了也没啥用，该写还得写。 所以转变一下思路，理性对待日报和周报。从我这些年的工作经验来看：「日报和周报其实是一个非常不错和有效的上下级沟通的工具，并且也是下级向上管理的一个不错的渠道」，所以建议认真对待日报和周报，不要把周报写成流水账之类的完全没有任何营养的文章。
一般日报和周报都是三部分构成：
 第一部分：上周工作总结，项目进度 第二部分：下周的工作安排 第三部分：本周工作反思、问题总结等  大多数同学都会「完成任务试」的着重写第一部分，但是其实第一部分，我个人感觉反而是最不重要的，如果TL知道你在做什么，你甚至可以一笔带过，直接给出：「本周xxx进展顺利，在计划内」就完事了。应该将精力用在第二部分和第三部分上，第二部分可以让你锦上添花，给人一种做事有规划的感觉，第三部分会让你脱颖而出，这部分也是上级最喜欢看的地方，可以通过这部分来让上级了解你的想法，也是你和上级沟通，向上管理的绝佳位置。
关于周报相关的逻辑，推荐下么的两篇文章，写的很不错。
推荐资料  周报的逻辑（转） 周报的逻辑·续（转）  </description>
    </item>
    
    <item>
      <title>Error Prone的介绍和使用</title>
      <link>https://wenchao.ren/posts/error-prone%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Mon, 12 Apr 2021 15:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/error-prone%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%92%8C%E4%BD%BF%E7%94%A8/</guid>
      <description>Error Prone的介绍和使用
今天在浏览GitHub的时候突然看到Google有一个项目Error Prone:
 Catch common Java mistakes as compile-time errors
 官方网站：https://errorprone.info/
Error Prone是什么？ 然后就翻看了一下官方文档了解了一下具体的作用。如果你之前没有了解过这个Error Prone，你可以把他对标为Snoar，FindBugs之类的静态代码分析工具就行。Error Prone就是Google研发的静态代码扫描工具，顾名思义，就是用于扫描Java各种易于出错的代码。
Error Prone产生的背景 Google在这篇论文Lessons from Building Static Analysis Tools at Google
 PDF Lessons from Building Static Analysis Tools at Google HTML Lessons from Building Static Analysis Tools at Google  中也提到了Google之前是如何做静态代码分析，以及Error Prone产生的一些原因。简单整理一下就是：
 Google内部早期使用FindBugs作为自己的静态代码分析工具，然后FindBug这种工具一般是定时触发重发，然后生成一个仪表盘，但是问题是大家一般都不会主动去翻看这个仪表盘，因为这个仪表盘不会在开发人员的常规工作流程中，于是Google内部发起了一个FixIt的小活动，简单说就是来集中几天解决这些发现的代码问题，后来他们将FindBugs等分析工具和Code Review工作结合起来搞了一个小平台，但是当时的内部整合不太好，使得开发人员觉的难用，也不怎么信任这个整合平台了。
 于是了，Google内部觉的，这种静态代码分析工作应该是参与到持续继承中，而且最好要快速，频繁，参与到开发人员的常规工作中，并且最好不但要发现问题，还要告诉开发人员怎么修。于是了，Google参考了之前Clang的一个经验，将静态代码分析工作整合到了javac，也就是在编译期来做静态代码分析工作，于是后来就有了这篇文章提到的Error Prone。
Error Prone的基本使用 我工作中一直使用的是Intellij+JDK8+Maven，所以参考官方文档https://errorprone.info/docs/installation，写了一个JDK8使用的小例子。下面贴一下核心代码。
首先是pom.xml中的配置，核心在于maven-compiler-plugin插件的配置, 同时因为大家项目开发过程中一般会使用Lombok等annotation processor依赖，所以下面的实例代码中也展示了ErrorProne如何和Lombok等依赖整合。
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt; &amp;lt;project xmlns=&amp;quot;http://maven.apache.org/POM/4.0.0&amp;quot; xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; xsi:schemaLocation=&amp;quot;http://maven.apache.org/POM/4.0.0 https://maven.</description>
    </item>
    
    <item>
      <title>Mac中安全地使用rm命令</title>
      <link>https://wenchao.ren/posts/mac%E4%B8%AD%E5%AE%89%E5%85%A8%E5%9C%B0%E4%BD%BF%E7%94%A8rm%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Mon, 12 Apr 2021 14:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/mac%E4%B8%AD%E5%AE%89%E5%85%A8%E5%9C%B0%E4%BD%BF%E7%94%A8rm%E5%91%BD%E4%BB%A4/</guid>
      <description>之前好几次工作中不小心在mac的terminal中执行了rm -rf命令来删除文件然后发现需要恢复的情况。于是找了一下发现果然有前人遇到了 同样的问题并且给出了优雅的解决方案。
 trash: CLI tool that moves files or folder to the trash
 trash命令可以实现将文件(夹)移入废纸篓, 并且支持指定使用-F指定使用Finder来删除文件(这种方式支持放回原处操作)
$ brew install trash $ trash -F [file-name]  因为我是使用zsh的，所以我添加下面的alias命令到我的.zshrc文件中
alias rm=&amp;lsquo;trash -F&amp;rsquo;
然后就可以在终端中正常的使用rm xxx或者rm -rf xx，操作后，就会发现被删除的文件存在mac的废纸篓中。可以还原。</description>
    </item>
    
    <item>
      <title>为什么知道了这么多道理却依旧过不好这一生</title>
      <link>https://wenchao.ren/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%9F%A5%E9%81%93%E4%BA%86%E8%BF%99%E4%B9%88%E5%A4%9A%E9%81%93%E7%90%86%E7%BC%BA%E4%BE%9D%E6%97%A7%E8%BF%87%E4%B8%8D%E5%A5%BD%E8%BF%99%E4%B8%80%E7%94%9F/</link>
      <pubDate>Tue, 01 Dec 2020 00:08:59 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%B8%BA%E4%BB%80%E4%B9%88%E7%9F%A5%E9%81%93%E4%BA%86%E8%BF%99%E4%B9%88%E5%A4%9A%E9%81%93%E7%90%86%E7%BC%BA%E4%BE%9D%E6%97%A7%E8%BF%87%E4%B8%8D%E5%A5%BD%E8%BF%99%E4%B8%80%E7%94%9F/</guid>
      <description>为什么知道了这么多道理，却依旧过不好这一生?
 这句话是网上一直比较流行的一句话，知乎和微博上也有很多人发出过这样的疑问，我曾经也翻看过关于这个问题的一些回答，之前有过一些零散的 想法，最近在加入美团之后，结合自己的所见所闻，在这里完整的梳理一下自己关于这个问题的思路吧。
关于这个问题，我觉的答案是下面3个，并且优先级从高到底（或者说从上往下）应该是如下的：
 努力的目标是否合理且正确？ 是否长期坚持知行合一？ 是否自己摄入的知识还是太少，有没有持续学习？  下面简单分开说一下吧。
努力的目标是否合理且正确 这一点其实包含好几层意思，首先需要有足够清晰的目标，并且目标是可以完成的，目标可以定的高一些，但是是努力一下有机会可以实现的，而不是 太高的目标以至于基本不可能实现的目标。另外这个目标要正确，这个正确怎么理解呢，首先应该是积极的向上的，不能是消极的。比如「想赚1000万」就是一个积极向上的目标，而像「想成为北京烟瘾最大的人」其实就是一个消极的目标。其次这个目标最好可量化，可量化的好处就是可以直观的检查我们的进度。比如「想成为有钱人」这个目标就没有量化，而「想赚1000万」就是一个可量化的指标。如果再有时间观念就更好了，比如「再5年内赚1000万」就是一个有时间观念并且积极的可量化的，并且有时效性的目标。
长期坚持的知行合一 美团其实是一家很强调「方法论」的公司，但是我觉的比起强调方法论来说，它本质其实更加强调「知行合一」。他只是通过强调「方法论」来一定程度上约束员工践行「知行合一」这个理念。这一点无论是从员工的入职手册上，还是学城（美团内部WIKI），平时的PRD的目录大纲，项目复盘的目录大纲，王兴等高管的讲话都可以看出这一点。
这一点也是我曾经呆过的几家公司没有沉淀下来的东西。比如之前在Qunar和便利蜂，公司的文化都很讲究务实和效率，这一点其实是CTO吴永强的风格。虽然我详细吴永强他个人一定有自己的一套做事方法论，而且他本人也确实践行的非常好，也影响了他身边的非常多的人，当然这一点也影响了我，毕竟我在两家公司加起来也干了快7年了。但是他并没有将这一套方法论沉淀下来，形成一种书面的，也可以普世（或者说适合公司内）传播的文档，然后公司层面通过这些文档来要求大家严格按照这些方法论来。
反而美团我个人感觉在这一点上做的非常的好。它强调「基本功」，因为这是公司发展的现阶段所要求的。为什么之前Qunar期间他的「野路子」可以work的很好呢，是因为那段时间业务高速发展，大家属于「抢地盘」试或者说类似于「战争时期」，他那么干是为了效率，所以他这套东西是可以很好的运转起来，但是当进入存量时代时，类似于战争过后需要休养生息一样，战争期间的那一套机制就不是非常的合适了，这个时候就需要另外一套机制来搞了，这也就是为什么美团在这个阶段一直在强调的「注重基本功」的原因。
持续学习 人和人的智商我赞同是有非常大的差距的，我也认同人和人的智商差距有时候比人和狗的差距还大。但是至少在我工作和学习的这些年中，我还没有 完全遇到那种可以100%无死角碾压我的人，或者说不需要100%无死角碾压，就是那种完全让我感觉我即便在怎么努力我也无法超越的人，至少我身边 是没有的。所以我想说的是，后天的努力对于绝大多数人来说，只要你够努力，还是追的上绝大多数人的，以大多数人的努力程度，还是轮不到拼天赋的。
最后，希望我可以做到这3点。</description>
    </item>
    
    <item>
      <title>Mac登录工作居住证官网</title>
      <link>https://wenchao.ren/posts/mac%E7%99%BB%E5%BD%95%E5%B7%A5%E4%BD%9C%E5%B1%85%E4%BD%8F%E8%AF%81%E5%AE%98%E7%BD%91/</link>
      <pubDate>Tue, 17 Nov 2020 23:43:58 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/mac%E7%99%BB%E5%BD%95%E5%B7%A5%E4%BD%9C%E5%B1%85%E4%BD%8F%E8%AF%81%E5%AE%98%E7%BD%91/</guid>
      <description>曾经我以为在Mac平台下是无法正常进入ZF的工作居住证网站的，因为虽然正常登录了以后，但是会出现乱码： 后来无意发现网上的一个解决办法就是，正常使用浏览器（随便一个）登录工作居住证官网后，然后在新标签页打开下面的链接: http://219.232.200.39/uamsso/SSOSecService?sid=e10adc3949ba59abbe56e057f20f883e&amp;amp;LinkType=online&amp;amp;LinkID=666
这样就可以正常的进入内部页面了，其他的一些关于工作居住证的操作都是OK的。真的是一个神奇的网站。在此感谢发现这个解决方案的同学。</description>
    </item>
    
    <item>
      <title>Best Wishes to Me</title>
      <link>https://wenchao.ren/posts/best-wishes-to-me/</link>
      <pubDate>Fri, 06 Nov 2020 14:11:09 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/best-wishes-to-me/</guid>
      <description>今天（2021年11月6日），是我在便利蜂工作的第939天，同时也是我在便利蜂的Last day。
虽然现在人的寿命越来越长，但是至少在我这一代来说，人的寿命满打满算下来也差不多是3万天(30000/365=82岁)左右。如果按照职业生涯来说，时间就更短了，我是13年7月本科毕业，当时我是22岁，很多人都说程序员这个行业是青春饭，35岁是一个坎，那么假设我的工作生涯很不幸运，在我35岁的时候被所谓的「优化」了，那么算下来我的职业生涯有13年。因此在便利蜂的这939天，其实是一段非常长的一个时间。只是很可惜，就这样吧，这段工作经历就到这里结束吧。
在这段工作期间，我努力的迫使自己去尝试改变自己之前在Qunar期间发现的自己的问题，有一些问题已经慢慢的有所改善，比如沟通能力，但是还是有不少问题没有太大的变化，比如缺乏耐心，不细心，表达能力等。同时在这段期间我也努力去争取达成我从Qunar辞职之前对自己下一份工作所定下的期望目标，只是很可惜，最终未能如愿，虽然我努力了，但是在公司的发展洪流中，个人的努力所能起的影响实在是太小了，最终我以一个失败者的姿态退出了这片战场，同时也没有收到自己直属TL的祝福，虽然收到了同事，TL&amp;rsquo;s TL的以及TL&amp;rsquo;s TL&amp;rsquo;s TL（CTO）的祝福（也可能他们只是例行公事般的询问和祝福一下罢了），但是毕竟他们距离我较远，所以从这一点说，我确实是一个Loser。
我不打算去说因为便利蜂的xxx原因使得我决定辞职，因为那些东西在我辞职的那一刻对我来说已经不重要了，重要的是我期望我在我剩下的这五六年内，通过自己的努力，所要取得的「成绩」是什么，以及为了取得这些成绩，我应该去做些什么？不过这些东西很多并不适合明确的公开写出来，所以也就不在这里书写了。
虽然因为我的辞职，曾经的期权已经作废，但是我依旧会给便利蜂送去我的祝福，希望它发展的越来越好！至少这样可以证明我当年放弃Qunar和携程的部分股票，加入便利蜂的决定并不是那么的错误。
或许若干年后便利蜂上市一飞冲天，我可能会留下悔恨的泪水，虽然我也不确定接下来的这份工作能具体干多久，是否能取得我预期的目标结果，但是我依旧会用我的热情和努力，去争取！
我其实一直对自己还算是有比较清晰的认识，就如同我在自我介绍这里面说的一样：
 我其实是一个非常平庸并且慢热的人！
 其实我不光平庸，慢热，如果再加一句的话：我的运气其实相对也一般般。那么在这个前提下，我对自己也只能说一句：Best Wishes to Me！</description>
    </item>
    
    <item>
      <title>Java脚本的语法解析示例</title>
      <link>https://wenchao.ren/posts/java%E8%84%9A%E6%9C%AC%E7%9A%84%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E7%A4%BA%E4%BE%8B/</link>
      <pubDate>Thu, 05 Nov 2020 15:36:58 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/java%E8%84%9A%E6%9C%AC%E7%9A%84%E8%AF%AD%E6%B3%95%E8%A7%A3%E6%9E%90%E7%A4%BA%E4%BE%8B/</guid>
      <description>早期搞了一个解析Java脚本的功能，其中有一处是需要解析Java语法，下面的代码贴了一下主要功能, 主要用到了tools中的一些类，但是代码使用tools.jar需要在maven中做一些额外配置：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.sun&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;tools&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.8&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;system&amp;lt;/scope&amp;gt; &amp;lt;systemPath&amp;gt;${JAVA_HOME}/lib/tools.jar&amp;lt;/systemPath&amp;gt; &amp;lt;/dependency&amp;gt;  上面的maven配置就是为了能够在代码中使用tools.jar中的类。
import com.google.common.base.Charsets; import com.google.common.base.Joiner; import com.google.common.base.Preconditions; import com.google.common.base.Strings; import com.google.common.collect.Maps; import com.sun.tools.javac.file.JavacFileManager; import com.sun.tools.javac.parser.JavacParser; import com.sun.tools.javac.parser.ParserFactory; import com.sun.tools.javac.tree.JCTree; import com.sun.tools.javac.util.Context; import com.sun.tools.javac.util.Name; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.util.CollectionUtils; import javax.tools.JavaFileManager; import java.nio.file.Files; import java.nio.file.Path; import java.util.List; import java.util.Map; public class JavaSourceCodeParser { private static final Logger logger = LoggerFactory.getLogger(JavaSourceCodeParser.class); private static final Joiner JOINER = Joiner.on(&amp;quot;\n&amp;quot;).skipNulls(); public static String parseFullyQualifiedClassName(String fileName, String scriptSourceCode) { Preconditions.</description>
    </item>
    
    <item>
      <title>Lombok中@Builder注解使用注意事项</title>
      <link>https://wenchao.ren/posts/lombok%E4%B8%ADbuilder%E6%B3%A8%E8%A7%A3%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</link>
      <pubDate>Thu, 05 Nov 2020 15:33:10 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/lombok%E4%B8%ADbuilder%E6%B3%A8%E8%A7%A3%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</guid>
      <description>Lombok使用@Builder注解时，默认是不能反序列化的，因为没有默认构造函数，因此可以通过增加下面2个注解来解决问题：
@NoArgsConstructor(access = AccessLevel.PUBLIC) @AllArgsConstructor(access = AccessLevel.PRIVATE) 
示例代码：
@Data @Builder @NoArgsConstructor(access = AccessLevel.PUBLIC) @AllArgsConstructor(access = AccessLevel.PRIVATE) public static class NamespaceItem { private String namespace; private Map&amp;lt;String, Set&amp;lt;String&amp;gt;&amp;gt; appOwners = Maps.newHashMap(); }  </description>
    </item>
    
    <item>
      <title>扫描指定package下面的类文件</title>
      <link>https://wenchao.ren/posts/%E6%89%AB%E6%8F%8F%E6%8C%87%E5%AE%9Apackage%E4%B8%8B%E9%9D%A2%E7%9A%84%E7%B1%BB%E6%96%87%E4%BB%B6/</link>
      <pubDate>Mon, 02 Nov 2020 20:00:11 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/%E6%89%AB%E6%8F%8F%E6%8C%87%E5%AE%9Apackage%E4%B8%8B%E9%9D%A2%E7%9A%84%E7%B1%BB%E6%96%87%E4%BB%B6/</guid>
      <description>下面的程序代码是使用Guava来完成操作的：
@Test public void test_scan() throws Exception { //using guava ClassPath classPath = ClassPath.from(ClassUtils.getDefaultClassLoader()); ImmutableSet&amp;lt;ClassPath.ClassInfo&amp;gt; topLevelClasses = classPath.getTopLevelClasses(); for (ClassPath.ClassInfo topLevelClass : topLevelClasses) { if (topLevelClass.getPackageName().equals(&amp;quot;com.xxx.xxx.com.xxx.xxx.metadata&amp;quot;)) { Class&amp;lt;?&amp;gt; clazz = topLevelClass.load(); Entity annotation = AnnotationUtils.findAnnotation(clazz, Entity.class); if (annotation != null) { System.out.println(topLevelClass.toString()); } } } }  </description>
    </item>
    
    <item>
      <title>使用GitHub Actions自动化部署博客到vps</title>
      <link>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E5%88%B0vps/</link>
      <pubDate>Mon, 02 Nov 2020 19:39:26 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8github-actions%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E5%88%B0vps/</guid>
      <description>GitHub的Actions可以用来协助我们做一些自动化的事情。比如当博客仓库有提交时，自动将生成的html文件部署到vps机器上。 我主要是参考了文章：利用 GitHub Actions 自动部署 Hugo 博客到自建 VPS
下面贴一下我的workflow配置：
# This is a basic workflow to help you get started with Actions name: deploy to vps workflow # Controls when the action will run. Triggers the workflow on push events # but only for the master branch on: push: branches: - main # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called &amp;quot;build&amp;quot; build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 with: submodules: &#39;recursive&#39; # Use ssh-agent to cache ssh keys - uses: webfactory/ssh-agent@v0.</description>
    </item>
    
    <item>
      <title>使用GitHub&#43;VPS做图床</title>
      <link>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8github&#43;vps%E5%81%9A%E5%9B%BE%E5%BA%8A/</link>
      <pubDate>Mon, 02 Nov 2020 13:37:13 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8github&#43;vps%E5%81%9A%E5%9B%BE%E5%BA%8A/</guid>
      <description>早期我是使用七牛做图床的，使用其实还是比较ok的，但是随着chrome的升级发现之前的图片加载不出来了。原因是出现ERR_SSL_VERSION_OR_CIPHER_MISMATCH。但是七牛的图床默认是http的，如果使用https是需要收费的。
我分析了一下我的现状：
 大量使用GitHub 有自己的vps 博客图片公开，不需要私密 图片不能丢，最好数据可以自己维护  基于这几个原因，我采样了使用GitHub做图床。突然通过Picgo上传到GitHub的特定仓库上，并且给Picgo安装了rename插件： https://github.com/liuwave/picgo-plugin-super-prefix#readme 这个插件还是功能比较足够的，我的配置为：img/{y}/{m}/{timestamp}-{hash}-{origin}
这样我就可以通过picgo来自动的将剪贴板中的图片上传到GitHub上。然后我又自己为GitHub的这个图床仓库设置了workflow。这样当有图片文件 被push到这个仓库的时候，workflow流程会自动的将图片文件夹通过rsync命令同步到vps的指定目录下。
下面贴一下我的workflow的文件：
# This is a basic workflow to help you get started with Actions name: sync to vps workflow # Controls when the action will run. Triggers the workflow on push events # but only for the master branch on: push: branches: - main # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called &amp;quot;build&amp;quot; build: # The type of runner that the job will run on runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 with: submodules: &#39;recursive&#39; # Use ssh-agent to cache ssh keys - uses: webfactory/ssh-agent@v0.</description>
    </item>
    
    <item>
      <title>从hexo迁移到hugo</title>
      <link>https://wenchao.ren/posts/%E4%BB%8Ehexo%E8%BF%81%E7%A7%BB%E5%88%B0hugo/</link>
      <pubDate>Fri, 30 Oct 2020 19:00:50 +0800</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BB%8Ehexo%E8%BF%81%E7%A7%BB%E5%88%B0hugo/</guid>
      <description>最近折腾了一下博客，主要做了下面几个事情：
 博客使用的静态生成工具从Hexo迁移到了Hugo 博客使用的图床从七牛迁移到了GitHub上 顺带还了一个博客皮肤（基于zozo），并自己定制了一下 顺带使用GitHub的workflow来自动化博客的部署。  历史的博客文章我会慢慢的迁移过来。</description>
    </item>
    
    <item>
      <title>Archives</title>
      <link>https://wenchao.ren/archives/archives/</link>
      <pubDate>Fri, 30 Oct 2020 16:15:01 +0800</pubDate>
      
      <guid>https://wenchao.ren/archives/archives/</guid>
      <description>This page contains an archive of all posts.</description>
    </item>
    
    <item>
      <title>chrome 和ie首页被劫持的解决办法</title>
      <link>https://wenchao.ren/posts/chrome-%E5%92%8Cie%E9%A6%96%E9%A1%B5%E8%A2%AB%E5%8A%AB%E6%8C%81%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</link>
      <pubDate>Mon, 19 Oct 2020 17:42:43 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/chrome-%E5%92%8Cie%E9%A6%96%E9%A1%B5%E8%A2%AB%E5%8A%AB%E6%8C%81%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</guid>
      <description>最近家里台式机的电脑的chrome和ie的首页被劫持，现象就是打开浏览器后地址栏为：http://kb1.gndh888.top， 然后过几秒就跳转到hao123网站。
这次的浏览器劫持比我之前遇到的劫持更恶心，之前的劫持通过360或者qq软件管家fix一下就解决了，这次的劫持这2个工具都没起作用，我也 按照网上的一些教程看注册表，看chrome的启动参数等等的都没有发现问题。
一开始的临时解法是对chrome.exe重命名一下，比如重命名为chrome11.exe虽然就不会出现这个问题，但是这样属于治标不治本。恶心了一周时间。 后来看到有人说使用火绒恶意木马专杀工具， 我其实没抱希望测试了一下，发现居然可以解决这个问题。
火绒恶意木马专杀工具(直接下载), 修复完重启一下，然后运行了一下解决了。</description>
    </item>
    
    <item>
      <title>avro schema的基本知识</title>
      <link>https://wenchao.ren/posts/avro-schema%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Fri, 24 Jul 2020 19:54:29 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/avro-schema%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/</guid>
      <description>官方文档：https://avro.apache.org/docs/current/spec.html#json_encoding

Schema 定义 Avro Schema使用JSON表示的话，会是下面三种类型之一：
 一个JSON字符串，命名已定义的类型 一个JSON对象，类似于：{&amp;quot;type&amp;quot;: &amp;quot;typeName&amp;quot; ...attributes...} 一个JSON array，代表union类型   基本类型 
Avro Schema的基本类型有：
 null 类似于java的null boolean，对应java中的boolean int 对应java中的int long 对应java中的long float 对应java中的float double 对应java中的double bytes 对应java中的byte string 对应java中的java.lang.String  
复杂类型
Avro支持六种复杂类型：records, enums, arrays, maps, unions and fixed.

Records Record类似于java中的Object，我们用来表示对象。它支持3个属性：
 name 一个JSON String，用来标识record的名称（必须项）大多数时候都是java中的class名称 namespace 一个JSON String用来限定name的范围，大多数时候都是java中的package名称 doc 一个JSON String，用来对这个record进行描述信息，（可选性） aliases 一个JSON array。用来提供别名（可选性） fields 一个JSON array，用来列出record的字段。（必须项）每个字段有如下的属性：  name 一个JSON String，用来标识字段名称 doc 一个JSON String，用来解释字段 type 字段的类型，一个Schema。 default 字段的默认值，字段的默认值取决于字段的类型。如果字段类型的一个union类型的话，那么字段的类型就取决于union中的第一个schema的类型。针对bytes和fixed类型的字段，字段的默认值都会是JSON String。where Unicode code points 0-255 are mapped to unsigned 8-bit byte values 0-255.</description>
    </item>
    
    <item>
      <title>双指针技巧</title>
      <link>https://wenchao.ren/posts/%E5%8F%8C%E6%8C%87%E9%92%88%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Fri, 24 Jul 2020 19:09:58 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%8F%8C%E6%8C%87%E9%92%88%E6%8A%80%E5%B7%A7/</guid>
      <description>双指针有两类：
 快慢指针  一般用于解决链表相关的问题，如判断链表是否存在环   左右指针  一般用于解决数组相关的问题，如二分查找    
快慢指针 
快慢指针一般都初始化指向链表的头结点 head，前进时快指针 fast 在前，慢指针 slow 在后，巧妙解决一些链表中的问题。

判断链表中是否存在环 因为单链表的特点是每个节点只知道下一个节点，所以一个指针的话无法判断链表中是否含有环的。
使用快慢指针的思想解决链表中是否存在环的问题，其实是非常简单的，快慢指针初始时都指向链表头结点，慢指针每次前进一步，快指针每次前进两步。
 如果链表不存在环，那么快指针最终会运行到链表的终端节点 如果链表存在环，那么快指针会多跑一会然后追上慢指针，此时说明链表存在环。  boolean hasCycle(ListNode head) { ListNode fast = head; ListNode slow = head; while(fast != null &amp;amp;&amp;amp; fast.next != null) { fast = fast.next.next; slow = slow.next; if (fast == slow) { return true; } } return false; }</description>
    </item>
    
    <item>
      <title>二分查找一般解法</title>
      <link>https://wenchao.ren/posts/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E4%B8%80%E8%88%AC%E8%A7%A3%E6%B3%95/</link>
      <pubDate>Fri, 24 Jul 2020 19:06:30 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E4%B8%80%E8%88%AC%E8%A7%A3%E6%B3%95/</guid>
      <description>二分查找模板 二分查找一般容易犯错的地方有3个：
 应该给mid加1还是应该减1，还是不加不减 上层的while循环里面的条件应该是left &amp;lt; right 还是 left &amp;lt;= right 计算mid时，最好是采用mid = left + (right - left)/2而不是mid = (left + right) /2。因为这样可以避免int溢出。  
二分查找的一般框架为：
int binarySearch(int[] nums, int target) { int left = 0; int right = nums.length ...; while(...) { int mid = left + (right - left) / 2; if (nums[mid] == target) { ... } else if (nums[mid] &amp;lt; target) { ... } else if (nums[mid] &amp;gt; target) { .</description>
    </item>
    
    <item>
      <title>leetcode-28 实现strStr()</title>
      <link>https://wenchao.ren/posts/leetcode-28-%E5%AE%9E%E7%8E%B0strstr/</link>
      <pubDate>Thu, 18 Jun 2020 23:12:42 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/leetcode-28-%E5%AE%9E%E7%8E%B0strstr/</guid>
      <description>题目描述:
给定一个 haystack 字符串和一个 needle 字符串，在 haystack 字符串中找出 needle 字符串出现的第一个位置 (从0开始)。如果不存在，则返回 -1。
来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/implement-strstr 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。
我的第一版答案：
class Solution { public int strStr(String haystack, String needle) { if (haystack == null) return 0; if (needle == null) return 0; int haystackLength = haystack.length(); int needleLength = needle.length(); if (needleLength &amp;gt; haystackLength) { return -1; } for (int i = 0; i &amp;lt; (haystack.length() - needleLength + 1); i++) { int j =0; while(j &amp;lt; needleLength) { if (haystack.</description>
    </item>
    
    <item>
      <title>使用nvm管理本地的node的版本</title>
      <link>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8nvm%E7%AE%A1%E7%90%86%E6%9C%AC%E5%9C%B0%E7%9A%84node%E7%9A%84%E7%89%88%E6%9C%AC/</link>
      <pubDate>Fri, 12 Jun 2020 11:52:31 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8nvm%E7%AE%A1%E7%90%86%E6%9C%AC%E5%9C%B0%E7%9A%84node%E7%9A%84%E7%89%88%E6%9C%AC/</guid>
      <description>类似于java可以使用jenv来管理本地的多个jdk的版本一样，node可以使用nvm来管理自己本地的多个版本的node。 本文章参考了下面2篇文章：
 Mac 使用 nvm 管理多版本 node nvm：安裝、切換不同 Node.js 版本的管理器  为了便于查看我直接简单整理一下。
如果想卸载之前的node, 第一篇文章中的
$ sudo npm uninstall npm -g $ sudo rm -rf /usr/local/lib/node /usr/local/lib/node_modules /var/db/receipts/org.nodejs.* $ sudo rm -rf /usr/local/include/node /Users/$USER/.npm $ sudo rm /usr/local/bin/node  里面的/var/db/receipts/org.nodejs.*在我的mac上已经没有这个目录了，记得调整一下。
安装nvm使用brew install nvm就行。然后根据brew的安装提示可以在.zshrc（linux的任何可以export的地方）增加：
# nvm配置 export NVM_DIR=&amp;quot;$HOME/.nvm&amp;quot; [ -s &amp;quot;/usr/local/opt/nvm/nvm.sh&amp;quot; ] &amp;amp;&amp;amp; . &amp;quot;/usr/local/opt/nvm/nvm.sh&amp;quot; # This loads nvm [ -s &amp;quot;/usr/local/opt/nvm/etc/bash_completion.d/nvm&amp;quot; ] &amp;amp;&amp;amp; . &amp;quot;/usr/local/opt/nvm/etc/bash_completion.d/nvm&amp;quot; # This loads nvm bash_completion%  nvm常用指令：</description>
    </item>
    
    <item>
      <title>Intellij IDEA配置maven环境变量</title>
      <link>https://wenchao.ren/posts/intellij-idea%E9%85%8D%E7%BD%AEmaven%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</link>
      <pubDate>Thu, 04 Jun 2020 18:31:44 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/intellij-idea%E9%85%8D%E7%BD%AEmaven%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/</guid>
      <description>有时候项目中会使用到tools.jar，一般我们会在maven的pom文件中配置：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.sun&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;tools&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.8&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;system&amp;lt;/scope&amp;gt; &amp;lt;systemPath&amp;gt;${JAVA_HOME}/lib/tools.jar&amp;lt;/systemPath&amp;gt; &amp;lt;/dependency&amp;gt;  一般公司的服务器上的jAVA_HOME是固定的，而大家电脑上的JAVA_HOME很可能不一样，那么在使用maven clean命令的时候很可能会出现下面的异常：
/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/bin/java -Dvisualvm.id=106646863982958 -Dmaven.multiModuleProjectDirectory=/Users/xkrivzooh/IdeaProjects/bistoury &amp;quot;-Dmaven.home=/Applications/IntelliJ IDEA.app/Contents/plugins/maven/lib/maven3&amp;quot; &amp;quot;-Dclassworlds.conf=/Applications/IntelliJ IDEA.app/Contents/plugins/maven/lib/maven3/bin/m2.conf&amp;quot; &amp;quot;-Dmaven.ext.class.path=/Applications/IntelliJ IDEA.app/Contents/plugins/maven/lib/maven-event-listener.jar&amp;quot; &amp;quot;-javaagent:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar=49260:/Applications/IntelliJ IDEA.app/Contents/bin&amp;quot; -Dfile.encoding=UTF-8 -classpath &amp;quot;/Applications/IntelliJ IDEA.app/Contents/plugins/maven/lib/maven3/boot/plexus-classworlds.license:/Applications/IntelliJ IDEA.app/Contents/plugins/maven/lib/maven3/boot/plexus-classworlds-2.6.0.jar&amp;quot; org.codehaus.classworlds.Launcher -Didea.version2020.1.1 --update-snapshots clean [INFO] Scanning for projects... [ERROR] [ERROR] Some problems were encountered while processing the POMs: [ERROR] &#39;dependencies.dependency.systemPath&#39; for com.sun:tools:jar must specify an absolute path but is ${JAVA_HOME}/lib/tools.jar @ line 133, column 16 @ [ERROR] The build could not read 1 project -&amp;gt; [Help 1] [ERROR] [ERROR] The project com.</description>
    </item>
    
    <item>
      <title>curl常见用例</title>
      <link>https://wenchao.ren/posts/curl%E5%B8%B8%E8%A7%81%E7%94%A8%E4%BE%8B/</link>
      <pubDate>Thu, 04 Jun 2020 12:21:43 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/curl%E5%B8%B8%E8%A7%81%E7%94%A8%E4%BE%8B/</guid>
      <description>整理一下curl的常见用法，避免我用到一些我没记住的用例时在去到处找.
http method curl -XGET http://www.baidu.com curl -XGET &amp;lsquo;http://www.baidu.com?a=b&amp;amp;c=d&#39; curl -XPOST http://www.baidu.com
常见POST JSON POST curl --header &amp;quot;Content-Type: application/json&amp;quot; \ --X POST \ -d &#39;{&amp;quot;username&amp;quot;:&amp;quot;xyz&amp;quot;,&amp;quot;password&amp;quot;:&amp;quot;xyz&amp;quot;}&#39; \ http://localhost:3000/api/login  设置cookie
curl -i -H &amp;quot;Application/json&amp;quot; -H &amp;quot;Content-type: application/json&amp;quot; -v --cookie &amp;quot;userName=xxx&amp;quot; -XPOST &#39;http://domain.com&#39; -d &#39; { &amp;quot;code&amp;quot;:&amp;quot;typecode2&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;typename2&amp;quot;, &amp;quot;remark&amp;quot;:&amp;quot;remark&amp;quot;, &amp;quot;structures&amp;quot;:[ { “dimensionType&amp;quot;:2 }, { &amp;quot;dimensionType&amp;quot;: 3 } ] }&#39;  也可以吧body放置在文件中post
curl -d &amp;quot;@data.json&amp;quot; -X POST http://localhost:3000/data  然后data.json文件中写json，比如：
{ &amp;quot;key1&amp;quot;:&amp;quot;value1&amp;quot;, &amp;quot;key2&amp;quot;:&amp;quot;value2&amp;quot; }  FORM POST application/x-www-form-urlencoded is the default:</description>
    </item>
    
    <item>
      <title>不在要Netty的Pinple的线程中乱设置拒绝策略</title>
      <link>https://wenchao.ren/posts/%E4%B8%8D%E5%9C%A8%E8%A6%81netty%E7%9A%84pinple%E7%9A%84%E7%BA%BF%E7%A8%8B%E4%B8%AD%E4%B9%B1%E8%AE%BE%E7%BD%AE%E6%8B%92%E7%BB%9D%E7%AD%96%E7%95%A5/</link>
      <pubDate>Wed, 03 Jun 2020 21:15:38 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%B8%8D%E5%9C%A8%E8%A6%81netty%E7%9A%84pinple%E7%9A%84%E7%BA%BF%E7%A8%8B%E4%B8%AD%E4%B9%B1%E8%AE%BE%E7%BD%AE%E6%8B%92%E7%BB%9D%E7%AD%96%E7%95%A5/</guid>
      <description>之前给业务同学排查问题时发现我们的Trace服务的某个地方当Trace的量特别大时，一个线程池会对业务层抛出RejectedExecutionException， 于是热心的我就顺手给这个地方加了一个RejectedExecutionHandler的实现，在这个里面加一个监控，然后就没了。这样是没问题的，但是 「手贱」的我看到本文件中另外一个地方的线程池也没有设置拒绝策略。他之前的代码如下：
String name = nettyClientConfig.getString(nameKey); String workerExecutorName = StringUtils.hasText(name) ? String.format(&amp;quot;%s.NettyClientCodecThread&amp;quot;, name) : &amp;quot;NettyClientCodecThread&amp;quot;; ThreadFactory threadFactory = new DefaultThreadFactory(workerExecutorName, false); this.defaultEventExecutorGroup = new DefaultEventExecutorGroup(threadSize, threadFactory);  我修改之后为：
String name = nettyClientConfig.getString(nameKey); String workerExecutorName = StringUtils.hasText(name) ? String.format(&amp;quot;%s.NettyClientCodecThread&amp;quot;, name) : &amp;quot;NettyClientCodecThread&amp;quot;; ThreadFactory threadFactory = new NamedThreadFactory(workerExecutorName, false); int maxPendingTasks = nettyClientConfig.getInteger(maxPendingTasksKey, 100); this.defaultEventExecutorGroup = new DefaultEventExecutorGroup(threadSize, threadFactory, maxPendingTasks, (task, executor) -&amp;gt; Metrics.counter(&amp;quot;NettyClientCodecThread.rejected.counter&amp;quot;).get().inc());  心想着老子又做了一回活雷锋。但是后来这个改动坑了一波，因为这个defaultEventExecutorGroup线程池是在Netty的pipeline中使用的：
Bootstrap handler = this.bootstrap.group(this.eventLoopGroupSelector).channel(useEpoll() ? EpollSocketChannel.</description>
    </item>
    
    <item>
      <title>从Netty的ResourceLeakDetector#Lavel的设计的一些感想</title>
      <link>https://wenchao.ren/posts/%E4%BB%8Enetty%E7%9A%84resourceleakdetector-lavel%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/</link>
      <pubDate>Wed, 03 Jun 2020 00:37:35 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BB%8Enetty%E7%9A%84resourceleakdetector-lavel%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3/</guid>
      <description>Netty中的ResourceLeakDetector#Level有4个级别：
 DISABLED 这种模式下不进行泄露监控。 SIMPLE 这种模式下以1/128的概率抽取ByteBuf进行泄露监控。 ADVANCED 在SIMPLE的基础上，每一次对ByteBuf的调用都会尝试记录调用轨迹，消耗较大 PARANOID 在ADVANCED的基础上，对每一个ByteBuf都进行泄露监控，消耗最大。  一般而言，在项目的初期使用SIMPLE模式进行监控，如果没有问题一段时间后就可以关闭。否则升级到ADVANCED或者PARANOID模式尝试确认泄露位置。
结合自己做中间件开发的一些感触吧：
 client端新增加的功能，最好都有一个对应的开关，便于出问题的时候及时调整，给自己留个后路 client的功能尽量支持动态升级和降级，非核心功能不要影响业务功能，分清楚主次。 client端的功能代码必要的时候一定需要辅有排查问题的辅助代码 非核心功能，能异步就异步，尽可能快，异步处理的时候，尤其是异步回调的时候，一定要风清楚代码是在哪个线程池中执行的。  </description>
    </item>
    
    <item>
      <title>Lettuce一定要打开redis集群拓扑刷新功能</title>
      <link>https://wenchao.ren/posts/lettuce%E4%B8%80%E5%AE%9A%E8%A6%81%E6%89%93%E5%BC%80redis%E9%9B%86%E7%BE%A4%E6%8B%93%E6%89%91%E5%88%B7%E6%96%B0%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Wed, 03 Jun 2020 00:21:22 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/lettuce%E4%B8%80%E5%AE%9A%E8%A6%81%E6%89%93%E5%BC%80redis%E9%9B%86%E7%BE%A4%E6%8B%93%E6%89%91%E5%88%B7%E6%96%B0%E5%8A%9F%E8%83%BD/</guid>
      <description>在使用Lettuce访问Redis的时候，一定要记得打开它的Redis 集群拓扑刷新功能，否则他压根就不存在高可用。因为他的集群拓扑刷新功能是默认没开启的。
 RedisCluster集群模式下master宕机主从切换期间Lettuce连接Redis无法使用报错Redis command timed out的问题 SpringBoot2.X与redis Lettuce集成踩坑 SpringBoot2.1.X使用Redis连接池Lettuce踩坑  上面的3个文章其实说的就是这个事情，在redis集群拓扑结构发生变化，比如Redis的master挂掉了后，lettuce的client端就会长时间不能恢复。因此可以通过下面的配置打开拓扑刷新功能：
//默认超时时间, lettuce默认超时时间为60s太长了，此处默认设置为15s private Long timeoutInMillis = Duration.ofSeconds(15).toMillis(); static ClusterClientOptions.Builder initDefaultClusterClientOptions(ClusterClientOptions.Builder builder) { ClusterTopologyRefreshOptions defaultClusterTopologyRefreshOptions = ClusterTopologyRefreshOptions.builder() //开启集群拓扑结构周期性刷新，和默认参数保持一致 .enablePeriodicRefresh(60, TimeUnit.SECONDS) //开启针对{@link RefreshTrigger}中所有类型的事件的触发器 .enableAllAdaptiveRefreshTriggers() //和默认一样，30s超时，避免短时间大量出现刷新拓扑的事件 .adaptiveRefreshTriggersTimeout(30, TimeUnit.SECONDS) //和默认一样重连5次先，然后在刷新集群拓扑 .refreshTriggersReconnectAttempts(5) .build(); return builder // 配置用于开启自适应刷新和定时刷新。如自适应刷新不开启，Redis集群变更时将会导致连接异常 .topologyRefreshOptions(defaultClusterTopologyRefreshOptions) //默认就是重连的，显示定义一下 .autoReconnect(true) //和默认一样最大重定向5次，避免极端情况无止境的重定向 .maxRedirects(5) //Accept commands when auto-reconnect is enabled, reject commands when auto-reconnect is disabled. .disconnectedBehavior(ClientOptions.DisconnectedBehavior.DEFAULT) .socketOptions(SocketOptions.builder().keepAlive(true).tcpNoDelay(true).build()) //取消校验集群节点的成员关系 .validateClusterNodeMembership(false); } public static ClusterClientOptions.Builder getDefaultClusterClientOptionBuilder() { return initDefaultClusterClientOptions(ClusterClientOptions.</description>
    </item>
    
    <item>
      <title>Netty如何检测ByteBuf没有release</title>
      <link>https://wenchao.ren/posts/netty%E5%A6%82%E4%BD%95%E6%A3%80%E6%B5%8Bbytebuf%E6%B2%A1%E6%9C%89release/</link>
      <pubDate>Mon, 01 Jun 2020 23:31:56 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/netty%E5%A6%82%E4%BD%95%E6%A3%80%E6%B5%8Bbytebuf%E6%B2%A1%E6%9C%89release/</guid>
      <description>Netty中的ByteBuf算是中间件开发中比较常用的API了，一般我们会使用PooledByteBuf来提升性能，但是这个玩意需要我们使用以后手动进行release，如果有时候忘记手动释放的话，会出现内存泄漏。 而且这种问题一般也没那么方便的排查。不过非常幸运的是Netty已经帮我们考虑到了这个问题，它提供了自己的检测工具:
 ResourceLeakDetector ResourceLeakTracker  基本思想 他的实现原理很巧妙，不过我们先不着急说Netty的实现，我们先想想如果我们自己来弄，我们一般会面临下面3个问题：
 被检测的对象创建的时候，我们就需要知道他创建了，然后做一些操作，比如该标记就标记，该计数就计数， 对象「无用」的时候，我们也需要知道这个时刻。这里的「无用」一般我们选择对象被GC时 我们还需要一种机制来判断对象在被GC之前有没有调用某个操作，比如release或者close操作。  下面以netty 4.0.46版本来说哈。
 第一个问题其实很好实现，在对象的构造函数中我们就可以做这些事情，因为对象的构造函数执行的时候，就是他被创建的时候 第二个问题，Netty是利用了Java中的java.lang.ref.PhantomReference和引用队列这个东西。java.lang.ref.PhantomReference有叫虚引用也有叫做幽灵引用的，叫法无所谓，它和软引用（SoftReference）、弱引用（WeakReference）不同，它并不影响对象的生命周期，如果一个对象与java.lang.ref.PhantomReference关联，则跟没有引用与之关联一样，在任何时候都可能被垃圾回收器回收。而且除过强引用之外，剩余的3种引用类型都有一个引用队列可以与之配合。当java清理调用不必要的引用后，会将这个引用本身（不是引用指向的值对象）添加到队列之中。比如你看PhantomReference的定义：  package java.lang.ref; /** * Phantom reference objects, which are enqueued after the collector * determines that their referents may otherwise be reclaimed. Phantom * references are most often used for scheduling pre-mortem cleanup actions in * a more flexible way than is possible with the Java finalization mechanism. * * &amp;lt;p&amp;gt; If the garbage collector determines at a certain point in time that the * referent of a phantom reference is &amp;lt;a * href=&amp;quot;package-summary.</description>
    </item>
    
    <item>
      <title>借助arthas排查重复类的问题</title>
      <link>https://wenchao.ren/posts/%E5%80%9F%E5%8A%A9arthas%E6%8E%92%E6%9F%A5%E9%87%8D%E5%A4%8D%E7%B1%BB%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 29 May 2020 20:04:55 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%80%9F%E5%8A%A9arthas%E6%8E%92%E6%9F%A5%E9%87%8D%E5%A4%8D%E7%B1%BB%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>现象描述 业务反馈他们的项目运行时出现Jackson中的com.fasterxml.jackson.databind.deser.SettableBeanProperty类的版本不对，和他们在pom中指定的版本不一致，这种问题一般都是因为项目的依赖（包括间接依赖）中，存在某些依赖有shade包，如果这些shade包打包的时候忘记修改package，那么就经常会出现这种问题。
解决思路 这种问题其实只要确定jvm加载的这个com.fasterxml.jackson.databind.deser.SettableBeanProperty到底来自哪个jar就可以帮助我们确定问题根源，而借助Arthas可以快速解决这个问题：
 使用Arthas连接具体环境的具体机器上的应用 在console中输入如下的命令： sc -fd com.fasterxml.jackson.databind.deser.SettableBeanProperty 查看console的输出，看其中的 code-source就可以指定这个类来自哪个jar了  ## 安装arthas curl -L https://alibaba.github.io/arthas/install.sh | sh ## $PID为自己项目运行的pid，注意修改， 此处使用tomcat用户是因为我们的程序是tomcat用户运行的 sudo -u tomcat -EH ./as.sh $PID ## arthas attach成功以后在console中输入 sc -fd com.fasterxml.jackson.databind.deser.SettableBeanProperty  下面贴一个sc命令的样例输出：
class-info com.fasterxml.jackson.databind.deser.impl.SetterlessProperty code-source /data/w/www/data-bbb-sea.aaa.com/webapps/ROOT/WEB-INF/lib/jackson-databind-2.10.3.jar name com.fasterxml.jackson.databind.deser.impl.SetterlessProperty isInterface false isAnnotation false isEnum false isAnonymousClass false isArray false isLocalClass false isMemberClass false isPrimitive false isSynthetic false simple-name SetterlessProperty modifier final,public annotation interfaces super-class +-com.fasterxml.jackson.databind.deser.SettableBeanProperty +-com.fasterxml.jackson.databind.introspect.ConcreteBeanPropertyBase +-java.</description>
    </item>
    
    <item>
      <title>dubbo对异常的一些处理</title>
      <link>https://wenchao.ren/posts/dubbo%E5%AF%B9%E5%BC%82%E5%B8%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86/</link>
      <pubDate>Fri, 29 May 2020 12:23:53 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/dubbo%E5%AF%B9%E5%BC%82%E5%B8%B8%E7%9A%84%E4%B8%80%E4%BA%9B%E5%A4%84%E7%90%86/</guid>
      <description>看了一下这个文章《Dubbo如何处理业务异常，这个一定要知道哦》，但是不赞同文章中的一些论点，这里整理一下dubbo源码中Provider端的ExceptionFilter中 对异常的处理逻辑。
在Dubbo的provider端的com.alibaba.dubbo.rpc.filter.ExceptionFilter中，有如下几个规则：
 如果是checked异常，直接抛出 在方法签名上有声明，直接抛出, 未在方法签名上定义的异常，在Provider端打印ERROR日志 异常类和接口类在同一jar包里，直接抛出 是JDK自带（以java.或者javax.开头）的异常，直接抛出 是Dubbo本身的异常（RpcException），直接抛出 如果以上几个规则都不满足的话，则将异常包装为RuntimeException抛给Consumer端  其中一个比较有意思的点就是如何根据一个class判断他所属的jar：
在ExceptionFilter中的逻辑是如下的：
// 异常类和接口类在同一jar包里，直接抛出 String serviceFile = ReflectUtils.getCodeBase(invoker.getInterface()); String exceptionFile = ReflectUtils.getCodeBase(exception.getClass()); if (serviceFile == null || exceptionFile == null || serviceFile.equals(exceptionFile)){ return result; }  其中 ReflectUtils.getCodeBase的代码如下：
public static String getCodeBase(Class&amp;lt;?&amp;gt; cls) { if (cls == null) return null; ProtectionDomain domain = cls.getProtectionDomain(); if (domain == null) return null; CodeSource source = domain.getCodeSource(); if (source == null) return null; URL location = source.</description>
    </item>
    
    <item>
      <title>Unexpected end of ZLIB input stream</title>
      <link>https://wenchao.ren/posts/unexpected-end-of-zlib-input-stream/</link>
      <pubDate>Sat, 04 Apr 2020 01:19:53 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/unexpected-end-of-zlib-input-stream/</guid>
      <description>前几天在项目开发是遇到了这个Unexpected end of ZLIB input stream异常。异常出现的位置：
Caused by: java.io.EOFException: Unexpected end of ZLIB input stream at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240) at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158) at java.util.zip.GZIPInputStream.read(GZIPInputStream.java:117) at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:122)  之前一开始没太想清楚，以为是我写的GzipFilter出现了问题，后来吃了个午饭才恍然大悟，是client端的数据传输有点问题。简单抽象一下场景就是client通过http接口给server上报 一些数据，这些数据使用了gzip来进行压缩。问题出现在这个gzip压缩这快。我看来看看早期的有问题的代码：
private byte[] buildRequestBody(List&amp;lt;LoggerEntity&amp;gt; loggerEntities) { try { try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); GZIPOutputStream gzipOutputStream = new GZIPOutputStream(byteArrayOutputStream)) { gzipOutputStream.write(JSON.writeValueAsBytes(loggerEntities)); return byteArrayOutputStream.toByteArray(); } } catch (IOException e) { throw new RuntimeException(e); } }  先说一下上面的代码是有问题的，问题在于try-with-resource里面的try中的2行代码，因为很可能gzipOutputStream没写完然后就已经return了。因此此处有两种处理办法，
第一种就是在try里面对gzipOutputStream进行close:
private byte[] buildRequestBody(List&amp;lt;LoggerEntity&amp;gt; loggerEntities) { try { try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); GZIPOutputStream gzipOutputStream = new GZIPOutputStream(byteArrayOutputStream)) { gzipOutputStream.</description>
    </item>
    
    <item>
      <title>mybatis层面限制SQL注入</title>
      <link>https://wenchao.ren/posts/mybatis%E5%B1%82%E9%9D%A2%E9%99%90%E5%88%B6sql%E6%B3%A8%E5%85%A5/</link>
      <pubDate>Fri, 03 Jan 2020 18:33:23 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/mybatis%E5%B1%82%E9%9D%A2%E9%99%90%E5%88%B6sql%E6%B3%A8%E5%85%A5/</guid>
      <description>最近根据公司的要求需要限制一波SQL注入的问题，因为公司有自己的数据库访问层组件，使用的数据库连接池为druid。 其实在druid的com.alibaba.druid.wall.WallFilter中提供了对sql依赖注入的检查，但是最终有下面几个原因我们没有 在druid层面来解决这个问题：
 我们依赖的druid版本1.0.8太低了，查看了一下druid后面不少版本的改进，对sql解析这块有大量的优化，因此贸然升级一个版本风险有点高 我们想修改一下WallConfig的配置，但是在1.0.8版本的druid中不太好获取WallConfig这个类的实例，高版本的druid优化了这个问题。而且在druid中MySqlWallProvider类是在com.alibaba.druid.wall.spipackage下面定义的，但是在WallFilter#init()方法中缺并没有使用spi等方式来拿这个类。 我们想最小化改动，尽量减少测试成本  在上面3个原因的考虑下，我们放弃了这种方案，转而将目光放在了公司统一使用的Mybatis上。
在Mybatis的org.apache.ibatis.scripting.xmltags.TextSqlNode中有一个字段为injectionFilter。
public class TextSqlNode implements SqlNode { private String text; private Pattern injectionFilter; //...其他代码省略 private static class BindingTokenParser implements TokenHandler { private DynamicContext context; private Pattern injectionFilter; public BindingTokenParser(DynamicContext context, Pattern injectionFilter) { this.context = context; this.injectionFilter = injectionFilter; } @Override public String handleToken(String content) { Object parameter = context.getBindings().get(&amp;quot;_parameter&amp;quot;); if (parameter == null) { context.getBindings().put(&amp;quot;value&amp;quot;, null); } else if (SimpleTypeRegistry.isSimpleType(parameter.getClass())) { context.</description>
    </item>
    
    <item>
      <title>Kettle缺少libwebkitgtk-1.0的问题</title>
      <link>https://wenchao.ren/posts/kettle%E7%BC%BA%E5%B0%91libwebkitgtk-1-0%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 03 Jan 2020 18:30:12 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/kettle%E7%BC%BA%E5%B0%91libwebkitgtk-1-0%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>前几天在搭建kettle集群的时候出现Kettle缺少libwebkitgtk-1.0的问题，我司的server系统为centos7，google了一下发现centos7的解决办法还是和 kettle软件提示的解决办法还是有点区别的。因此记录一下。
[blibee@1.d.fd.dev.bj1.wormpex.com /home/w/kettle/data-integration]$ sudo ./carte.sh ./pwd/carte-config-master-15000.xml ####################################################################### WARNING: no libwebkitgtk-1.0 detected, some features will be unavailable Consider installing the package with apt-get or yum. e.g. &#39;sudo apt-get install libwebkitgtk-1.0-0&#39; #######################################################################  解决办法：
sudo wget ftp://ftp.pbone.net/mirror/ftp5.gwdg.de/pub/opensuse/repositories/home:/matthewdva:/build:/EPEL:/el7/RHEL_7/x86_64/webkitgtk-2.4.9-1.el7.x86_64.rpm sudo yum install webkitgtk-2.4.9-1.el7.x86_64.rpm  </description>
    </item>
    
    <item>
      <title>缩短class路径</title>
      <link>https://wenchao.ren/posts/%E7%BC%A9%E7%9F%ADclass%E8%B7%AF%E5%BE%84/</link>
      <pubDate>Fri, 27 Dec 2019 19:31:50 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E7%BC%A9%E7%9F%ADclass%E8%B7%AF%E5%BE%84/</guid>
      <description>如果有时候在打印一些class日志时，经常会遇到class full name太长的问题，这个时候可以借助logback中的ch.qos.logback.classic.pattern.TargetLengthBasedClassNameAbbreviator来缩短输出。 ​</description>
    </item>
    
    <item>
      <title>AttachNotSupportedException和jstack失败的常见原因</title>
      <link>https://wenchao.ren/posts/attachnotsupportedexception%E5%92%8Cjstack%E5%A4%B1%E8%B4%A5%E7%9A%84%E5%B8%B8%E8%A7%81%E5%8E%9F%E5%9B%A0/</link>
      <pubDate>Fri, 27 Dec 2019 19:15:21 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/attachnotsupportedexception%E5%92%8Cjstack%E5%A4%B1%E8%B4%A5%E7%9A%84%E5%B8%B8%E8%A7%81%E5%8E%9F%E5%9B%A0/</guid>
      <description>最近在公司升级Bistoury Agent时发现，有不少应用出AttachNotSupportedException异常：
com.sun.tools.attach.AttachNotSupportedException: Unable to open socket file: target process not responding or HotSpot VM not loaded at sun.tools.attach.LinuxVirtualMachine.&amp;lt;init&amp;gt;(LinuxVirtualMachine.java:106) ~[tools.jar:na] at sun.tools.attach.LinuxAttachProvider.attachVirtualMachine(LinuxAttachProvider.java:78) ~[tools.jar:na] at com.sun.tools.attach.VirtualMachine.attach(VirtualMachine.java:250) ~[tools.jar:na] at qunar.tc.bistoury.commands.arthas.ArthasStarter.attachAgent(ArthasStarter.java:74) ~[bistoury-commands-1.4.22.jar:na] at qunar.tc.bistoury.commands.arthas.ArthasStarter.start(ArthasStarter.java:57) ~[bistoury-commands-1.4.22.jar:na] at qunar.tc.bistoury.commands.arthas.ArthasEntity.start(ArthasEntity.java:82) [bistoury-commands-1.4.22.jar:na]  但是这样应用的行为和监控指标都是特别正常的，此时如果给这些应用使用:sudo -u tomcat jstack [pid]（备注我们的应用是tomcat用户运行的）的话，会发现jstack 使用出问题，一个例子为：
sudo -u tomcat /home/w/java/default/bin/jstack 691167 691167: Unable to open socket file: target process not responding or HotSpot VM not loaded The -F option can be used when the target process is not responding  然后查看tomcat的catalina.</description>
    </item>
    
    <item>
      <title>git移除对部分文件的追踪</title>
      <link>https://wenchao.ren/posts/git%E7%A7%BB%E9%99%A4%E5%AF%B9%E9%83%A8%E5%88%86%E6%96%87%E4%BB%B6%E7%9A%84%E8%BF%BD%E8%B8%AA/</link>
      <pubDate>Fri, 27 Dec 2019 19:09:44 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/git%E7%A7%BB%E9%99%A4%E5%AF%B9%E9%83%A8%E5%88%86%E6%96%87%E4%BB%B6%E7%9A%84%E8%BF%BD%E8%B8%AA/</guid>
      <description>如果不小心将某些不需要被git管理的文件加入了git中，取消的办法如下：
 在当前目录下.gitignore文件里面加入不需要进行版本控制器的文件 执行git rm -r --cached 文件名命令 执行gti commit &amp;amp;&amp;amp; git push提交修改  </description>
    </item>
    
    <item>
      <title>Bistoury源码阅读-Agent启动</title>
      <link>https://wenchao.ren/posts/bistoury%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link>
      <pubDate>Wed, 25 Dec 2019 00:57:24 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/bistoury%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid>
      <description>我们先以Bistoury Agent在本地启动为例子，入口类是bistoury-independent-agentmodule中的qunar.tc.bistoury.indpendent.agent.Main或者 qunar.tc.bistoury.indpendent.agent.MainTest类。一般我们测试的时候，都是通过MainTest类。
同时通过MainTest的写法，我们其实可以看到BistouryAgent启动的几个核心配置项：
 bistoury.app.lib.class 其实就是指定一个class，然后基于这个class找它的classLoader，一般我们都会指定为公司所有项目都会有的一个基础类。 bistoury.lib.dir bistoury的lib地址 bistoury.store.path bistoury的store地址 bistoury.proxy.host Bistoury Proxy的域名  从Main类中，我们可以看到一个方法，这个方法的功能倒是没啥说的，此处可以学习一下java.lang.management.ManagementFactory的API。
public static void log() { List&amp;lt;String&amp;gt; args = ManagementFactory.getRuntimeMXBean().getInputArguments(); for (String arg : args) { logger.info(&amp;quot;Command line argument: {}&amp;quot;, arg); } }  Bistoury Agent的核心类其实就是qunar.tc.bistoury.agent.AgentClient类。这个类主要有下面几个功能：
 通过HTTP形式的调用{bistoury.proxy.host}/proxy/config/foragent地址获取一个BistouryProxy server的基础信息，ip，port，heartbeatSec。 此处之所以通过这种HTTP形式的调用，有几个目的：  一定程度上面的负载均衡，因为我们一般会给BistouryProxy分配一个域名，那么此处的LB其实就是依赖NGINX了。 拉取BistouryProxy的一些基础信息，目前拉取到的信息比较少，核心的只有一个heartbeatSec   建立Bistoury Agent和Bistoury Proxy之间的Netty长连接。这部分的代码主要是在qunar.tc.bistoury.agent.AgentNettyClient类中 通过一个单线程的调度线程来启动FailoverTask。也就是如果Bistoury Agent和Bistoury Proxy之间的连接失败或者连接因为其他原因断开等，那么这个线程目前会没1分钟check一下。 这里之所以每分钟check一下，而不是立即尝试，我觉的考虑主要是防止连接proxy的突发流量过大，同时Bistoury的使用其实并不是一个高频的产品，所以很多时候没必要立即恢复。当然 这样也带来一个问题，就是如果发布或是某台宕机的话，那一个agent可能就要最大1分钟才能重新可用了。  接下来我们来看看qunar.tc.bistoury.agent.AgentNettyClient的代码，这个其实就是负责和Bistoury Proxy之间的连接建立工作。有下面几个核心点，只要摸清楚这几个关键点，那么Agent启动流程的东西就全搞明白了。
 AgentInfoRefreshTask  定时给BistouryProxy发送获取Bistoury Agent相关配置信息的请求。内部是基于SingleThreadScheduledExecutor实现，默认间隔10分钟请求一次。涉及到的命令code为: qunar.tc.bistoury.remoting.protocol.CommandCode#REQ_TYPE_REFRESH_AGENT_INFO   HeartbeatTask  定时给BistouryProxy发送心跳消息。这个时间间隔是在BistouryProxy端控制的，默认是30秒心跳一次。默认心跳超时时间阈值为（30 * 2 + 30/2） 也就是75秒。不过这块的心跳请求构建的代码是有点挫的。   DefaultTaskStore  Agent端的Task管理对象。内部基于ConcurrentHashMap存储tasks。同时有一个bistoury-task-clear（单线程的scheduler）每10秒钟检查一下存储的tasks。如果某个task长时间没有完成，那么这个任务就会从ConcurrentHashMap中移除它。核心方法有：  boolean register(Task task); void finish(String id); void cancel(String id); void close();     通过SPI的方式加载所有的TaskFactory, 每一个TaskFactory都对应一个qunar.</description>
    </item>
    
    <item>
      <title>dubbo源码解析系列</title>
      <link>https://wenchao.ren/posts/dubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/</link>
      <pubDate>Sat, 21 Dec 2019 01:35:19 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/dubbo%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/</guid>
      <description>这个是dubbo源码解析系列的目录，不定时更新&amp;hellip;</description>
    </item>
    
    <item>
      <title>Intellij IDEA中搜索yaml中的key</title>
      <link>https://wenchao.ren/posts/intellij-idea%E4%B8%AD%E6%90%9C%E7%B4%A2yaml%E4%B8%AD%E7%9A%84key/</link>
      <pubDate>Thu, 10 Oct 2019 19:00:12 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/intellij-idea%E4%B8%AD%E6%90%9C%E7%B4%A2yaml%E4%B8%AD%E7%9A%84key/</guid>
      <description>因yaml文件相比于properties文件来结构更加清晰，所以现在无论是公司项目中还是开源的项目中，yaml文件越来越常见。 曾经的特别讨厌使用yaml文件主要是因为搜索yaml文件中的key的时候太麻烦：
比如有下面的yaml文件：
wsearch: zk: address: xxxxxxxx  在Idea中使用Command+Shitf+F搜索wsearch.zk.address的时候是搜索不到的
就因为这个原因，导致我特别的厌烦yaml的配置。
今天才发现原来在IDEA可以使用如下的方式搜索到。那就是使用Idea的Search everywhere功能。按2下shift，然后在搜索就好了：
发现在Stack Overflow也有人问这个问题How to find specific property key in a yaml file using intellij idea?</description>
    </item>
    
    <item>
      <title>正向代理和反向代理</title>
      <link>https://wenchao.ren/posts/%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/</link>
      <pubDate>Wed, 25 Sep 2019 13:02:31 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/</guid>
      <description>经常会听到正向代理和反向代理的说法，那么具体怎么区分呢？
正向代理 正向代理，是在用户端的。
比如需要访问某些国外网站，我们可能需要购买vpn。并且vpn是在我们的用户浏览器端设置的(并不是在远端的服务器设置)。浏览器先访问vpn地址，vpn地址转发请求，并最后将请求结果原路返回来。
反向代理 有正向代理，就有反向代理，反向代理是作用在服务器端的，是一个虚拟ip(VIP)。对于用户的一个请求，会转发到多个后端处理器中的一台来处理该具体请求。
区别  正向代理:客户端 &amp;lt;一&amp;gt; 代理 一&amp;gt;服务端 反向代理:客户端 一&amp;gt;代理 &amp;lt;一&amp;gt; 服务端  正向代理和反向代理的区别在于 代理的对象不一样,正向代理的代理对象是客户端,反向代理的代理对象是服务端
参考资料  正向代理和反向代理的配置示例  </description>
    </item>
    
    <item>
      <title>数据库MHA架构介绍</title>
      <link>https://wenchao.ren/posts/%E6%95%B0%E6%8D%AE%E5%BA%93mha%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 23 Sep 2019 20:25:45 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E6%95%B0%E6%8D%AE%E5%BA%93mha%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/</guid>
      <description>MHA（Master High Availability）目前在MySQL高可用方面是一个相对成熟的解决方案，它由日本人youshimaton开发，是一套优秀的作为MySQL高可用性环境下故障切换和主从提升的高可用软件。在MySQL故障切换过程中，MHA能做到0~30秒之内自动完成数据库的故障切换操作，并且在进行故障切换的过程中，MHA能最大程度上保证数据库的一致性。
适用场景 目前MHA主要支持一主多从的架构，要搭建MHA，要求一个复制集群必须最少有3台数据库服务器，一主二从，即一台充当Master，一台充当备用Master，另一台充当从库。出于成本考虑，淘宝在此基础上进行了改造，目前淘宝开发的TMHA已经支持一主一从。
MAH组件 MHA由两部分组成：
 MHA Manager（管理节点） MHA Node（数据节点）。  MHA Manager MHA Manager可以单独部署在一台独立的机器上管理多个master-slave集群，也可以部署在一台slave节点上。主要用来运行一些工具，比如masterha_manager工具实现自动监控MySQL Master和实现master故障切换，其它工具实现手动实现master故障切换、在线mater转移、连接检查等等。一个Manager可以管理多 个master-slave集群
MHA Manager会定时探测集群中的master节点，当master出现故障时，它可以自动将最新的slave提升为新的master，然后将所有其他的slave重新指向新的master。整个故障转移过程对应用程序完全透明。
MHA Node MHA Node运行在每台MySQL服务器上，类似于Agent。主要作用有：
 保存二进制日志 如果能够访问故障master，会拷贝master的二进制日志 应用差异中继日志 从拥有最新数据的slave上生成差异中继日志，然后应用差异日志。 清除中继日志 在不停止SQL线程的情况下删除中继日志  工作原理简述 因为MHA Manager会不断的去探测master节点，因此当master出现故障时，通过对比slave之间I/O线程读取masterbinlog的位置，选取最接近的slave做为latestslave。 其它slave通过与latest slave对比生成差异中继日志。在latest slave上应用从master保存的binlog，同时将latest slave提升为master。最后在其它slave上应用相应的差异中继日志并开始从新的master开始复制。
在MHA实现Master故障切换过程中，MHA Node会试图访问故障的master（通过SSH），如果可以访问（不是硬件故障，比如InnoDB数据文件损坏等），会保存二进制文件，以最大程度保 证数据不丢失。MHA和半同步复制一起使用会大大降低数据丢失的危险。流程如下：
 从宕机崩溃的master保存二进制日志事件(binlog events)。 识别含有最新更新的slave。 应用差异的中继日志(relay log)到其它slave。 应用从master保存的二进制日志事件(binlog events)。 提升一个slave为新master并记录binlog file和position。 使其它的slave连接新的master进行复制。 完成切换manager主进程OFFLINE  </description>
    </item>
    
    <item>
      <title>pt-fingerprint 安装和基本使用</title>
      <link>https://wenchao.ren/posts/pt-fingerprint-%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Tue, 10 Sep 2019 19:23:41 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/pt-fingerprint-%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</guid>
      <description>sudo wget percona.com/get/percona-toolkit.tar.gz ## 安装相关依赖 sudo yum install perl -y sudo yum install perl-DBI -y sudo yum install perl-DBD-MySQL -y sudo yum install perl-Time-HiRes -y sudo yum install perl-IO-Socket-SSL -y sudo yum install perl-Digest-MD5.x86_64 -y  初步使用
[$ /home/w/percona/percona-toolkit-3.0.13/bin]$ ./pt-fingerprint --query &amp;quot;select a, b, c from users where id = 500&amp;quot; select a, b, c from users where id = ? [$ /home/w/percona/percona-toolkit-3.0.13/bin]$ ./pt-fingerprint --query &amp;quot;update test set a =1 , b=2 where id = 3&amp;quot; update test set a =?</description>
    </item>
    
    <item>
      <title>gc Roots对象有哪些</title>
      <link>https://wenchao.ren/posts/gc-roots%E5%AF%B9%E8%B1%A1%E6%9C%89%E5%93%AA%E4%BA%9B/</link>
      <pubDate>Tue, 03 Sep 2019 12:07:27 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/gc-roots%E5%AF%B9%E8%B1%A1%E6%9C%89%E5%93%AA%E4%BA%9B/</guid>
      <description>JVM的垃圾自动回收是我们经常说的一个话题，这里的垃圾的含义是：
 内存中已经不再被使用到的对象就是垃圾
 要进行垃圾回收，如何判断一个对象是否可以被回收？ 一般有两种办法：
 引用计数法  实现简单，但是没法解决对象之间的循环引用问题   枚举根节点做可达性分析  通过一系列名为“GC Roots”的对象作为起始点，从“GC Roots”对象开始向下搜索，如果一个对象到“GC Roots”没有任何引用链相连，说明此对象可以被回收    常见的常见的GC Root有如下：
 通过System Class Loader或者Boot Class Loader加载的class对象，通过自定义类加载器加载的class不一定是GC Root 处于激活状态的线程 栈中的对象 本地方法栈中 JNI (Native方法)的对象 JNI中的全局对象 正在被用于同步的各种锁对象 JVM自身持有的对象，比如系统类加载器等。  </description>
    </item>
    
    <item>
      <title>Synchronized的一些东西</title>
      <link>https://wenchao.ren/posts/synchronized%E7%9A%84%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF/</link>
      <pubDate>Mon, 02 Sep 2019 12:39:02 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/synchronized%E7%9A%84%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF/</guid>
      <description>synchronized是Java中解决并发问题的一种最常用的方法，从语法上讲synchronized总共有三种用法：
 修饰普通方法 修饰静态方法 修饰代码块  synchronized 原理 为了查看synchronized的原理，我们首先反编译一下下面的代码, 这是一个synchronized修饰代码块的demo
public class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(&amp;quot;Method 1 start&amp;quot;); } } }  从上面截图可以看到，synchronized的实现依赖2个指令：
 monitorenter monitorexit  但是从上面的截图可以看到有一个monitorenter和2个monitorexit，这里之所以有2个monitorexit是因为synchronized的锁释放有2种情况：
 方法正常执行完毕synchronized的范围，也就是正常情况下的锁释放 synchronized圈起来的范围内的代码执行抛出异常，导致锁释放  monitorenter 关于这个指令，jvm中的描述为：
 Each object is associated with a monitor. A monitor is locked if and only if it has an owner. The thread that executes monitorenter attempts to gain ownership of the monitor associated with objectref, as follows:</description>
    </item>
    
    <item>
      <title>ThreadPoolExecutor相关</title>
      <link>https://wenchao.ren/posts/threadpoolexecutor%E7%9B%B8%E5%85%B3/</link>
      <pubDate>Fri, 30 Aug 2019 12:42:06 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/threadpoolexecutor%E7%9B%B8%E5%85%B3/</guid>
      <description>java中的线程池相关的东西抛不开ThreadPoolExecutor，本文就简单的说说这个ThreadPoolExecutor。
先看一个ThreadPoolExecutor的demo，然后我们说说它的相关参数
import java.util.concurrent.LinkedBlockingQueue; import java.util.concurrent.RejectedExecutionHandler; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class Test { private static ThreadPoolExecutor threadPoolExecutor; public static void main(String[] args) { threadPoolExecutor = new ThreadPoolExecutor( 4, 8, 0, TimeUnit.MICROSECONDS, new LinkedBlockingQueue&amp;lt;&amp;gt;(100), new RejectedExecutionHandler() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { } }); System.out.println(threadPoolExecutor.getCorePoolSize()); //4 System.out.println(threadPoolExecutor.getMaximumPoolSize()); //8 System.out.println(threadPoolExecutor.getPoolSize());//0 boolean b = threadPoolExecutor.prestartCoreThread(); System.out.println(threadPoolExecutor.getCorePoolSize());//4 System.out.println(threadPoolExecutor.getMaximumPoolSize());//8 System.out.println(threadPoolExecutor.getPoolSize());//1 int i = threadPoolExecutor.prestartAllCoreThreads(); System.out.println(threadPoolExecutor.getCorePoolSize());//4 System.out.println(threadPoolExecutor.getMaximumPoolSize());//8 System.out.println(threadPoolExecutor.getPoolSize());//4 } }  参数介绍 ThreadPoolExecutor的几个参数是必须要清楚的：</description>
    </item>
    
    <item>
      <title>js避免快速点击</title>
      <link>https://wenchao.ren/posts/js%E9%81%BF%E5%85%8D%E5%BF%AB%E9%80%9F%E7%82%B9%E5%87%BB/</link>
      <pubDate>Thu, 29 Aug 2019 20:50:40 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/js%E9%81%BF%E5%85%8D%E5%BF%AB%E9%80%9F%E7%82%B9%E5%87%BB/</guid>
      <description>通过JavaScript避免按钮快速被点击，有一般都是通过修改状态，然后延迟恢复状态来弄的，demo如下：
onRefresh: function () { $(&amp;quot;button[name=&#39;refresh&#39;]&amp;quot;).attr(&#39;disabled&#39;, true); $(&#39;#jar-debug-table&#39;).bootstrapTable(&#39;removeAll&#39;); getAllClass(); setTimeout(function () { $(&amp;quot;button[name=&#39;refresh&#39;]&amp;quot;).attr(&#39;disabled&#39;, false); }, 3000); }  </description>
    </item>
    
    <item>
      <title>关于elasticsearch的access log</title>
      <link>https://wenchao.ren/posts/%E5%85%B3%E4%BA%8Eelasticsearch%E7%9A%84access-log/</link>
      <pubDate>Fri, 09 Aug 2019 10:49:39 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%85%B3%E4%BA%8Eelasticsearch%E7%9A%84access-log/</guid>
      <description>今天才知道原来elasticsearch其实并没有传统意义上的access log。我这里说的传统意义 上的access log是指类似于tomcat，nginx等的access log。 我们公司在访问elasticsearch的时候，java语言相关的使用的公司封装的（其实是我封装的） elasticsearch client，在这个封装的client里面，实现了传统意义上的类似于dubbo-consumer-access.log 一样的东西，记录了请求时间，traceId，请求的参数信息，请求是否成功的状态，响应时间，响应的内容等等 的信息。但是在elasticsearch的server端其实是并没有这些内容的。 Google了一下，发现elasticsearch其实并没有access log这个功能。但是elasticsearch缺提供了slow log的这个功能。 虽然说slow log和access log其实是两种不同的东西，但是在一定程度上slow log其实可以做到access log的一部分功能。
elasticsearch的slow log 这个日志的目的是捕获那些超过指定时间阈值的查询和索引请求。这个日志用来追踪由用户产生的很慢的请求很有用。 默认情况，慢日志是不开启的。要开启它，需要定义具体动作（query，fetch 还是 index），你期望的事件记录等级（ WARN 、 DEBUG 等），以及时间阈值。
query query阶段的配置，日志记录在_index_isearch_slowlog.log结尾的文件中，下面的日志级别可以根据实际 的需求来选择，如果想关闭某个配置的话，使用#号注释，或者设置值为-1就行。当然需要重启elasticsearch 以使得配置修改生效。
 index.search.slowlog.threshold.query.warn: 10s #超过10秒的query产生1个warn日志 index.search.slowlog.threshold.query.info: 5s #超过5秒的query产生1个info日志 index.search.slowlog.threshold.query.debug: 2s #超过2秒的query产生1个debug日志 index.search.slowlog.threshold.query.trace: 500ms #超过500毫秒的query产生1个trace日志  fetch fetch阶段的配置
 index.search.slowlog.threshold.fetch.warn: 1s index.search.slowlog.threshold.fetch.info: 800ms index.search.slowlog.threshold.fetch.debug: 500ms index.search.slowlog.threshold.fetch.trace: 200ms  index 索引阶段的配置
 index.indexing.slowlog.threshold.index.warn: 10s ##索引数据超过10秒产生一个warn日志 index.indexing.slowlog.threshold.index.info: 5s ##索引数据超过5秒产生一个info日志 index.indexing.slowlog.threshold.index.debug: 2s ##索引数据超过2秒产生一个ddebug日志 index.</description>
    </item>
    
    <item>
      <title>dubbo xml解析</title>
      <link>https://wenchao.ren/posts/dubbo-xml%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Thu, 08 Aug 2019 00:46:01 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/dubbo-xml%E8%A7%A3%E6%9E%90/</guid>
      <description>在之前的文章《如何在spring中自定义xml标签并解析》中我用实际的例子展示了，如何在spring中自定义xml标签，同时如何解析这个xml标签。
本篇文章主要来看看dubbo中对应的源代码
如上图所示，这些类就是dubbo解析xml的相关实现的核心类，核心原理已经在之前的文章中详细描述了，如果大家在阅读dubbo代码，想看某个element的解析类的话，可以在DubboNamespaceHandler中来找：
public class DubboNamespaceHandler extends NamespaceHandlerSupport { static { Version.checkDuplicate(DubboNamespaceHandler.class); } @Override public void init() { registerBeanDefinitionParser(&amp;quot;application&amp;quot;, new DubboBeanDefinitionParser(ApplicationConfig.class, true)); registerBeanDefinitionParser(&amp;quot;module&amp;quot;, new DubboBeanDefinitionParser(ModuleConfig.class, true)); registerBeanDefinitionParser(&amp;quot;registry&amp;quot;, new DubboBeanDefinitionParser(RegistryConfig.class, true)); registerBeanDefinitionParser(&amp;quot;config-center&amp;quot;, new DubboBeanDefinitionParser(ConfigCenterBean.class, true)); registerBeanDefinitionParser(&amp;quot;metadata-report&amp;quot;, new DubboBeanDefinitionParser(MetadataReportConfig.class, true)); registerBeanDefinitionParser(&amp;quot;monitor&amp;quot;, new DubboBeanDefinitionParser(MonitorConfig.class, true)); registerBeanDefinitionParser(&amp;quot;metrics&amp;quot;, new DubboBeanDefinitionParser(MetricsConfig.class, true)); registerBeanDefinitionParser(&amp;quot;provider&amp;quot;, new DubboBeanDefinitionParser(ProviderConfig.class, true)); registerBeanDefinitionParser(&amp;quot;consumer&amp;quot;, new DubboBeanDefinitionParser(ConsumerConfig.class, true)); registerBeanDefinitionParser(&amp;quot;protocol&amp;quot;, new DubboBeanDefinitionParser(ProtocolConfig.class, true)); registerBeanDefinitionParser(&amp;quot;service&amp;quot;, new DubboBeanDefinitionParser(ServiceBean.class, true)); registerBeanDefinitionParser(&amp;quot;reference&amp;quot;, new DubboBeanDefinitionParser(ReferenceBean.class, false)); registerBeanDefinitionParser(&amp;quot;annotation&amp;quot;, new AnnotationBeanDefinitionParser()); } }  基本上就是一个xml element对应一个BeanDefinitionParser</description>
    </item>
    
    <item>
      <title>如何在spring中自定义xml标签并解析</title>
      <link>https://wenchao.ren/posts/%E5%A6%82%E4%BD%95%E5%9C%A8spring%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89xml%E6%A0%87%E7%AD%BE%E5%B9%B6%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Thu, 08 Aug 2019 00:29:14 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%A6%82%E4%BD%95%E5%9C%A8spring%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89xml%E6%A0%87%E7%AD%BE%E5%B9%B6%E8%A7%A3%E6%9E%90/</guid>
      <description>如果大家使用过dubbo那么大概率看见过&amp;lt;dubbo:application ...&amp;gt;类似的配置。这其实就是一种xml标签的自定义，当然dubbo的实现中也会有自己的解析。
这篇文章主要就说一下xml标签的自定义和解析。本篇文章中的代码仓库地址为：https://github.com/xkrivzooh/spring-define-and-parse-example
大家按照上面的demo例子跑一下就会明白完整流程。其中有一些注意点我列了一下：
 .xsd文件中的targetNamespace定义了以后，后续其他的比如xmlns的值，spring.handlers以及spring.schemas中的值需要对应上 xsd:element定义的就是将来会在xml文件中用到的元素，例如&amp;lt;dubbo:application&amp;gt;中的application xsd:attribute定义的就是模型类中的属性，例如&amp;lt;dubbo:application name=&amp;quot;xxx&amp;quot;&amp;gt;中的name，并且可以指定属性类型，进而起到检测的作用（当我们定义的是int，如果在xml中的值是非int型的，直接会报错）。 通常为每一个xsd:element都要注册一个BeanDefinitionParser。 person-demo.xml中的&amp;lt;AnyStringYouWant:person name=&amp;quot;name1&amp;quot; age=&amp;quot;1&amp;quot;/&amp;gt;中的AnyStringYouWant你可以随意替换  </description>
    </item>
    
    <item>
      <title>dubbo 线程模型</title>
      <link>https://wenchao.ren/posts/dubbo-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Wed, 07 Aug 2019 12:04:36 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/dubbo-%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/</guid>
      <description>dubbo的线程模型设计的算是非常不错的了，值得我们学习。下图是dubbo的线程模型图：
IO线程和业务线程的选择原则 在官方文档中，对于线程模型的选择说的比较清楚：
 如果事件处理的逻辑能迅速完成，并且不会发起新的 IO 请求，比如只是在内存中记个标识，则直接在 IO 线程上处理更快，因为减少了线程池调度。 但如果事件处理逻辑较慢，或者需要发起新的 IO 请求，比如需要查询数据库，则必须派发到线程池，否则 IO 线程阻塞，将导致不能接收其它请求。如果用 IO 线程处理事件，又在事件处理过程中发起新的 IO 请求，比如在连接事件中发起登录请求，会报“可能引发死锁”异常，但不会真死锁。  我们平时在编写代码的时候，也需要遵循这个大原则。
dubbo的线程模型配置起来也是比较简单的：
&amp;lt;dubbo:protocol name=&amp;quot;dubbo&amp;quot; dispatcher=&amp;quot;all&amp;quot; threadpool=&amp;quot;fixed&amp;quot; threads=&amp;quot;100&amp;quot; /&amp;gt;  比如上面的配置中，dubbo协议使用了all Dispatcher，内部使用固定100大小的线程池。
根据上面的线程模型图来看，当dubbo provider收到dubbo consumer的请求以后，会通过Dispather模块来进行请求分发，在这个Dispather模块中决定了dubbo的部分功能使用哪个线程池。然后在ThreadPool模块中提供了好几个线程池实现。基本上算是覆盖到了绝大多数场景。
dubbo Dispatcher分类  all 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。 direct 所有消息都不派发到线程池，全部在 IO 线程上直接执行。 message 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 execution 只请求消息派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 connection 在 IO 线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池。  默认情况下的dispatcher为all，这样的话可以尽可能的提示吞吐量。
dubbo的ThreadPool  fixed 固定大小线程池，启动时建立线程，不关闭，一直持有。(缺省) cached 缓存线程池，空闲一分钟自动删除，需要时重建。 limited 可伸缩线程池，但池中的线程数只会增长不会收缩。只增长不收缩的目的是为了避免收缩时突然来了大流量引起的性能问题。 eager 优先创建Worker线程池。在任务数量大于corePoolSize但是小于maximumPoolSize时，优先创建Worker来处理任务。当任务数量大于maximumPoolSize时，将任务放入阻塞队列中。阻塞队列充满时抛出RejectedExecutionException。(相比于cached:cached在任务数量超过maximumPoolSize时直接抛出异常而不是将任务放入阻塞队列)  从dispatcher和threadpool模块可以看出，dubbo之所以搞这么多dispatcher和threadpool的实现，就是基于线程模型的选择大原则，使得可以针对不同的业务场景，业务使用方可以自主选择不同的实现。从这一点上来说，作为一个rpc框架，dubbo在这方面的考量和实现算是非常不错的了。
源码阅读 dubbo的线程模型相关的代码，我们从org.apache.dubbo.remoting.transport.netty4.NettyServer这个类开始看，在NettyServer中：</description>
    </item>
    
    <item>
      <title>不规范的查询导致elasticsearch StackOverflowError</title>
      <link>https://wenchao.ren/posts/%E4%B8%8D%E8%A7%84%E8%8C%83%E7%9A%84%E6%9F%A5%E8%AF%A2%E5%AF%BC%E8%87%B4elasticsearch-stackoverflowerror/</link>
      <pubDate>Tue, 06 Aug 2019 19:04:33 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%B8%8D%E8%A7%84%E8%8C%83%E7%9A%84%E6%9F%A5%E8%AF%A2%E5%AF%BC%E8%87%B4elasticsearch-stackoverflowerror/</guid>
      <description>昨天我们公司的elasticsearch的集群有几个节点出现了StackOverflowError，然后es进程退出的问题。最终排查发现导致es节点出现下面的异常：
[2019-08-05T20:31:35,367][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [order6] fatal error in thread [elasticsearch[order6][search][T#38]], exiting java.lang.StackOverflowError: null at org.apache.lucene.store.DataInput.readVLong(DataInput.java:184) ~[lucene-core-6.3.0.jar:6.3.0 a66a44513ee8191e25b477372094bfa846450316 - shalin - 2016-11-02 19:47:11] at org.apache.lucene.store.DataInput.readVLong(DataInput.java:169) ~[lucene-core-6.3.0.jar:6.3.0 a66a44513ee8191e25b477372094bfa846450316 - shalin - 2016-11-02 19:47:11] at org.apache.lucene.util.fst.FST.readUnpackedNodeTarget(FST.java:931) ~[lucene-core-6.3.0.jar:6.3.0 a66a44513ee8191e25b477372094bfa846450316 - shalin - 2016-11-02 19:47:11] at org.apache.lucene.util.fst.FST.readNextRealArc(FST.java:1143) ~[lucene-core-6.3.0.jar:6.3.0 a66a44513ee8191e25b477372094bfa846450316 - shalin - 2016-11-02 19:47:11] at org.apache.lucene.util.fst.FST.readFirstRealTargetArc(FST.java:992) ~[lucene-core-6.3.0.jar:6.3.0 a66a44513ee8191e25b477372094bfa846450316 - shalin - 2016-11-02 19:47:11] at org.apache.lucene.util.fst.FST.findTargetArc(FST.java:1270) ~[lucene-core-6.3.0.jar:6.3.0 a66a44513ee8191e25b477372094bfa846450316 - shalin - 2016-11-02 19:47:11] at org.apache.lucene.util.fst.FST.findTargetArc(FST.java:1186) ~[lucene-core-6.3.0.jar:6.3.0 a66a44513ee8191e25b477372094bfa846450316 - shalin - 2016-11-02 19:47:11] at org.</description>
    </item>
    
    <item>
      <title>java 常见的OOM case</title>
      <link>https://wenchao.ren/posts/java-%E5%B8%B8%E8%A7%81%E7%9A%84oom-case/</link>
      <pubDate>Tue, 06 Aug 2019 18:32:50 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java-%E5%B8%B8%E8%A7%81%E7%9A%84oom-case/</guid>
      <description>OOM是java.lang.OutOfMemoryError异常的简称，在日常工作中oom还算是比较常见的一种问题吧。出现OOM意味着jvm已经无法满足新对象对内存的申请了，本文整理了一下oom的常见case和一般情况下的解决方法。
处理OOM问题，绝大多数情况下jmap和MAT工具可以解决99%的问题。
Java heap space 表现现象为：
java.lang.OutOfMemoryError: Java heap space  可能的原因  内存泄漏 堆大小设置不合理 JVM处理引用不及时，导致内存无法释放 代码中可能存在大对象分配  解决办法  一般情况下，都是先通过jmap命令，把堆内存dump下来，使用mat工具分析一下，检查是否因为代码问题，存在内存泄露 也可能是下游服务出问题，导致内存中的数据不能很快的处理掉，进而引起oom 调整-Xmx参数，加大堆内存 还有一点容易被忽略，检查是否有大量的自定义的 Finalizable 对象，也有可能是框架内部提供的，考虑其存在的必要性  PermGen space 永久代是HotSot虚拟机对方法区的具体实现，存放了被虚拟机加载的类信息、常量、静态变量、JIT编译后的代码等。
一般情况下的异常表现为：
java.lang. OutOfMemoryError : PermGen space  可能的原因  在Java7之前，频繁的错误使用String.intern()方法 运行期间生成了大量的代理类，导致方法区被撑爆，无法卸载  解决办法  检查是否永久代空间是否设置的过小 检查代码中是否存错误的创建过多的代理类  Metaspace JDK8后，元空间替换了永久带，元空间使用的是本地内存，还有其它细节变化：
 字符串常量由永久代转移到堆中 和永久代相关的JVM参数已移除  一般情况下的异常表现为：
java.lang.OutOfMemoryError: Metaspace  可能的原因 类似PermGen space
解决办法  通过命令行设置 -XX: MaxMetaSpaceSize 增加 metaspace 大小，或者取消-XX: maxmetsspacedize 其他类似PermGen space  unable to create new native Thread 这种情况的一般表现为：</description>
    </item>
    
    <item>
      <title>使用maven-shade-plugin解决依赖冲突</title>
      <link>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8maven-shade-plugin%E8%A7%A3%E5%86%B3%E4%BE%9D%E8%B5%96%E5%86%B2%E7%AA%81/</link>
      <pubDate>Thu, 01 Aug 2019 13:00:38 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8maven-shade-plugin%E8%A7%A3%E5%86%B3%E4%BE%9D%E8%B5%96%E5%86%B2%E7%AA%81/</guid>
      <description>一般遇到一些比较复杂恶心的依赖冲突，传统的通过dependencyManagement和exclusion有时候是解不了的。这种问题最常见的是netty相关的。 这种情况下我们可以使用maven-shade-plugin插件。
比如我这边的一个case是我们依赖的redisson需要4.1.36.Final的netty，而我们公司其他的组件必须依赖4.0.46.Final的netty，而这2个版本的netty 是不兼容的，因此我当时就使用了maven-shade-plugin插件来解决这个问题。
因为我们公司内部的netty都是4.0.46.Final，因此我专门搞了一个redisson-shade的maven module，这个module只有pom.xml文件，在这个module中我们依赖 redisson，然后将redisson内部依赖的netty的package路径进行修改, 然后其他的module通过依赖redisson-shade的maven module就好了。
pom.xml文件内容为:
&amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.redisson&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;redisson&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; &amp;lt;build&amp;gt; &amp;lt;plugins&amp;gt; &amp;lt;plugin&amp;gt; &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;maven-shade-plugin&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.2.1&amp;lt;/version&amp;gt; &amp;lt;executions&amp;gt; &amp;lt;execution&amp;gt; &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt; &amp;lt;goals&amp;gt; &amp;lt;goal&amp;gt;shade&amp;lt;/goal&amp;gt; &amp;lt;/goals&amp;gt; &amp;lt;configuration&amp;gt; &amp;lt;relocations&amp;gt; &amp;lt;relocation&amp;gt; &amp;lt;pattern&amp;gt;io.netty&amp;lt;/pattern&amp;gt; &amp;lt;shadedPattern&amp;gt;com.xxx.io.netty.redisson&amp;lt;/shadedPattern&amp;gt; &amp;lt;/relocation&amp;gt; &amp;lt;/relocations&amp;gt; &amp;lt;/configuration&amp;gt; &amp;lt;/execution&amp;gt; &amp;lt;/executions&amp;gt; &amp;lt;/plugin&amp;gt; &amp;lt;/plugins&amp;gt; &amp;lt;finalName&amp;gt;redisson-shade&amp;lt;/finalName&amp;gt; &amp;lt;/build&amp;gt;  在上面的pom中，在package阶段，会把redisson依赖的netty的package路径从io.netty修改为com.xxx.io.netty.redisson。这样就解决了依赖冲突问题。
不过我是在同一个工程中搞多个maven module来弄的，因此本地测试时候，一般都需要专门搞一个测试工程，这个算是一个麻烦点。</description>
    </item>
    
    <item>
      <title>maven scope介绍</title>
      <link>https://wenchao.ren/posts/maven-scope%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Thu, 01 Aug 2019 12:03:33 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/maven-scope%E4%BB%8B%E7%BB%8D/</guid>
      <description>maven的scope有下面6种：
 test compile 默认scope runntime provided system import  下面我们分别说一下每个scope的含义
test scope为test表示依赖项目仅仅参与测试相关的工作，包括测试代码的编译，执行，test scope的依赖项不具有传递性，仅适用于测试和执行类路径。
一般像我们使用的junit的测试相关的jar都是使用test scope的，比如：
&amp;lt;!-- https://mvnrepository.com/artifact/junit/junit --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;4.12&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt;  compile maven的scope默认就是compile，什么都不配置也就是意味着compile。compile表示被依赖项目需要参与当前项目的编译，当然后续的测试，运行周期也参与其中 ，是一个比较强的依赖。打包的时候通常需要包含进去，具有依赖传递性。
比如：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;commons-lang&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;commons-lang&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.6&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt;  runntime runntime表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过编译而已，说实话在终端的项目（非开源，企业内部系统）中，和compile区别不是很大。compile只需要知道接口就足够了。mysql jdbc驱动架包就是一个很好的例子，一般scope为runntime。另外runntime的依赖通常和optional搭配使用，optional为true。我可以用A实现，也可以用B实现。
比较常见的mysql-connector-java一般就设置为runtime scope：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;6.0.6&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt;  provider provided意味着打包的时候可以不用包进去，别的设施(比如jdk或者其他的Web Container)会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是在打包阶段做了exclude的动作。 provider scope的一个很好的用例是部署在某个容器（如tomcat）中的Web应用程序，其中容器本身已经提供了一些库。比如常见的servlet-api一般就是provider的：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;javax.servlet&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;servlet-api&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.5&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt; &amp;lt;/dependency&amp;gt;  system 从参与度来说，也provided相同，不过被依赖项不会从maven仓库抓，而是从本地文件系统拿，一定需要配合systemPath属性使用。需要记住的重要一点是，如果不存在依赖关系或者位于与systemPath指向的位置不同的位置，则在不同的计算机上构建具有系统范围依赖关系的项目可能会失败：
一个例子：
&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.baeldung&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;custom-dependency&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;1.3.2&amp;lt;/version&amp;gt; &amp;lt;scope&amp;gt;system&amp;lt;/scope&amp;gt; &amp;lt;systemPath&amp;gt;${project.basedir}/libs/custom-dependency-1.3.2.jar&amp;lt;/systemPath&amp;gt; &amp;lt;/dependency&amp;gt;  import 此范围已在Maven 2.</description>
    </item>
    
    <item>
      <title>redis Pipeline</title>
      <link>https://wenchao.ren/posts/redis-pipeline/</link>
      <pubDate>Wed, 31 Jul 2019 12:42:29 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/redis-pipeline/</guid>
      <description>在基于request-response的请求模型中，一般都会涉及下面几个阶段：
 client发送命令 命令在网络上传输 server收到命令并开始执行 server返回结果  在这个过程中我们可以看到有2次网络传输，而这两次网络传输的耗时成为：RTT (Round Trip Time)。例如，如果 RTT 时间是250毫秒（网络连接很慢的情况下）， 即使服务端每秒能处理100k的请求量，那我们每秒最多也只能处理4个请求。如果使用的是本地环回接口，RTT 就短得多，但如如果需要连续执行多次写入，这也是一笔很大的开销。下面的图是传统的 N次request-response的交互图：
 一般情况下我们为了解决rtt耗时太长的问题，会采样批处理的解决方案，也就是将请求参数批量发给server端，server端处理完这些请求以后，在一次性返回结果。
 在redis中，已经提供了一些批量操作命令，比如mget，mset等命令。但是也有不少命令是没有批量操作命令的，但是为了解决这个问题，redis支持Pipeline。
Pipeline 并不是一种新的技术或机制，很多技术上都使用过。RTT 在不同网络环境下会不同，例如同机房和同机房会比较快，跨机房跨地区会比较慢。Redis 很早就支持 Pipeline 技术，因此无论你运行的是什么版本，你都可以使用 Pipeline 操作 Redis。如果客户端和服务端的网络延时越大，那么Pipeline的效果越明显。
Pipeline 能将一组 Redis 命令进行组装，通过一次 RTT 传输给 Redis，再将这组 Redis 命令按照顺序执行并将结果返回给客户端。上图没有使用 Pipeline 执行了 N 条命令，整个过程需要 N 次 RTT。下图为使用 Pipeline 执行 N 条命令，整个过程仅需要 1 次 RTT：
Pipeline 基本使用 我比较喜欢用的lettuce中对pipeline的使用方式（Asynchronous Pipelining）如下：
StatefulRedisConnection&amp;lt;String, String&amp;gt; connection = client.connect(); RedisAsyncCommands&amp;lt;String, String&amp;gt; commands = connection.async(); // disable auto-flushing commands.setAutoFlushCommands(false); // perform a series of independent calls List&amp;lt;RedisFuture&amp;lt;?</description>
    </item>
    
    <item>
      <title>redisson redlock代码阅读</title>
      <link>https://wenchao.ren/posts/redisson-redlock%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/</link>
      <pubDate>Mon, 29 Jul 2019 12:15:16 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/redisson-redlock%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/</guid>
      <description>本文章未完，待续
redisson redlock基本使用 RLock lock1 = redissonInstance1.getLock(&amp;quot;lock1&amp;quot;); RLock lock2 = redissonInstance2.getLock(&amp;quot;lock2&amp;quot;); RLock lock3 = redissonInstance3.getLock(&amp;quot;lock3&amp;quot;); RedissonRedLock lock = new RedissonRedLock(lock1, lock2, lock3); // 同时加锁：lock1 lock2 lock3 // 红锁在大部分节点上加锁成功就算成功。 lock.lock(); ... lock.unlock();  另外Redisson还通过加锁的方法提供了leaseTime的参数来指定加锁的时间。超过这个时间后锁便自动解开了。
RedissonRedLock lock = new RedissonRedLock(lock1, lock2, lock3); // 给lock1，lock2，lock3加锁，如果没有手动解开的话，10秒钟后将会自动解开 lock.lock(10, TimeUnit.SECONDS); // 为加锁等待100秒时间，并在加锁成功10秒钟后自动解开 boolean res = lock.tryLock(100, 10, TimeUnit.SECONDS); ... lock.unlock();  源码阅读 RedissonRedLock类继承了RedissonMultiLock，基于redlock算法，这个类重写了RedissonMultiLock的failedLocksLimit和calcLockWaitTime方法
public class RedissonRedLock extends RedissonMultiLock { /** * Creates instance with multiple {@link RLock} objects.</description>
    </item>
    
    <item>
      <title>Select、poll、Epoll、KQueue区别</title>
      <link>https://wenchao.ren/posts/selectepollkqueue%E5%8C%BA%E5%88%AB/</link>
      <pubDate>Fri, 26 Jul 2019 23:00:48 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/selectepollkqueue%E5%8C%BA%E5%88%AB/</guid>
      <description>在早期的文章《unix IO模型》中我们介绍了5种IO模型，如下图是几种IO模型的对比
从上面的图可以看出，从左到右，越往后，阻塞越少，理论上效率也就越优。
其中Select对应的是第三种IO模型：I/O Multiplexing IO多路复用模型，而epoll与kqueue其实和Select一样也属于I/O Multiplexing IO多路复用模型，只是相比于select来说多了一些高级特性而已，可以看做拥有了第四种模型的某些特性，比如callback的回调机制。
IO多路复用的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll，kqueue这些个function会不断的轮询所负责的所有socket，当某个socket就绪（一般是读就绪或者写就绪），就通知用户进程。
I/O Multiplexing IO多路复用模型 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。
所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回
更详细的描述可以查看《/unix-IO模型/#I-O-多路复用（-IO-multiplexing）》
Select int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);  select 函数监视的文件描述符分3类，分别是:
 writefds readfds exceptfds  调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。
select的优缺点 优点  跨平台  select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。    缺点  单个进程打开的文件描述是有一定限制的，它由FD_SETSIZE设置，默认值是1024，采用数组存储，虽然可以通过编译内核改变，但相对麻烦。 另外在检查数组中是否有文件描述需要读写时，采用的是线性扫描的方法，即不管这些socket是不是活跃的，我都轮询一遍，所以效率比较低  poll int poll (struct pollfd *fds, unsigned int nfds, int timeout);  不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。
struct pollfd { int fd; /* file descriptor */ short events; /* requested events to watch */ short revents; /* returned events witnessed */ };  pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。</description>
    </item>
    
    <item>
      <title>Intellij IDEA 2019.2 对http client的功能增强</title>
      <link>https://wenchao.ren/posts/intellij-idea-2019-2-%E5%AF%B9http-client%E7%9A%84%E5%8A%9F%E8%83%BD%E5%A2%9E%E5%BC%BA/</link>
      <pubDate>Fri, 26 Jul 2019 12:33:52 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/intellij-idea-2019-2-%E5%AF%B9http-client%E7%9A%84%E5%8A%9F%E8%83%BD%E5%A2%9E%E5%BC%BA/</guid>
      <description>在Intellij IDEA 2019.2中对http client的功能进行了增强，主要有2个：
 HTTP client supports cURL requestsULTIMATE HTTP client keeps cookies  HTTP client supports cURL requestsULTIMATE Now you can paste a cURL request string into the HTTP client and have the IDE automatically convert it to a full request.
HTTP client keeps cookies Suppose you’ve made one request to authenticate on the service, and in subsequent requests you would like to call some endpoints that require additional permissions.</description>
    </item>
    
    <item>
      <title>定时任务的常见触发方式</title>
      <link>https://wenchao.ren/posts/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%B8%B8%E8%A7%81%E8%A7%A6%E5%8F%91%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Wed, 24 Jul 2019 23:26:19 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%B8%B8%E8%A7%81%E8%A7%A6%E5%8F%91%E6%96%B9%E5%BC%8F/</guid>
      <description>中间件项目中，经常会有下面的场景：
 client的定时重试 client定时向server端发心跳包 server端对client的判活 &amp;hellip;  这种其实都是在某一个时间点触发一些任务，但是当任务量很大时，怎么做比较高效呢？
比如client定时向server发心跳包，在server端如何对client进行判活呢？一般我们的做法主要有下面的几种，当任务量很大的时候我们一般都会采样环形队列/HashedWheelTimer方法。
轮询扫描 轮询扫描是最简单的处理方式，也非常的常见：
 用一个Map来记录每一个client最近一次请求时间last_packet_time 当client有请求包来到，实时更新这个Map 同时有一个线程来专门的不断扫描这个map，比如当检查client的last_packet_time是否超过30s，如果超过则进行超时处理  多timer触发  用一个Map来记录每一个client最近一次请求时间last_packet_time 当某个client有请求包来到，实时更新这个Map，并同时对这个client的请求包启动一个timer，30s之后触发 每个client的请求包对应的timer触发后，看Map中，查看这个client的last_packet_time是否超过30s，如果超过则进行超时处理  环形队列/HashedWheelTimer 这种方案简单描述如下：
 环形队列，本质是一个数组，比如30s超时，就创建一个index从0到30的环形队列 环上每一个slot是一个Set，表示：任务集合 同时还有一个Map，记录某个client落在环上的哪个slot里 同时启动一个timer，每隔1s，在上述环形队列中移动一格，移动到数组最后一个元素时候，又从第一个开始，也就是：0-&amp;gt;1-&amp;gt;2-&amp;gt;3…-&amp;gt;29-&amp;gt;30-&amp;gt;0… 有一个Current Index指针来标识刚检测过的slot  当某client有请求包到达时：
 从Map结构中，查找出这个client存储在哪一个slot里 从这个slot的Set结构中，删除这个client 将client重新加入到新的slot中，具体是哪一个slot呢 =&amp;gt; Current Index指针所指向的上一个slot，因为这个slot，会被timer在30s之后扫描到 更新Map，这个client对应slot的index值  哪些元素会被超时掉呢？
因为Current Index每秒种移动一个slot，这个slot对应的Set中所有client都应该被集体超时！如果最近30s有请求包来到，一定被放到Current Index的前一个slot了，Current Index所在的slot对应Set中所有元素，都是最近30s没有请求包来到的。所以，当没有超时时，Current Index扫到的每一个slot的Set中应该都没有元素。
优势：
 只需要1个timer timer每1s只需要一次触发，消耗CPU很低 批量超时，Current Index扫到的slot，Set中所有元素都应该被超时掉  </description>
    </item>
    
    <item>
      <title>构造函数中使用Spring  @Value注解</title>
      <link>https://wenchao.ren/posts/%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E4%B8%AD%E4%BD%BF%E7%94%A8spring-value%E6%B3%A8%E8%A7%A3/</link>
      <pubDate>Tue, 23 Jul 2019 20:39:04 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%E4%B8%AD%E4%BD%BF%E7%94%A8spring-value%E6%B3%A8%E8%A7%A3/</guid>
      <description>如果想在构造函数中使用的@value注解的话，demo如下：
// File: sample/Message.groovy package sample import org.springframework.beans.factory.annotation.* import org.springframework.stereotype.* @Component class Message { final String text // Use @Autowired to get @Value to work. @Autowired Message( // Refer to configuration property // app.message.text to set value for // constructor argument message. @Value(&#39;${app.message.text}&#39;) final String text) { this.text = text } }  </description>
    </item>
    
    <item>
      <title>bloom filter</title>
      <link>https://wenchao.ren/posts/bloom-filter/</link>
      <pubDate>Mon, 22 Jul 2019 12:53:50 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/bloom-filter/</guid>
      <description>当系统设计中出现多级缓存结构时，为了防止大量不存在的key值击穿高速缓存（比如主存），去直接访问低速缓存（如本地磁盘），我们一般需要将这部分key值，直接拦截在高速缓存阶段。这里，当然可以使用普通的hash table，也可以使用bitmap，但是这两种方式都比较耗费内存，当面对海量key值时，问题会变得更加严重。这时，就该介绍我们的主角bloom filter出场了。
一般的，bloom filter用于判断一个key值是否在一个set中，拥有比hash table/bitmap更好的空间经济性。如果bloom filter指示一个key值“不在”一个set中，那么这个判断是100%准确的。这样的特性，非常适合于上述的缓存场景。
bloom filter原理   首先估计要判断的set中的元素个数N，然后选定k个独立的哈希函数。根据N和k，选定一个长度为M的bit array。
  遍历set中的N个元素
 对每个元素，使用k个哈希函数，得到k个哈希值（一般为一个大整数） 将上述bit array中，k个哈希值所对应的bit置1    对于需要判断的key值
 使用k个哈希函数，得到k个哈希值 如果k个哈希值所对应的bit array中的值均为1，则判断此值在set中“可能”存在；否则，判定“一定”不存在    根据上面的原理我们其实可以看到，bloom filter有以下特点：
 比较节省空间 bloom的识别准确率和数据大小，k个哈希函数有关 如果bloom filter判断key不存在，那么就一定不存在，100%不存在。 如果bloom filter判断key存在，那么可能存在，也可能不存在  bloom filter优缺点 优点：   插入、查找都是常数时间
  多个hash函数之间互相独立，可以并行计算
  不需要存储元素本身，从而带来空间效率优势，以及一些保密上的优势
  bloom filter的bitmap可以进行交、并、差运算
  缺点：  判断元素是否在集合中的结果其实是不准确的 bloom filter中的元素是不能删除的  bloom filter的实际使用 guava bloom filter guava中提供了bloom filter的一种实现:com.</description>
    </item>
    
    <item>
      <title>推荐一些前端组件</title>
      <link>https://wenchao.ren/posts/%E6%8E%A8%E8%8D%90%E4%B8%80%E4%BA%9B%E5%89%8D%E7%AB%AF%E7%BB%84%E4%BB%B6/</link>
      <pubDate>Mon, 08 Jul 2019 13:02:25 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E6%8E%A8%E8%8D%90%E4%B8%80%E4%BA%9B%E5%89%8D%E7%AB%AF%E7%BB%84%E4%BB%B6/</guid>
      <description>我个人的前端水平其实是很挫的，但是据我所知很多公司的内部系统大多数是没有前端fe同学支持的，一般都是自己写的，这个时候一个文档健全，demo完善，上手简单的的前端组件库可以说是一大利器。因此这篇文章推荐一下我最近使用的比较不错的前端组件
bizcharts 官方地址：https://bizcharts.net/index
基于商业场景下的数据可视化解决方案，BizCharts主打电商业务图表可视化，沉淀电商业务线的可视化规范。在 React 项目中实现常见图表和自定义图表。
bizcharts支持桌面端和移动端，demo非常的丰富。与其是有现成的例子可以直接修改使用, 比如它的图标示例:
iceworks 官方地址：https://ice.work/iceworks
iceworks：让前端工程变的轻松便捷
参考资料  bizcharts iceworks  </description>
    </item>
    
    <item>
      <title>使用阿里云maven镜像加速</title>
      <link>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91maven%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F/</link>
      <pubDate>Mon, 08 Jul 2019 12:42:09 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91maven%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F/</guid>
      <description>maven是一个好东西，但是默认情况下，maven使用的是中央仓央是：http://repo1.maven.org/maven2和http://uk.maven.org/maven2。这两个镜像在国内 访问其实是比较慢的，因此我们需要尽可能使用国内同步好的镜像。
我在国内选择的是阿里云的镜像：公共代理库
maven的配置为：打开maven的配置文件(windows机器一般在maven安装目录的conf/settings.xml)，在&amp;lt;mirrors&amp;gt;&amp;lt;/mirrors&amp;gt;标签中添加mirror子节点:
&amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;aliyunmaven&amp;lt;/id&amp;gt; &amp;lt;mirrorOf&amp;gt;*&amp;lt;/mirrorOf&amp;gt; &amp;lt;name&amp;gt;阿里云公共仓库&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;https://maven.aliyun.com/repository/public&amp;lt;/url&amp;gt; &amp;lt;/mirror&amp;gt;  其他的如gradle的配置指南请参见公共代理库中描述的那样操作就好了。
但是一般情况下在公司开发的时候，公司也会有自己的maven镜像仓库，这个时候搞多个mirror就好了。
参考资料：  Maven镜像地址大全  </description>
    </item>
    
    <item>
      <title>java.nio.ByteBuffer</title>
      <link>https://wenchao.ren/posts/java-nio-bytebuffer/</link>
      <pubDate>Tue, 02 Apr 2019 19:50:21 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java-nio-bytebuffer/</guid>
      <description>Java NIO Buffers用于和NIO Channel交互。 我们从Channel中读取数据到buffers里，从Buffer把数据写入到Channels。Buffer本质上就是一块内存区，可以用来写入数据，并在稍后读取出来。这块内存被NIO Buffer包裹起来，对外提供一系列的读写方便开发的接口。java中java.nio.Buffer的常见实现类如下，不过我们这里只说一下ByteBuffer这个实现。
Buffer的重要属性 Buffer缓冲区实质上就是一块内存，用于写入数据，也供后续再次读取数据，为了便于理解，你可以把它理解为一个字节数组。它有有四个重要属性：
public abstract class Buffer { // Invariants: mark &amp;lt;= position &amp;lt;= limit &amp;lt;= capacity private int mark = -1; private int position = 0; private int limit; private int capacity; }   capacity  这个属性表示这个Buffer最多能放多少数据，在创建buffer的时候指定。int类型。   position 下一个要读写的元素位置（从0开始），当使用buffer的相对位置进行读/写操作时，读/写会从这个下标进行，并在操作完成后，buffer会更新下标的值。  写模式：当写入数据到Buffer的时候需要从一个确定的位置开始，默认初始化时这个位置position为0，一旦写入了数据比如一个字节，整形数据，那么position的值就会指向数据之后的一个单元，position最大可以到capacity-1. 读模式：当从Buffer读取数据时，也需要从一个确定的位置开始。buffer从写入模式变为读取模式时，position会归0，每次读取后，position向后移动。   limit 在Buffer上进行的读写操作都不能越过这个limit。  写模式：limit的含义是我们所能写入的最大数据量，它等同于buffer的容量capacity 读模式：limit则代表我们所能读取的最大数据量，他的值等同于写模式下position的位置。换句话说，您可以读取与写入数量相同的字节数。   mark  一个临时存放的位置下标，用户选定的position的前一个位置或-1。  调用mark()会将mark设为当前的position的值，以后调用reset()会将position属性设 置为mark的值。mark的值总是小于等于position的值，如果将position的值设的比mark小，当前的mark值会被抛弃掉。      注：</description>
    </item>
    
    <item>
      <title>确保数据落盘</title>
      <link>https://wenchao.ren/posts/%E7%A1%AE%E4%BF%9D%E6%95%B0%E6%8D%AE%E8%90%BD%E7%9B%98/</link>
      <pubDate>Sat, 30 Mar 2019 10:58:06 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E7%A1%AE%E4%BF%9D%E6%95%B0%E6%8D%AE%E8%90%BD%E7%9B%98/</guid>
      <description>在之前的文章《unix IO模型》我们曾经提到过，用户空间，内核空间，缓存IO等概念。关于这些概念，大家可以阅读这篇文章，在本篇文章中，我们就不在涉及这些概念了。
IO缓冲机制 大家需要有一个认知就是我们平时写的程序，在将数据到文件中时，其实数据不会立马写入磁盘中进行持久化存储的，而是会经过层层缓存，如下图所示：
其中这每层缓存都有自己的刷新时机，每层缓存都刷新后才会写入磁盘进行持久化存储。这些缓存的存在目的本意都是为了加速读写操作，因为如果每次读写都对应真实磁盘操作，那么读写的效率会大大降低。但是同样带来的坏处是如果期间发生掉电或者别的故障，还未写入磁盘的数据就丢失了。对于数据安全敏感的应用，比如数据库，比如交易程序，这是无法忍受的。所以操作系统提供了保证文件落盘的机制。
在上面这图中说明了操作系统到磁盘的数据流，以及经过的缓冲区。首先数据会先存在于应用的内存空间，如果调用库函数写入，库函数可能还会把数据缓存在库函数所维护的缓冲区空间中，比如C标准库stdio提供的方法就会进行缓存，目的是为了减少系统调用的次数。这两个缓存都是在用户空间中的。库函数缓存flush时，会调用write系统调用将数据写入内核空间，内核同样维护了一个页缓存（page cache），操作系统会在合适的时间把脏页的数据写入磁盘。即使是写入磁盘了，磁盘也可能维护了一个缓存，在这个时候掉电依然会丢失数据的，只有写入了磁盘的持久存储物理介质上，数据才是真正的落盘了，是安全的。
比如在网络套接字上侦听连接并将从每个客户端接收的数据写入文件的应用程序。 在关闭连接之前，服务器确保将接收到的数据写入稳定存储器，并向客户端发送此类确认，请看下面的简化代码(代码中已经注释)：
int sock_read(int sockfd, FILE *outfp, size_t nrbytes) { int ret; size_t written = 0; //example of an application buffer char *buf = malloc(MY_BUF_SIZE); if (!buf) return -1; //take care of reading the data from the socket //and writing it to the file stream while (written &amp;lt; nrbytes) { ret = read(sockfd, buf, MY_BUF_SIZE); if (ret =&amp;lt; 0) { if (errno == EINTR) continue; return ret; } written += ret; ret = fwrite((void *)buf, ret, 1, outfp); if (ret !</description>
    </item>
    
    <item>
      <title>unix IO模型</title>
      <link>https://wenchao.ren/posts/unix-io%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Fri, 29 Mar 2019 00:11:52 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/unix-io%E6%A8%A1%E5%9E%8B/</guid>
      <description>在之前的文章《理解同步、异步、阻塞和非阻塞》我们谈了一下关于同步、异步、阻塞和非阻塞的理解。这篇文章，我打算来谈谈unix的io模型，其中会涉及到下面的内容：
 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 异步 I/O（asynchronous IO） 信号驱动式IO模型(signal-driven IO model)  背景知识 在开始正式的介绍unix的io模型之前，我们需要科普一些背景知识，便于大家正确的理解unix io模型。
同步、异步、阻塞和非阻塞 这些概念请查看我之前的文章《理解同步、异步、阻塞和非阻塞》
文件描述符fd 文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。
文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。
用户空间（user space）与内核空间（kernel space） 学习 Linux 时，经常可以看到两个词：User space（用户空间）和 Kernel space（内核空间）。
简单说，Kernel space 是 Linux 内核的运行空间，User space 是用户程序的运行空间。为了安全，它们是隔离的，即使用户的程序崩溃了，内核也不受影响。
Kernel space 可以执行任意命令，调用系统的一切资源；User space 只能执行简单的运算，不能直接调用系统资源，必须通过系统接口（又称 system call），才能向内核发出指令。
str = &amp;quot;my string&amp;quot; // 用户空间 x = x + 2 // 用户空间 file.write(str) // 切换到内核空间 y = x + 4 // 切换回用户空间  上面代码中，第一行和第二行都是简单的赋值运算，在 User space 执行。第三行需要写入文件，就要切换到 Kernel space，因为用户不能直接写文件，必须通过内核安排。第四行又是赋值运算，就切换回 User space。</description>
    </item>
    
    <item>
      <title>理解同步、异步、阻塞和非阻塞</title>
      <link>https://wenchao.ren/posts/%E7%90%86%E8%A7%A3%E5%90%8C%E6%AD%A5%E5%BC%82%E6%AD%A5%E9%98%BB%E5%A1%9E%E5%92%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/</link>
      <pubDate>Wed, 27 Mar 2019 19:42:17 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E7%90%86%E8%A7%A3%E5%90%8C%E6%AD%A5%E5%BC%82%E6%AD%A5%E9%98%BB%E5%A1%9E%E5%92%8C%E9%9D%9E%E9%98%BB%E5%A1%9E/</guid>
      <description>关于同步、异步、阻塞和非阻塞这个概念性问题，这可能是非常容易混淆的概念之一，特别是那些刚开始解除网络编程的人来说。本篇文章争取来说清楚这个问题，如果有错误之处，恳请批评指正。
写在前面 首先大家心中需要有以下的清晰认知：
 阻塞操作不等于同步（blocking operation does NOT equal to synchronous） 非阻塞操作不等于异步（non-blocking operation does NOT equal to asynchronous）  事实上，同步异步于阻塞和非阻塞没有什么直接的关联关系。
同步和异步 同步和异步关注的是 通信机制 (communication mechanism)
 同步是指在发出一个function调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到调用结果了。这个结果可能是一个正确的期望结果，也可能是因为异常原因（比如超时）导致的失败结果。换句话说，就是由调用者主动等待这个调用的结果。   Synchronous is, when we started a function call, the call will not return anything until it gets the result, the function needs to finish everything before it can give anything to us.
  异步是调用在发出之后，本次调用过程就直接返回了，并没有同时没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态变化、事件通知等机制来通知调用者，或通过回调函数处理这个调用。   Asynchronous does not need to wait for the function completes its operation, once we call it, it returns immediately without any result, the function uses callback function (or other notification method) to &amp;ldquo;notify&amp;rdquo; us to get the value after it completes execution.</description>
    </item>
    
    <item>
      <title>集群调用容错的套路</title>
      <link>https://wenchao.ren/posts/%E9%9B%86%E7%BE%A4%E8%B0%83%E7%94%A8%E5%AE%B9%E9%94%99%E7%9A%84%E5%A5%97%E8%B7%AF/</link>
      <pubDate>Tue, 26 Mar 2019 00:33:17 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E9%9B%86%E7%BE%A4%E8%B0%83%E7%94%A8%E5%AE%B9%E9%94%99%E7%9A%84%E5%A5%97%E8%B7%AF/</guid>
      <description>在日常的工作和系统设计中，我们经常会使用RPC调用，而我们所部署的服务一般也都是集群模式。我们知道在分布式系统架构中，因为有很多的可能性，比如服务发布重启，网络抖动等问题，都可能会导致RPC调用失败，一般情况下我们的集群调用设计都需要有一定的容错策略。本篇文章就总结一下常见的集群调用容错套路：
 Failover Cluster Failfast Cluster Failsafe Cluster Failback Cluster Forking Cluster Broadcast Cluster  Failover Cluster Failover Cluster模式就是 失败自动切换，当出现失败，重试其它服务器，这种一般通常用于幂等操作，比如读操作，但重试会带来更长延迟。一般实现这种模式的时候，需要注意的是重试的时候优先剔除刚刚出问题的节点，优先选择其余节点。
Failfast Cluster Failfast Cluster是快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。
Failsafe Cluster Failfast Cluster是失败安全，出现异常时，直接忽略，就是fire and forget。比如一些场景下写入审计日志等操作，失败了也就失败了，可以忍受。
Failback Cluster Failback Cluster是失败自动恢复，异步记录失败请求，定时重发。通常用于消息通知操作。
Forking Cluster Forking Cluster 并行调用多个服务器，只要其中一个成功即返回。这种通常用于实时性要求较高的读操作，但需要浪费更多服务资源。
Broadcast Cluster Broadcast Cluster是广播调用。就是广播请求到所有提供者，逐个调用，任意一台报错则报错，通常用于通知所有提供者更新缓存或日志等本地资源信息。</description>
    </item>
    
    <item>
      <title>java中的zero copy</title>
      <link>https://wenchao.ren/posts/java%E4%B8%AD%E7%9A%84zero-copy/</link>
      <pubDate>Thu, 14 Mar 2019 13:52:46 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java%E4%B8%AD%E7%9A%84zero-copy/</guid>
      <description>在web应用程序中，我们经常会在server和client之间传输数据。比如server发数据给client，server首先将数据从硬盘读出之后，然后原封不动的通过socket传输给client，大致原理如下：
File.read(fileDesc, buf, len); Socket.send(socket, buf, len);  下面的例子展示了传统的数据复制实现
import java.io.DataOutputStream; import java.io.FileInputStream; import java.io.IOException; import java.net.Socket; import java.net.UnknownHostException; public class TraditionalClient { public static void main(String[] args) { int port = 2000; String server = &amp;quot;localhost&amp;quot;; Socket socket = null; String lineToBeSent; DataOutputStream output = null; FileInputStream inputStream = null; int ERROR = 1; // connect to server try { socket = new Socket(server, port); System.out.println(&amp;quot;Connected with server &amp;quot; + socket.</description>
    </item>
    
    <item>
      <title>linux命令-grep</title>
      <link>https://wenchao.ren/posts/linux%E5%91%BD%E4%BB%A4-grep/</link>
      <pubDate>Mon, 11 Mar 2019 20:50:48 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/linux%E5%91%BD%E4%BB%A4-grep/</guid>
      <description>常见参数说明 grep [OPTIONS] PATTERN [FILE...] grep [OPTIONS] [-e PATTERN]... [-f FILE]... [FILE...] OPTIONS: -e: 使用正则搜索 -i: 不区分大小写 -v: 查找不包含指定内容的行 -w: 按单词搜索 -c: 统计匹配到的次数 -n: 显示行号 -r: 逐层遍历目录查找 -A: 显示匹配行及前面多少行, 如: -A3, 则表示显示匹配行及前3行 -B: 显示匹配行及后面多少行, 如: -B3, 则表示显示匹配行及后3行 -C: 显示匹配行前后多少行, 如: -C3, 则表示显示批量行前后3行 --color: 匹配到的内容高亮显示 --include: 指定匹配的文件类型 --exclude: 过滤不需要匹配的文件类型  常见用法 #多文件查询 grep leo logs.log logs_back.log #查找即包含leo又包含li的行 grep leo logs.log | grep li #查找匹配leo或者匹配li的行 grep leo | li logs.log #显示匹配行前2行 grep leo logs.</description>
    </item>
    
    <item>
      <title>mysql binlog初步介绍</title>
      <link>https://wenchao.ren/posts/mysql-binlog%E5%88%9D%E6%AD%A5%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 11 Mar 2019 12:24:56 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/mysql-binlog%E5%88%9D%E6%AD%A5%E4%BB%8B%E7%BB%8D/</guid>
      <description>binlog 即二进制日志,它记录了数据库上的所有改变，并以二进制的形式保存在磁盘中； 它可以用来查看数据库的变更历史、数据库增量备份和恢复、Mysql的复制（主从数据库的复制）。
mysql binlog解析 binlog有三种格式:
 Statement 基于SQL语句的复制(statement-based replication,SBR)， Row 基于行的复制(row-based replication,RBR)， Mixed 混合模式复制(mixed-based replication,MBR)。  在我这边mysql 5.7.20版本中默认是使用Row的, 而且默认情况下没有开启binlog
mysql&amp;gt; select version(); +-----------+ | version() | +-----------+ | 5.7.20 | +-----------+ 1 row in set (0.00 sec) mysql&amp;gt; mysql&amp;gt; mysql&amp;gt; show variables like &#39;binlog_format&#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | binlog_format | ROW | +---------------+-------+ 1 row in set (0.00 sec) mysql&amp;gt; show variables like &#39;log_bin&#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_bin | OFF | +---------------+-------+ 1 row in set (0.</description>
    </item>
    
    <item>
      <title>ERROR 1728 (HY000): Cannot load from mysql.procs_priv. The table is probably corrupted</title>
      <link>https://wenchao.ren/posts/error-1728-hy000-cannot-load-from-mysql-procs-priv-the-table-is-probably-corrupted/</link>
      <pubDate>Mon, 11 Mar 2019 12:23:46 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/error-1728-hy000-cannot-load-from-mysql-procs-priv-the-table-is-probably-corrupted/</guid>
      <description>今天在搞mysql binlog收集时，需要创建一个mysql用户，结果出现了：
ERROR 1728 (HY000): Cannot load from mysql.procs_priv. The table is probably corrupted异常
解决办法:
sudo mysql_upgrade -u root -p
注意后面的用户名和密码自己修改为自己的哈。
~ » sudo mysql_upgrade -u root -p Password: Enter password: Checking if update is needed. Checking server version. Running queries to upgrade MySQL server. Checking system database. mysql.columns_priv OK mysql.db OK mysql.engine_cost OK mysql.event OK mysql.func OK mysql.general_log OK mysql.gtid_executed OK mysql.help_category OK mysql.help_keyword OK mysql.help_relation OK mysql.help_topic OK mysql.</description>
    </item>
    
    <item>
      <title>监控系统的模板功能</title>
      <link>https://wenchao.ren/posts/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Mon, 11 Mar 2019 12:20:55 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%A8%A1%E6%9D%BF%E5%8A%9F%E8%83%BD/</guid>
      <description>这篇文章主要总结整理一下最近工作中两套监控系统合并过程中使用模板功能来抽象【针对应用相关的监控】 和【针对机器相关的监控】的差异。
背景 公司主要的监控系统主要是针对应用级别的，所有的指标都是属于App级别的，也就是想看某条监控指标数据，首先需要选定具体的APP。 然后公司也有一套针对机器相关的监控系统，因为机器相关的监控很多时候是以机器为维度的，也就是想查询某个机器的指标数据时 需要先指定具体的机器。同时由于公司业务的特殊性，也需要增加对多种终端类型（比如门店、货柜）的监控相关的功能。同时考虑到开发 、维护等成本等等的诸多因素，现在需要将两套监控系统合并。因此需要一套通用的模型来抽象这种问题。
其中这套模式本身并不复杂，没啥好说的。
模板功能模型 下面这个图是我当时设计的模板功能
简单描述一下这个图：
 Endpoint可以属于多个EndpointGroup EndpointGroup可以绑定多个Template, 一个Template也可以绑定到多个EndpointGroup上面 Template会包含多个Metric数据，同时也支持继承关系，也就是子模板会继承父模板的指标数据，如果父子模板中都含有同样的 Metric的话，那么子模板的会覆盖父模板的指标数据。 Endpoint也可以直接和Template来进行绑定，而不需要通过EndpointGroup Endpoint可以直接添加Metric数据，这个主要是因为目前针对App相关的监控都是属于这种情况的。  Endpoint 因为监控指标数据可能属于一个APP，也可能属于一个机器、门店、货柜、咖啡机等。所以使用了抽象的Endpoint来描述指标的归属。
EndpointGroup 含义很简单，之所以弄这个主要是为了做批处理，这样就不需要针对每一个Endpoint来进行操作了，一次可以操作一批。
Template Template其实就是一些Metric的集合，支持继承功能，继承功能其实并不是必须的，可以通过组合来实现，最后我们选择了继承 的方式，主要是考虑到针对下面的场景，继承这种方式描述起来更好一些：
所有的机器都有一些基础监控，比如针对cpu、内存的监控，我们将这些基础的指标数据使用template1来进行打包，然后如果想对所有的 dev环境的机器增加一些监控指标，这些监控指标我们使用Template2来打包
如果使用组合的方式，需要对Endpoint或者EndpointGroup绑定Template1和Template2，而如何使用继承的话，只需要绑定Template2 就好。
Metric 这个比较简单，所有监控系统的基本语义。</description>
    </item>
    
    <item>
      <title>Iterable和Iterator结合使用的一个小例子</title>
      <link>https://wenchao.ren/posts/iterable%E5%92%8Citerator%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8F%E4%BE%8B%E5%AD%90/</link>
      <pubDate>Mon, 11 Mar 2019 12:19:25 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/iterable%E5%92%8Citerator%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8F%E4%BE%8B%E5%AD%90/</guid>
      <description>这篇文章主要是记录一下使用Iterable和Iterator用作迭代处理的一个例子。基于这种模式可以很方便的实现 流式处理
public class Array&amp;lt;T&amp;gt; implements Iterable&amp;lt;T&amp;gt; { T[] values; // this contains the actual elements of the array // Constructor that takes a &amp;quot;raw&amp;quot; array and stores it public Array(T[] values) { this.values = values; } // This is a private class that implements iteration over the elements // of the list. It is not accessed directly by the user, but is used in // the iterator() method of the Array class.</description>
    </item>
    
    <item>
      <title>Guava CacheLoader中当load方法返回null</title>
      <link>https://wenchao.ren/posts/guava-cacheloader%E4%B8%AD%E5%BD%93load%E6%96%B9%E6%B3%95%E8%BF%94%E5%9B%9Enull/</link>
      <pubDate>Mon, 11 Mar 2019 12:18:40 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/guava-cacheloader%E4%B8%AD%E5%BD%93load%E6%96%B9%E6%B3%95%E8%BF%94%E5%9B%9Enull/</guid>
      <description>Guava LoadingCache在实际工作中用的还是比较频繁的。但是最近在review代码时，发现有些同学在使用CacheLoader时没有注意到 CacheLoader#load方法的注释：
/** * Computes or retrieves the value corresponding to {@code key}. * * @param key the non-null key whose value should be loaded * @return the value associated with {@code key}; &amp;lt;b&amp;gt;must not be null&amp;lt;/b&amp;gt; * @throws Exception if unable to load the result * @throws InterruptedException if this method is interrupted. {@code InterruptedException} is * treated like any other {@code Exception} in all respects except that, when it is caught, * the thread&#39;s interrupt status is set */ public abstract V load(K key) throws Exception;  源码中明确指出了这个方法不能返回null。但是在review代码时发现很多同学没注意到到这个，而在部分情况下存在返回null的情况。 一般使用Optional封装一下就好了。</description>
    </item>
    
    <item>
      <title>mysql sleep</title>
      <link>https://wenchao.ren/posts/mysql-sleep/</link>
      <pubDate>Mon, 11 Mar 2019 12:17:55 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/mysql-sleep/</guid>
      <description>今天看到了一个sql：
select count(*), sleep(5) from test  第一次看到这个sleep函数，所以专门研究了一波。这个函数的语法是：SLEEP(duration), 其中duration的单位是秒
 Sleeps (pauses) for the number of seconds given by the duration argument, then returns 0. The duration may have a fractional part. If the argument is NULL or negative, SLEEP() produces a warning, or an error in strict SQL mode.
  When sleep returns normally (without interruption), it returns 0:
 简单的说他可以让sql在执行的时候sleep一段时间。
mysql&amp;gt; SELECT SLEEP(1000); +-------------+ | SLEEP(1000) | +-------------+ | 0 | +-------------+  When SLEEP() is the only thing invoked by a query that is interrupted, it returns 1 and the query itself returns no error.</description>
    </item>
    
    <item>
      <title>brew操作mysql</title>
      <link>https://wenchao.ren/posts/brew%E6%93%8D%E4%BD%9Cmysql/</link>
      <pubDate>Mon, 11 Mar 2019 12:16:40 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/brew%E6%93%8D%E4%BD%9Cmysql/</guid>
      <description> 安装mysql brew install mysql 启动mysql brew services start mysql  停止mysql brew services stop mysql 重启mysql brew services restart mysql  在我的机器上，my.cnf文件的位置在：/usr/local/etc/my.cnf，默认情况下里面的内容为：
# Default Homebrew MySQL server config [mysqld] # Only allow connections from localhost bind-address = 127.0.0.1  </description>
    </item>
    
    <item>
      <title>java日志不打印异常栈</title>
      <link>https://wenchao.ren/posts/java%E6%97%A5%E5%BF%97%E4%B8%8D%E6%89%93%E5%8D%B0%E5%BC%82%E5%B8%B8%E6%A0%88/</link>
      <pubDate>Mon, 11 Mar 2019 12:15:10 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java%E6%97%A5%E5%BF%97%E4%B8%8D%E6%89%93%E5%8D%B0%E5%BC%82%E5%B8%B8%E6%A0%88/</guid>
      <description>问题描述 今天在排查一个问题的时候发现在日志输出中，只有异常的Message,并没有详细的异常堆栈。
问题解释 对于这个问题的官方解释为:
 The compiler in the server VM now provides correct stack backtraces for all &amp;ldquo;cold&amp;rdquo; built-in exceptions. For performance purposes, when such an exception is thrown a few times, the method may be recompiled. After recompilation, the compiler may choose a faster tactic using preallocated exceptions that do not provide a stack trace. To disable completely the use of preallocated exceptions, use this new flag: -XX:-OmitStackTraceInFastThrow.</description>
    </item>
    
    <item>
      <title>log4j-api-2.11.1.jar ClassFormatException</title>
      <link>https://wenchao.ren/posts/log4j-api-2-11-1-jar-classformatexception/</link>
      <pubDate>Fri, 08 Mar 2019 17:02:13 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/log4j-api-2-11-1-jar-classformatexception/</guid>
      <description>今天在部署系统的时候tomcat出现下面的异常信息：
Mar 08, 2019 4:18:05 PM org.apache.catalina.startup.ContextConfig processAnnotationsJar SEVERE: Unable to process Jar entry [META-INF/versions/9/module-info.class] from Jar [jar:file:/xxxxx/webapps/ROOT/WEB-INF/lib/log4j-api-2.11.1.jar!/] for annotations org.apache.tomcat.util.bcel.classfile.ClassFormatException: Invalid byte tag in constant pool: 19 at org.apache.tomcat.util.bcel.classfile.Constant.readConstant(Constant.java:133) at org.apache.tomcat.util.bcel.classfile.ConstantPool.&amp;lt;init&amp;gt;(ConstantPool.java:60) at org.apache.tomcat.util.bcel.classfile.ClassParser.readConstantPool(ClassParser.java:209) at org.apache.tomcat.util.bcel.classfile.ClassParser.parse(ClassParser.java:119) at org.apache.catalina.startup.ContextConfig.processAnnotationsStream(ContextConfig.java:2134) at org.apache.catalina.startup.ContextConfig.processAnnotationsJar(ContextConfig.java:2010) at org.apache.catalina.startup.ContextConfig.processAnnotationsUrl(ContextConfig.java:1976) at org.apache.catalina.startup.ContextConfig.processAnnotations(ContextConfig.java:1961) at org.apache.catalina.startup.ContextConfig.webConfig(ContextConfig.java:1319) at org.apache.catalina.startup.ContextConfig.configureStart(ContextConfig.java:878) at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:376) at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119) at org.apache.catalina.util.LifecycleBase.fireLifecycleEvent(LifecycleBase.java:90) at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5322) at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150) at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:901) at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:877) at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:633) at org.apache.catalina.startup.HostConfig.deployDirectory(HostConfig.java:1120) at org.apache.catalina.startup.HostConfig$DeployDirectory.run(HostConfig.java:1678) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.</description>
    </item>
    
    <item>
      <title>java Unsafe介绍</title>
      <link>https://wenchao.ren/posts/java-unsafe%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 04 Mar 2019 19:34:03 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java-unsafe%E4%BB%8B%E7%BB%8D/</guid>
      <description>Unsafe类对于并发编程来说是个很重要的类，J.U.C里的源码到处充斥着这个类的方法调用。
这个类的最大的特点在于，它提供了硬件级别的CAS原子操作。CAS可以说是实现了最轻量级的锁，当多个线程尝试使用CAS同时更新同一个变量时，只有其中的一个线程能成功地更新变量的值，而其他的线程将失败。然而，失败的线程并不会被挂起。
CAS操作包含了三个操作数： 需要读写的内存位置，进行比较的原值，拟写入的新值。
在Unsafe类中，实现CAS操作的方法是： compareAndSwapXXX
例如:
public native boolean compareAndSwapObject(Object obj, long offset, Object expect, Object update);   obj是我们要操作的目标对象 offset表示了目标对象中，对应的属性的内存偏移量 expect是进行比较的原值 update是拟写入的新值。   所以该方法实现了对目标对象obj中的某个成员变量（field）进行CAS操作的功能。
那么，要怎么获得目标field的内存偏移量offset呢？ Unsafe类为我们提供了一个方法：
public native long objectFieldOffset(Field field);  该方法的参数是我们要进行CAS操作的field对象，要怎么获得这个field对象呢？最直接的办法就是通过反射了：
Class&amp;lt;?&amp;gt; k = FutureTask.class; Field stateField = k.getDeclaredField(&amp;quot;state&amp;quot;);  这样一波下来，我们就能对FutureTask的state属性进行CAS操作了o(￣▽￣)o
除了compareAndSwapObject，Unsafe类还提供了更为具体的对int和long类型的CAS操作：
public native boolean compareAndSwapInt(Object obj, long offset, int expect, int update); public native boolean compareAndSwapLong(Object obj, long offset, long expect, long update);  从方法签名可以看出，这里只是把目标field的类型限定成int和long类型，而不是通用的Object.</description>
    </item>
    
    <item>
      <title>FutureTask使用和源码解析</title>
      <link>https://wenchao.ren/posts/futuretask%E4%BD%BF%E7%94%A8%E5%92%8C%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Mon, 04 Mar 2019 18:45:29 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/futuretask%E4%BD%BF%E7%94%A8%E5%92%8C%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/</guid>
      <description>本篇文章说一下java.util.concurrent中的FutureTask。 FutureTask是一个同步工具类，它实现了Future语义，表示了一种抽象的可生成结果的计算。在包括线程池在内的许多工具类中都会用到，弄懂它的实现将有利于我们更加深入地理解Java异步操作实现。
下面是FutureTask的类图：
在分析它的源码之前, 我们需要先了解一些预备知识。本篇我们先来看看FutureTask中所使用到的接口：Runnable、Callable、Future、RunnableFuture以及所使用到的工具类Executors，Unsafe。
FutureTask所使用到的接口 Runnable 创建线程最重要的是传递一个run()方法, 这个run方法定义了这个线程要做什么事情, 它被抽象成了Runnable接口:
@FunctionalInterface public interface Runnable { public abstract void run(); }  从这里可以看出Runnable最大的问题有下面2个：
 没有返回值，我们不能从里面返回相关的处理结果 不能抛出checked exception  而这2个问题，导致我们很多时候使用Runnable其实都会丧失很多的灵活性。而为了解决这两个问题，JDK提供了Callable。
Callable @FunctionalInterface public interface Callable&amp;lt;V&amp;gt; { /** * Computes a result, or throws an exception if unable to do so. * * @return computed result * @throws Exception if unable to compute a result */ V call() throws Exception; }  对于Runnable可以知道，Callable解决了Runnable的两个最大的问题。但是Callable自己带来了一个问题：就是如何获取返回值。</description>
    </item>
    
    <item>
      <title>类加载器那些事儿（一）</title>
      <link>https://wenchao.ren/posts/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF%E4%B8%80/</link>
      <pubDate>Thu, 28 Feb 2019 13:40:49 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%E9%82%A3%E4%BA%9B%E4%BA%8B%E5%84%BF%E4%B8%80/</guid>
      <description>在之前的文章《Java类的生命周期》我们谈了一下类的生命周期。 在这篇文章中，我们谈谈java的类加载器哪些事情。从下面的JVM架构图可以看到
class Loader subSystem负责管理和维护java类的生命周期的前三个阶段:
 加载 链接 初始化  当我们编写一个java的源文件后，我们对这个xxx.java编译会得到xxx.class的字节码文件，因为jvm只能运行字节码文件。为了能够使用这个class字节码文件，我们就会用到java中的ClassLoader。 而我们这篇文章就来说说java类加载器的那些事情。
ClassLoader是什么 ClassLoader顾名思义就是用来加载Class的。它负责将Class的字节码形式转换成内存形式的Class对象。
类的加载方式比较灵活，我们最常用的加载方式有下面几种：
 一种是根据类的全路径名找到相应的class文件，然后从class文件中读取文件内容； 另一种是从jar文件中读取 从网络中获取，比如早期的Applet 基于字节码生成技术生成的代理类  字节码的本质就是一个字节数组（byte[]），它有特定的复杂的内部格式。因为字节码文件有一定的格式，而且由ClassLoader进行加载，那么我们其实可以通过定制ClassLoader来实现字节码加密，原理很简单：
 加密：对java源代码进行编译得到字节码文件，然后使用某种算法对字节码文件进行加密 解密：定制的ClassLoader会先使用加密算法对应的解密算法对加密的字节码文件进行解密，然后使用在正常加载jvm标准的字节码格式文件。  3个重要的ClassLoader 在上面的JVM架构图中，我们可以看到在类的加载阶段有3个重要的ClassLoader，下面分别介绍一下这3个比较重要的ClassLoader。
启动类加载器(BootstrapClassLoader) 这个类加载器负责加载JVM运行时核心类， 将&amp;lt;JAVA_HOME&amp;gt;\lib目录下的核心类库或-Xbootclasspath参数指定的路径下的jar包加载到虚拟机内存中,这个 ClassLoader比较特殊，它是由C/C++代码实现的，我们将它称之为「根加载器」。此类加载器并不继承于java.lang.ClassLoader,不能被java程序直接调用。
注意必由于虚拟机是按照文件名识别加载jar包的，如rt.jar，如果文件名不被虚拟机识别，即使把jar包丢到lib目录下也是没有作用的(出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类)。
扩展类加载器(ExtensionClassLoader) 这个类加载器sun.misc.Launcher$ExtClassLoader由Java语言实现的，是Launcher的静态内部类, 它负责加载&amp;lt;JAVA_HOME&amp;gt;/lib/ext目录下或者由系统变量-Djava.ext.dir指定位路径中的类库，开发者可以直接使用使用这个类加载器。
常见的比如 swing 系列、内置的 js 引擎、xml 解析器等等都是由这个类加载器加载的， 这些库名通常以javax开头，它们的jar包位于&amp;lt;JAVA_HOME&amp;gt;\lib\ext目录下的类库。
//ExtClassLoader类中获取路径的代码 private static File[] getExtDirs() { //加载&amp;lt;JAVA_HOME&amp;gt;/lib/ext目录中的类库 String s = System.getProperty(&amp;quot;java.ext.dirs&amp;quot;); File[] dirs; if (s != null) { StringTokenizer st = new StringTokenizer(s, File.pathSeparator); int count = st.</description>
    </item>
    
    <item>
      <title>Java类的生命周期</title>
      <link>https://wenchao.ren/posts/java%E7%B1%BB%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</link>
      <pubDate>Wed, 27 Feb 2019 21:06:42 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java%E7%B1%BB%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/</guid>
      <description>当我们编写一个java的源文件后，经过编译会生成一个后缀名为class的文件，这种文件叫做字节码文件，只有这种字节码文件才能够在java虚拟机中运行。 java类的生命周期就是指一个class文件从加载到卸载的全过程
我们看一下下面的jvm架构图，这个图太经典了，希望大家收藏这个图。 一个java类的完整的生命周期会经历下面五个阶段， 当然也有在加载或者连接之后没有被初始化就直接被使用的情况，如上图所示
 加载 连接 初始化 使用 卸载  对象基本上都是在jvm的堆区中创建，在创建对象之前，会触发类加载（加载、连接、初始化），当类初始化完成后，根据类信息在堆区中实例化类对象，初始化非静态变量、非静态代码以及默认构造方法，当对象使用完之后会在合适的时候被jvm垃圾收集器回收。
对象的生命周期只是类的生命周期中使用阶段的主动引用的一种情况（下面有提主动引用的含义）。而类的整个生命周期则要比对象的生命周期长的多。
加载 加载阶段是类的生命周期中的第一个阶段, 在加载阶段，jvm就是找到需要加载的类并把类的信息加载到jvm的方法区中，然后在堆区中实例化一个java.lang.Class对象，作为方法区中这个类的信息的入口。
类的加载方式比较灵活，我们最常用的加载方式有下面几种：
 一种是根据类的全路径名找到相应的class文件，然后从class文件中读取文件内容； 另一种是从jar文件中读取 从网络中获取，比如早期的Applet 基于字节码生成技术生成的代理类  对于加载的时机，各个虚拟机的做法并不一样，但是有一个原则，就是当jvm“预期”到一个类将要被使用时，就会在使用它之前对这个类进行加载。比如说，在一段代码中出现了一个类的名字，jvm在执行这段代码之前并不能确定这个类是否会被使用到，于是，有些jvm会在执行前就加载这个类，而有些则在真正需要用的时候才会去加载它，这取决于具体的jvm实现。我们常用的hotspot虚拟机是采用的后者，就是说当真正用到一个类的时候才对它进行加载。
链接 链接阶段。有一点需要注意的是有时：链接阶段并不会等加载阶段完全完成之后才开始，而是交叉进行，可能一个类只加载了一部分之后，连接阶段就已经开始了。但是这两个阶段总的开始时间和完成时间总是固定的：加载阶段总是在连接阶段之前开始，连接阶段总是在加载阶段完成之后完成。
连接阶段完成之后会根据使用的情况（直接引用还是被动引用）来选择是否对类进行初始化。
这个阶段的主要任务就是做一些加载后的验证工作以及一些初始化前的准备工作，可以细分为三个步骤：
 验证  当一个类被加载之后，必须要验证一下这个类是否合法，比如这个类是不是符合字节码的格式、变量与方法是不是有重复、数据类型是不是有效、继承与实现是否合乎标准等等。总之，这个阶段的目的就是保证加载的类是能够被jvm所运行。   准备  准备阶段的工作就是为类的静态变量分配内存并设为jvm默认的初值，对于非静态的变量，则不会为它们分配内存。有一点需要注意，这时候，静态变量的初值为jvm默认的初值，而不是我们在程序中设定的初值。jvm默认的初值是这样的：  基本类型（int、long、short、char、byte、boolean、float、double）的默认值为0。 引用类型的默认值为null。 常量的默认值为我们程序中设定的值，比如我们在程序中定义final static int a = 100，则准备阶段中a的初值就是100。     解析  这一阶段的任务就是把常量池中的符号引用转换为直接引用。在解析阶段，jvm会将所有的类或接口名、字段名、方法名转换为具体的内存地址。  符号引用：简单的理解就是字符串，比如引用一个类，java.util.ArrayList 这就是一个符号引用，字符串引用的对象不一定被加载。 直接引用：指针或者地址偏移量。引用对象一定在内存（已经加载）      初始化 如果一个类被直接引用，就会触发类的初始化。在java中，直接引用的情况有：
 通过new关键字实例化对象、读取或设置类的静态变量、调用类的静态方法。 通过反射方式执行以上三种行为。 初始化子类的时候，会触发父类的初始化。 作为程序入口直接运行时（也就是直接调用main方法）。  除了以上四种情况，其他使用类的方式叫做被动引用，而被动引用不会触发类的初始化。</description>
    </item>
    
    <item>
      <title>一致性哈希</title>
      <link>https://wenchao.ren/posts/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/</link>
      <pubDate>Wed, 27 Feb 2019 12:37:11 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/</guid>
      <description>本文谈谈一致性哈希，一致性哈希作为「负载均衡」中比较常见的一种实现，经常会有意无意的被大家使用到。我希望通过这篇文章可以使得你完全明白：
 一致性哈希要解决的问题 一致性哈希的原理 一致性哈希的优点 一致性哈希的不适用场景 如何手动实现一致性哈希 常见开源代码中的一致性哈希实现  一致性哈希要优化的问题 一致性哈希要解决的问题，或者说目标，其实用一句话概括就是：在hash value区间有限并且可能会发生变化的情况下，相同的hash key尽可能得到同一个hash value。
上面短短的一句话，我们可以得到一些重要信息：
一致性哈希的原理 上面短短的
一致性哈希负载均衡需要保证的是“相同的请求尽可能落到同一个服务器上”，注意这短短的一句描述，却包含了相当大的信息量。“相同的请求” — 什么是相同的请求？一般在使用一致性哈希负载均衡时，需要指定一个 key 用于 hash 计算
一致性哈希的优点 相比于传统的「取模」哈希，一致性哈希减少了因为服务节点变更导致的key的映射关系失效的数量
一致性哈希的不适用场景 </description>
    </item>
    
    <item>
      <title>nginx开启gzip</title>
      <link>https://wenchao.ren/posts/nginx%E5%BC%80%E5%90%AFgzip/</link>
      <pubDate>Wed, 27 Feb 2019 11:05:07 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/nginx%E5%BC%80%E5%90%AFgzip/</guid>
      <description>前几天看到一个nginx的文章，突然想到我还没有为我的博客的nginx开启gzip压缩呢。所以今天就弄了一下。
下面是我在nginx配置中增加的开启gzip相关的配置：
nginx增加gzip配置 #gzip gzip on; gzip_min_length 1k; gzip_buffers 4 32k; gzip_comp_level 4; gzip_types text/plain application/javascript application/x-javascript text/javascript text/xml text/css; gzip_vary on; gzip_disable &amp;quot;MSIE [1-6]\.&amp;quot;;  然后重新使得nginx加载配置:
/usr/sbin/nginx -s reload  gzip指令描述  gzip on 这个指令用来控制开启或者关闭gzip模块，默认值为gzip off代表默认不开启gzip压缩 gzip_min_length 1k 置允许压缩的页面最小字节数，页面字节数从header头中的Content-Length中进行获取。默认值: 0 ，不管页面多大都压缩 gzip_buffers 4 32k 设置系统获取几个单位的缓存用于存储gzip的压缩结果数据流。 例如 4 4k 代表以4k为单位，按照原始数据大小以4k为单位的4倍申请内存。 4 8k 代表以8k为单位，按照原始数据大小以8k为单位的4倍申请内存默认值: gzip_buffers 4 4k/8k 如果没有设置，默认值是申请跟原始数据相同大小的内存空间去存储gzip压缩结果。 gzip_comp_level 4 gzip压缩级别，压缩级别 1-9，级别越高压缩率越大，当然压缩时间也就越长（传输快但比较消耗cpu）。默认值：1 gzip_types text/plain application/javascript application/x-javascript text/javascript text/xml text/css 压缩类型，匹配MIME类型进行压缩.默认值: gzip_types text/html 也就是默认不对js/css文件进行压缩。需要注意的是此处不能使用通配符，比如:text/*,无论是否指定text/html, 这种类型的都会被压缩。 gzip_vary on 和http头有关系，加个vary头，给代理服务器用的，有的浏览器支持压缩，有的不支持，所以避免浪费不支持的也压缩，所以根据客户端的HTTP头来判断，是否需要压缩 gzip_disable &amp;quot;MSIE [1-6]\.</description>
    </item>
    
    <item>
      <title>几个有意思的java小题目</title>
      <link>https://wenchao.ren/posts/%E5%87%A0%E4%B8%AA%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84java%E5%B0%8F%E9%A2%98%E7%9B%AE/</link>
      <pubDate>Fri, 22 Feb 2019 19:56:10 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%87%A0%E4%B8%AA%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84java%E5%B0%8F%E9%A2%98%E7%9B%AE/</guid>
      <description>null + String 写出下面代码执行结果:
// 1. 打印 null String String s = null; System.out.println(s); String str = null; str = str + &amp;quot;!&amp;quot;; System.out.println(str);  这个片段程序不会出现NPE，正常输出：
null null!  我一开始以为第二个输出会抛出NPE。google了一下，看到了这篇文章Java String 对 null 对象的容错处理, 里面有解释：
对于代码片段：
String s = null; s = s + &amp;quot;!&amp;quot;; System.out.print(s);  编译器生成的字节码为：
L0 LINENUMBER 27 L0 ACONST_NULL ASTORE 1 L1 LINENUMBER 28 L1 NEW java/lang/StringBuilder DUP INVOKESPECIAL java/lang/StringBuilder.&amp;lt;init&amp;gt; ()V ALOAD 1 INVOKEVIRTUAL java/lang/StringBuilder.append (Ljava/lang/String;)Ljava/lang/StringBuilder; LDC &amp;quot;!</description>
    </item>
    
    <item>
      <title>个人认为的工作多年的开发者的3个最重要技能</title>
      <link>https://wenchao.ren/posts/%E4%B8%AA%E4%BA%BA%E8%AE%A4%E4%B8%BA%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%A4%9A%E5%B9%B4%E7%9A%84%E5%BC%80%E5%8F%91%E8%80%85%E7%9A%843%E4%B8%AA%E6%9C%80%E9%87%8D%E8%A6%81%E6%8A%80%E8%83%BD/</link>
      <pubDate>Wed, 20 Feb 2019 20:47:33 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%B8%AA%E4%BA%BA%E8%AE%A4%E4%B8%BA%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%A4%9A%E5%B9%B4%E7%9A%84%E5%BC%80%E5%8F%91%E8%80%85%E7%9A%843%E4%B8%AA%E6%9C%80%E9%87%8D%E8%A6%81%E6%8A%80%E8%83%BD/</guid>
      <description>不知道你觉的工作多年的开发者最重要的技能是什么？
随着工作经历的增长，我个人越来越觉得对于开发者来说，很多时候个人的专业技能反而并不是最重要的，甚至在很多的时候是最廉价，最容易被替代的。反而是一些非专业技能确显得更加的重要。我觉的按照优先级从高到低依次为：
 业务洞察力 技术视野 非常高执行力  业务洞察力 业务洞察力是值：在当下能够做出合理的判断，清楚Team或者公司做什么事情收益最大，很多时候是 战略层面 的问题
我们经常会面临有无穷无尽的事情要做，有无穷无尽的事情可以做，但是
 我们应该先做哪个呢？ 做哪些事情的收益是最大的呢？ 做哪些事情做了和没做对公司区别没那么大呢？ 甚至哪些做哪些事情是吃力不讨好的呢？  这里不是说大家要做老油条，而是要有超前的眼光，跳出仅仅作为一个代码编写员的视角，站在更高的层次来思索事情的优先级。
那平时应该怎么做呢？
我觉的首先要有这方面的意识，其次多了解公司各个team正在做的事情，通过新闻、自己的观察、茶前饭后的闲聊、帮其他组同事排查问题时听到的等等的手段，尽可能多的获取一些公司大的层面的一些信息，然后了解公司的一些计划，公司业务的重点发展方向，业务团队经常的痛点是什么等等的。然后基于这些信息，来辅助我们做优先级判断。
技术视野 技术视野即技术选型能力，是 战术层面 的问题，在清楚做什么事情后，需要进一步解决怎么做的问题，也就是能够给出合理的技术选型方案：是完全基于开源的方案，还是基于开源二次开发的方案，还是完全自研的方案，同时要有一定的前瞻性，保持自己对业界技术风向的敏感度。但是一切都要结合公司实际发展情况，要务实不能务虚。不能盲目的追求高新技术，一定要把握好技术风险。
非常高执行力 执行力是技术落地执行层面的问题，一旦技术设计方案确定后，需要能够快速完成。一般工作多年的同事的执行力往往都是比较高的，毕竟手熟。所以要注意培养自己的前两点能力。
这3点层层递进，最重要的是先把技术战略问题思考清楚，然后再进一步解决技术战术问题，最后是快速落地执行的问题。</description>
    </item>
    
    <item>
      <title>谈谈缓存</title>
      <link>https://wenchao.ren/posts/%E8%B0%88%E8%B0%88%E7%BC%93%E5%AD%98/</link>
      <pubDate>Mon, 18 Feb 2019 23:09:08 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E8%B0%88%E8%B0%88%E7%BC%93%E5%AD%98/</guid>
      <description>本文谈谈缓存相关的问题，主要从下面几个点来出发，但是主要谈分布式缓存。
 为什么需要用缓存？ 缓存的原理是什么？ 有哪几种类的缓存？ 分布式缓存一般存在哪些问题？  缓存一致性问题 先操作数据库，还是先操作缓存 缓存应该淘汰还是应该修改 缓存不一致应该怎么解？ 缓存并发更新问题 缓存穿透问题 缓存雪崩 缓存的「无底洞」现象    为什么需要用缓存 一般我们使用缓存，其实都是为了加速数据访问。
缓存的原理是什么 因为缓存是为了加速数据访问。而为了达到这个目的，往往是采用了下面的思路：
 将数据缓存在更高速的读取设备上。比如cpu的寄存器 将数据提前运算并加载，可以减少同样数据的获取步骤，进而提升效率，大多数应用级的cache都是基于这种思路。 将数据放置在距离用户更近的地点，可以加快数据获取速度。比如CDN就是这种思路。  有哪几类缓存 常见的缓存有下面几类：
  CDN(Content Delivery Network 内容分发网络)。
 CND的基本原理是广泛采用各种缓存服务器，将这些缓存服务器分布更接近用户的网络中，在用户访问网站时，利用全局负载技术将用户的访问指向距离最近的工作正常的缓存服务器上，由缓存服务器直接响应用户请求。一般会缓存图片，css js甚至视频。    反向代理缓存
 反向代理位于应用服务器机房，处理所有对WEB服务器的请求。 如果用户请求的页面在代理服务器上有缓冲的话，代理服务器直接将缓冲内容发送给用户。如果没有缓冲则先向WEB服务器发出请求，取回数据，本地缓存后再发送给用户。通过降低向WEB服务器的请求数，从而降低了WEB服务器的负载。这种缓存一般都是缓存一些静态资源，比如css js和图片。比如常见的nginx就可以用来干这个事情。    应用本地缓存
 指的是在应用中的缓存组件，其最大的优点是应用和cache是在同一个进程内部，请求缓存非常快速，没有过多的网络开销。比如常见的guava cache。    分布式缓存
 指的是与应用分离的缓存组件或服务，其最大的优点是自身就是一个独立的应用，与本地应用隔离，多个应用可直接的共享缓存。    分布式缓存一般存在哪些问题？ 缓存一致性问题 一个最简单的场景，应用架构为Spring MVC + redis缓存 + mysql存储。如果保证redis缓存和mysql数据库的数据一致性呢？</description>
    </item>
    
    <item>
      <title>如何设计密码重置功能</title>
      <link>https://wenchao.ren/posts/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E5%AF%86%E7%A0%81%E9%87%8D%E7%BD%AE%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Mon, 18 Feb 2019 18:50:35 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E5%AF%86%E7%A0%81%E9%87%8D%E7%BD%AE%E5%8A%9F%E8%83%BD/</guid>
      <description>密码重置功能或者说密码找回功能是互联网行业的一项基本功能，本篇文章主要总结一下完成这个功能需要注意的一些点，主要遵循以下几条原则：
 密码要「安全的」存储，我个人一般推荐bcrypt加密算法，当然PBKDF2和scrypt也很不错。 密码找回功能，不能告诉用户原来密码，而是应该让用户重置密码。  尽量不要采用预先分配一个密码，然后告诉用户这个初始密码再让用户去修改的办法。 而是应该发给用户一个有时效性的链接，让用户在规定的时间内，通过这个连接来重置密码。 避免前后端明文传递密码   密码找回的时候，因为需要知道找回谁的密码，所以需要一个身份标识，一般建议采用邮箱或者手机号，需要注意的是，无论这个身份是否存在，都不能页面提示 这个身份是否存在，避免被扫描。而是应该无论用户输入邮箱还是电话，都正常发验证码。如果用户瞎填，他自然收不到验证码。建议在邮件内容中提示是正在重置xxx网站的密码。 同时为了避免机器人，在重置密码提交form时需要增加验证码环节 尽可能让用户在正确填写验证码以后，在验证一下一些问题。比如一些用户提前设置的问题，如果没有的话，可以根据具体的业务，比如登录地点，上次登录时间，xxx是你的朋友么等等的问题。  </description>
    </item>
    
    <item>
      <title>zoopeeper Unexpected Exception: java.nio.channels.CancelledKeyException</title>
      <link>https://wenchao.ren/posts/zoopeeper-unexpected-exception-java-nio-channels-cancelledkeyexception/</link>
      <pubDate>Mon, 18 Feb 2019 12:20:17 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/zoopeeper-unexpected-exception-java-nio-channels-cancelledkeyexception/</guid>
      <description>在运维过程中zookeeper(版本：3.4.9)出现下面的异常：
2019-02-18 11:12:04,043 [myid:] - ERROR [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@178] - Unexpected Exception: java.nio.channels.CancelledKeyException at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73) at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77) at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:151) at org.apache.zookeeper.server.ZooKeeperServer.finishSessionInit(ZooKeeperServer.java:663) at org.apache.zookeeper.server.ZooKeeperServer.revalidateSession(ZooKeeperServer.java:625) at org.apache.zookeeper.server.ZooKeeperServer.reopenSession(ZooKeeperServer.java:633) at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:926) at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:418) at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:198) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:244) at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203) at java.lang.Thread.run(Thread.java:745) 2019-02-18 11:12:04,100 [myid:] - ERROR [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@178] - Unexpected Exception: java.nio.channels.CancelledKeyException at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73) at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:77) at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:151) at org.apache.zookeeper.server.ZooKeeperServer.finishSessionInit(ZooKeeperServer.java:663) at org.apache.zookeeper.server.ZooKeeperServer.revalidateSession(ZooKeeperServer.java:625) at org.apache.zookeeper.server.ZooKeeperServer.reopenSession(ZooKeeperServer.java:633) at org.apache.zookeeper.server.ZooKeeperServer.processConnectRequest(ZooKeeperServer.java:926) at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:418) at org.apache.zookeeper.server.NIOServerCnxn.readPayload(NIOServerCnxn.java:198) at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:244) at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203) at java.lang.Thread.run(Thread.java:745)  这个是这个版本的zookeeper的一个bug，具体参见：https://issues.</description>
    </item>
    
    <item>
      <title>Java中的堆和栈</title>
      <link>https://wenchao.ren/posts/java%E4%B8%AD%E7%9A%84%E5%A0%86%E5%92%8C%E6%A0%88/</link>
      <pubDate>Fri, 15 Feb 2019 20:37:45 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java%E4%B8%AD%E7%9A%84%E5%A0%86%E5%92%8C%E6%A0%88/</guid>
      <description>堆和栈都是Java用来在RAM中存放数据的地方。
堆   Java的堆是一个运行时数据区，类的对象从堆中分配空间。这些对象通过new等指令建立，通过垃圾回收器来销毁。
  堆的优势是可以动态地分配内存空间，需要多少内存空间不必事先告诉编译器，因为它是在运行时动态分配的。但缺点是，由于需要在运行时动态分配内存，所以存取速度较慢。
  堆内存满的时候抛出java.lang.OutOfMemoryError: Java Heap Space错误
  可以使用-Xms和-Xmx JVM选项定义开始的大小和堆内存的最大值
  存储在堆中的对象是全局可以被其他线程访问的
  栈  栈中主要存放一些基本数据类型的变量（byte，short，int，long，float，double，boolean，char）和对象的引用，但对象本身不存放在栈中，而是存放在堆（new 出来的对象）或者常量池中(对象可能在常量池里)（字符串常量对象存放在常量池中。）。 栈的优势是，存取速度比堆快，栈数据可以共享。但缺点是，存放在栈中的数据占用多少内存空间需要在编译时确定下来，缺乏灵活性。 当栈内存满的时候，Java抛出java.lang.StackOverFlowError 和堆内存比，栈内存要小的多 明确使用了内存分配规则（LIFO） 可以使用-Xss定义栈的大小 栈内存不能被其他线程所访问。  静态域 存放静态成员（static定义的）
常量池 存放字符串常量和基本类型常量（public static final）
举例说明栈数据可以共享 String 可以用以下两种方式来创建：
String str1 = newString(&amp;quot;abc&amp;quot;); String str2 = &amp;quot;abc&amp;quot;;  第一种使用new来创建的对象，它存放在堆中。每调用一次就创建一个新的对象。
第二种是先在栈中创建对象的引用str2，然后查找栈中有没有存放“abc”，如果没有，则将“abc”存放进栈，并将str2指向“abc”，如果已经有“abc”， 则直接将str2指向“abc”。
public static void main(String[] args) { String str1 = newString(&amp;quot;abc&amp;quot;); String str2 = newString(&amp;quot;abc&amp;quot;); System.out.println(str1 == str2); }  输出结果为：false</description>
    </item>
    
    <item>
      <title>java中创建Completed future</title>
      <link>https://wenchao.ren/posts/java%E4%B8%AD%E5%88%9B%E5%BB%BAcompleted-future/</link>
      <pubDate>Fri, 15 Feb 2019 20:25:53 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java%E4%B8%AD%E5%88%9B%E5%BB%BAcompleted-future/</guid>
      <description>在Java中如何创建Completed future呢？
Java8中可以Future future = CompletableFuture.completedFuture(value); Guava中可以Futures.immediateFuture(value) Apache commons Lang中可以Future&amp;lt;T&amp;gt; future = ConcurrentUtils.constantFuture(T myValue);</description>
    </item>
    
    <item>
      <title>为Spring boot项目增加Servlet</title>
      <link>https://wenchao.ren/posts/%E4%B8%BAspring-boot%E9%A1%B9%E7%9B%AE%E5%A2%9E%E5%8A%A0servlet/</link>
      <pubDate>Fri, 15 Feb 2019 20:08:30 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%B8%BAspring-boot%E9%A1%B9%E7%9B%AE%E5%A2%9E%E5%8A%A0servlet/</guid>
      <description>为Spring boot项目增加Servlet有好多种方式
方式1 Just add a bean for the servlet. It&amp;rsquo;ll get mapped to /{beanName}/.
@Bean public Servlet foo() { return new FooServlet(); }  Note that if you actually want it mapped to /something/* rather than /something/ you will need to use ServletRegistrationBean
方式2 使用ServletRegistrationBean
@Bean public ServletRegistrationBean servletRegistrationBean(){ return new ServletRegistrationBean(new FooServlet(),&amp;quot;/someOtherUrl/*&amp;quot;); }  如果想增加多个的话，就类似下面的方式
@Bean public ServletRegistrationBean axisServletRegistrationBean() { ServletRegistrationBean registration = new ServletRegistrationBean(new AxisServlet(), &amp;quot;/services/*&amp;quot;); registration.addUrlMappings(&amp;quot;*.jws&amp;quot;); return registration; } @Bean public ServletRegistrationBean adminServletRegistrationBean() { return new ServletRegistrationBean(new AdminServlet(), &amp;quot;/servlet/AdminServlet&amp;quot;); }  方式3 通过实现WebApplicationInitializer或者ServletContextInitializer或者ServletContainerInitializer接口</description>
    </item>
    
    <item>
      <title>谈谈程序员成长</title>
      <link>https://wenchao.ren/posts/%E8%B0%88%E8%B0%88%E7%A8%8B%E5%BA%8F%E5%91%98%E6%88%90%E9%95%BF/</link>
      <pubDate>Fri, 15 Feb 2019 02:33:17 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E8%B0%88%E8%B0%88%E7%A8%8B%E5%BA%8F%E5%91%98%E6%88%90%E9%95%BF/</guid>
      <description>本篇文章主要侧重的是程序员的硬技能方面的，不涉及软技能。
关于程序员的成长，网上有很多人都在谈，也有很多的文章，我也看过许多，个人感觉写的最好的是下面2个文章，建议大家阅读。
 如何快速成长为技术大牛？阿里资深技术专家的总结亮了！ 张一鸣：10年面试2000人，我发现混的好的人，全都有同一个特质。  我此处整理一下：
误区 拜大牛为师  大牛很忙，不太可能单独给你开小灶 大牛不多，不太可能每个团队都有技术大牛，只能说团队里面会有比你水平高的人，即使他每天给你开小灶，最终你也只能提升到他的水平  综合上述的几个原因，我认为对于大部分人来说，要想成为技术大牛，首先还是要明白“主要靠自己”这个道理，适当的时候可以通过请教大牛或者和大牛探讨来提升自己，但大部分时间还是自己系统性、有针对性的提升。
业务代码一样很牛逼 有人认为写业务代码一样可以很牛逼，理由是业务代码一样可以有各种技巧，例如可以使用封装和抽象使得业务代码更具可扩展性，可以通过和产品多交流以便更好的理解和实现业务，日志记录好了问题定位效率可以提升10倍等等。
业务代码一样有技术含量，这点是肯定的，业务代码中的技术是每个程序员的基础，但只是掌握了这些技巧，并不能成为技术大牛，就像游戏中升级打怪一样，开始打小怪，经验值很高，越到后面经验值越少，打小怪已经不能提升经验值了，这个时候就需要打一些更高级的怪，刷一些有挑战的副本了，成为技术大牛的路也是类似的，你要 不断的提升自己的水平，然后面临更大的挑战，通过应对这些挑战从而使自己水平更上一级，然后如此往复。所以我认为：业务代码都写不好的程序员肯定无法成为技术大牛，但只把业务代码写好的程序员也还不能成为技术大牛。
上班太忙没时间自己学习 很多人认为自己没有成为技术大牛并不是自己不聪明，也不是自己不努力，而是中国的这个环境下，技术人员加班都太多了，导致自己没有额外的时间进行学习。
这个理由有一定的客观性，毕竟和欧美相比，我们的加班确实要多一些，但这个因素只是一个需要克服的问题，并不是不可逾越的鸿沟，毕竟我们身边还是有那么多的大牛也是在中国这个环境成长起来的。
我认为有几个误区导致了这种看法的形成：
1）上班做的都是重复工作，要想提升必须自己额外去学习 2）学习需要大段的连续时间
正确的做法 做的更多，做的比你主管安排给你的任务更多。 要想有机会，首先你得从人群中冒出来，要想冒出来，你就必须做到与众不同，要做到与众不同，你就要做得更多！不分哪些是我该做的、哪些不是我该做的
怎么做得更多呢？可以从以下几个方面着手：
 熟悉更多业务，不管是不是你负责的；熟悉更多代码，不管是不是你写的 熟悉端到端 比如说你负责web后台开发，但实际上用户发起一个http请求，要经过很多中间步骤才到你的服务器（例如浏览器缓存、DNS、nginx等），服务器一般又会经过很多处理才到你写的那部分代码（路由、权限等）这整个流程中的很多系统或者步骤，绝大部分人是不可能去参与写代码的，但掌握了这些知识对你的综合水平有很大作用，例如方案设计、线上故障处理这些更加有含金量的技术工作都需要综合技术水平。 “系统性”、“全局性”、“综合性”这些字眼看起来比较虚，但其实都是技术大牛的必备的素质，要达到这样的境界，必须去熟悉更多系统、业务、代码。 自学 以垃圾回收为例，我自己平时就抽时间学习了这些知识，学了1年都没用上，但后来用上了几次，每次都解决了卡死的大问题，而有的同学，写了几年的java代码，对于stop-the-world是什么概念都不知道，更不用说去优化了。  这一点和张一鸣说的是完全吻合的：
 我工作时，不分哪些是我该做的、哪些不是我该做的 我做事从不设边界  做的更好 要知道这个世界上没有完美的东西，你负责的系统和业务，总有不合理和可以改进的地方，这些“不合理”和“可改进”的地方，都是更高级别的怪物，打完后能够增加更多的经验值。识别出这些地方，并且给出解决方案，然后向主管提出，一次不行两次，多提几次，只要有一次落地了，这就是你的机会。
多练 在做职业等级沟通的时候，发现有很多同学确实也在尝试Do more、Do better，但在执行的过程中，几乎每个人都遇到同一个问题：光看不用效果很差，怎么办？
例如：
 学习了jvm的垃圾回收，但是线上比较少出现FGC导致的卡顿问题，就算出现了，恢复业务也是第一位的，不太可能线上出现问题然后让每个同学都去练一下手，那怎么去实践这些jvm的知识和技能呢？ Netty我也看了，也了解了Reactor的原理，但是我不可能参与Netty开发，怎么去让自己真正掌握Reactor异步模式呢？ 看了《高性能MySQL》，但是线上的数据库都是DBA管理的，测试环境的数据库感觉又是随便配置的，我怎么去验证这些技术呢？ 框架封装了DAL层，数据库的访问我们都不需要操心，我们怎么去了解分库分表实现？  诸如此类问题还有很多，我这里分享一下个人的经验，其实就是3个词：learning、trying、teaching！
Learning 这个是第一阶段，看书、google、看视频、看别人的博客都可以，但要注意一点是 “系统化”，特别是一些基础性的东西，例如JVM原理、Java编程、网络编程，HTTP协议等等，这些基础技术不能只通过google或者博客学习，我的做法一般是先完整的看完一本书全面的了解，然后再通过google、视频、博客去有针对性的查找一些有疑问的地方，或者一些技巧。
Trying 这个步骤就是解答前面提到的很多同学的疑惑的关键点，形象来说就是“自己动手丰衣足食”，也就是自己去尝试搭建一些模拟环境，自己写一些测试程序。例如：
 Jvm垃圾回收：可以自己写一个简单的测试程序，分配内存不释放，然后调整各种jvm启动参数，再运行的过程中使用jstack、jstat等命令查看jvm的堆内存分布和垃圾回收情况。这样的程序写起来很简单，简单一点的就几行，复杂一点的也就几十行。 Reactor原理：自己真正去尝试写一个Reactor模式的Demo，不要以为这个很难，最简单的Reactor模式代码量（包括注释）不超过200行（可以参考Doug Lee的PPT）。自己写完后，再去看看netty怎么做，一对比理解就更加深刻了。 MySQL：既然有线上的配置可以参考，那可以直接让DBA将线上配置发给我们（注意去掉敏感信息），直接学习；然后自己搭建一个MySQL环境，用线上的配置启动；要知道很多同学用了很多年MySQL，但是连个简单的MySQL环境都搭不起来。 框架封装了DAL层：可以自己用JDBC尝试去写一个分库分表的简单实现，然后与框架的实现进行对比，看看差异在哪里。 用浏览器的工具查看HTTP缓存实现，看看不同种类的网站，不同类型的资源，具体是如何控制缓存的；也可以自己用Python写一个简单的HTTP服务器，模拟返回各种HTTP Headers来观察浏览器的反应。  当然，如果能够在实际工作中使用，效果会更好，毕竟实际的线上环境和业务复杂度不是我们写个模拟程序就能够模拟的，但这样的机会可遇不可求，大部分情况我们还真的只能靠自己模拟，然后等到真正业务要用的时候，能够信手拈来。</description>
    </item>
    
    <item>
      <title>谈谈面试</title>
      <link>https://wenchao.ren/posts/%E8%B0%88%E8%B0%88%E9%9D%A2%E8%AF%95/</link>
      <pubDate>Fri, 15 Feb 2019 01:55:51 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E8%B0%88%E8%B0%88%E9%9D%A2%E8%AF%95/</guid>
      <description>今晚看到了这篇文章最近面试Java后端开发的感受, 深有感触，感觉写的特别好，有感而发，所以打断写这么一篇文章，来谈谈我的经验吧。
面试是一个综合评分的过程 首先一定要认识到面试是个综合评分的过程，不一定仅仅是考察面试者的技术能力，很多时候都会有一定（甚至很大）的运气成分，这一点其实很像公司内部的 晋级答辩一样，这一点一定要深刻理解。比如碰上一个性格不错，有礼貌的面试官，通过的可能性大多数情况下都会比全程绷着脸，接N个电话的面试官好一些。 所以即便最后最差情况，面试没过，不一定代表你能力不行，可能是技能和工作要求不匹配啊，可能是运气不行啊，说不定换一个面试官就过了等等的，都是可能 的，所以首先要正确看待面试这个环节。
面试前 不可否认的是，跳槽可能（基本）是互联网行业涨薪最快的手段了。而跳槽以后的薪资，虽然会在一定程度上参考上一家公司的薪资水平，但是只要你在面试阶段 的表现能够表现出：【上一家公司在薪资方面亏了我，我其实更值钱】这种情况的话，一般情况下，新公司在初步评级，以及你在和HR的薪资谈判过程，会占有很大 的主动性，所以面试期间的表现很重要。
为了面试期间表现的好，那么一定是需要提前准备的。虽然我不太喜欢突击准备，更喜欢平时多留心留意。但是不可否认面试其实是一个异常情况出现概率很大的事情。指不定面试官会问到哪里，所以面试者首先一定要尽可能的准备充分，并且有意的在简历以及在面试过程中，留下一些【坑】来引导面试官跟随自己提前准备好 的计划来面试，这样可以减少面试过程中异常情况发生。
简历不要瞎编，项目经验不要瞎编，一定要能自圆其说，最起码写在简历上面的要不怕被人问，如果被人三两句问住那就不好了，所以尽可能的多考虑一下面试官 会问什么，同时也百度一下最近的面试题，自己也做做相关的面试题。
面试期间 面试期间要注意的东西其实很多时候是表达能力。不能茶壶里面煮饺子，倒不出来。
尽量提前演练几下，提前想好如果面试官问xxx的时候，怎么回答，这样就不至于太仓促了。另外贴一下这篇文章最近面试Java后端开发的感受中提到的一些注意点，我觉的写的挺好的：
 别让人感觉你只会山寨别人的代码 单机版够用？适当了解些分布式 别就知道增删改查，得了解性能优化 围绕数据结构和性能优化准备面试题 至少了解如何看日志排查问题 通读一段底层代码，作为加分项 把上述技能嵌入到你做过的项目里  我总结了一下主要突出下面2个方面：
大家都能做的事情，你做的比别人好 首先普通的事情能做，这是最基本的要求，活都不会干，怎么可能面试通过呢。其次一定要让别人觉的大家都能做的事情，你已经做的比别人好了。就如同上面提到的那篇文章说的：别让人感觉你只会山寨别人的代码。要能发现当前的不足和并给出解决方案。如果和大多数人的水平差不多了，在这点就没法体现出你的优势了。
举几个例子：
 都在用Spring + Mybatis + mysql架构，现有的系统的问题在哪，你做了哪些改进和优化 线上出现问题了，你如何更快速的排查出来了呢？有没有自己的一套方法论和问题QA集 现有项目代码存在安全漏洞，你是否主动发现并fix上线了呢？ 项目有delay了，是否主动推动了呢？  别人不知道的你知道，别人干不了的你可以 别人不知道的你知道，别人干不了的你可以，这才是你真正和HR谈价钱的资本。所以一定要注重这个表达。当然了很少出现别人完全不知道的和别人完全干不了的情况，所以很多的时候是需要扩展知识的深度和广度
比如：
 熟悉Nginx dubbo redis等的原理和底层 比如熟悉业界的cloud native等  最后希望大家都能找到好工作，拿到好工资</description>
    </item>
    
    <item>
      <title>Mac privoxy&#43;Shadowsocks&#43;iTerm2走代理</title>
      <link>https://wenchao.ren/posts/mac-privoxy-shadowsocks-iterm2%E8%B5%B0%E4%BB%A3%E7%90%86/</link>
      <pubDate>Fri, 15 Feb 2019 00:12:33 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/mac-privoxy-shadowsocks-iterm2%E8%B5%B0%E4%BB%A3%E7%90%86/</guid>
      <description>本篇文章讲述如何通过Shadowsocks和privoxy让mac的终端terminal可以翻墙
首先下载安装privoxy
brew install privoxy  默认就会启动privoxy，可通过以下终端命令查看privoxy进程是否启动成功：
ps aux | grep privoxy  然后配置privoxy，配置文件在/usr/local/etc/privoxy/config, 在文件末尾的listen-address 127.0.0.1:8118的下一行增加： forward-socks5 / 127.0.0.1:1086 .，其中1086是shadowsocks的本地Sock5监听端口
终端执行以下两条命令即可访问privoxy：
export http_proxy=&#39;http://localhost:8118&#39; export https_proxy=&#39;http://localhost:8118&#39;  一般建议将这2个命令增加到zsh的.zshrc文件中去。
至于terminal中ping google失败的问题，请参考请问如何在ss代理下ping通google.com?, 其实此时已经termianl可以访问外网了，不信的话试试：
~ » curl -I -XGET https://www.google.com HTTP/1.1 200 Connection established HTTP/2 200 date: Thu, 14 Feb 2019 16:18:17 GMT expires: -1 cache-control: private, max-age=0 content-type: text/html; charset=ISO-8859-1 p3p: CP=&amp;quot;This is not a P3P policy! See g.co/p3phelp for more info.&amp;quot; server: gws x-xss-protection: 1; mode=block x-frame-options: SAMEORIGIN set-cookie: 1P_JAR=2019-02-14-16; expires=Sat, 16-Mar-2019 16:18:17 GMT; path=/; domain=.</description>
    </item>
    
    <item>
      <title>Linux查看端口占用情况</title>
      <link>https://wenchao.ren/posts/linux%E6%9F%A5%E7%9C%8B%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E6%83%85%E5%86%B5/</link>
      <pubDate>Thu, 14 Feb 2019 23:10:41 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/linux%E6%9F%A5%E7%9C%8B%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E6%83%85%E5%86%B5/</guid>
      <description>排查问题的时候，可能需要知道这个端口目前被哪个服务占用着，在linux中，一般会用到lsof和netstat这2个命令。比如检查80端口的占用情况
lsof [root@VM_43_49_centos ~]# sudo lsof -i:80 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME nginx 5358 root 6u IPv4 236554022 0t0 TCP *:http (LISTEN) nginx 5358 root 7u IPv6 236554023 0t0 TCP *:http (LISTEN) nginx 28325 nginx 6u IPv4 236554022 0t0 TCP *:http (LISTEN) nginx 28325 nginx 7u IPv6 236554023 0t0 TCP *:http (LISTEN)  netstat [root@VM_43_49_centos ~]# sudo netstat -tunlp | grep 80 tcp 0 0 0.0.0.0:80 0.</description>
    </item>
    
    <item>
      <title>dubbo-SPI扩展(二)</title>
      <link>https://wenchao.ren/posts/dubbo-spi%E6%89%A9%E5%B1%95%E4%BA%8C/</link>
      <pubDate>Wed, 13 Feb 2019 19:26:39 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/dubbo-spi%E6%89%A9%E5%B1%95%E4%BA%8C/</guid>
      <description>本篇文章主要描述一下dubbo的扩展点中的一些基本概念和常见的一些注解
基本概念 扩展点(Extension Point) 扩展点其实就是一个Java的接口。比如dubbo中的LoadBalance接口其实就是一个扩展点
@SPI(RandomLoadBalance.NAME) public interface LoadBalance { @Adaptive(&amp;quot;loadbalance&amp;quot;) &amp;lt;T&amp;gt; Invoker&amp;lt;T&amp;gt; select(List&amp;lt;Invoker&amp;lt;T&amp;gt;&amp;gt; invokers, URL url, Invocation invocation) throws RpcException; }  扩展(Extension) 扩展其实扩展点的实现类。比如以扩展点LoadBalance来说，RandomLoadBalance其实就是他的一个实现类，也是一个扩展。
package org.apache.dubbo.rpc.cluster.loadbalance; import org.apache.dubbo.common.URL; import org.apache.dubbo.rpc.Invocation; import org.apache.dubbo.rpc.Invoker; import java.util.List; import java.util.concurrent.ThreadLocalRandom; /** * random load balance. * */ public class RandomLoadBalance extends AbstractLoadBalance { public static final String NAME = &amp;quot;random&amp;quot;; @Override protected &amp;lt;T&amp;gt; Invoker&amp;lt;T&amp;gt; doSelect(List&amp;lt;Invoker&amp;lt;T&amp;gt;&amp;gt; invokers, URL url, Invocation invocation) { int length = invokers.</description>
    </item>
    
    <item>
      <title>dubbo-SPI扩展(一)</title>
      <link>https://wenchao.ren/posts/dubbo-spi%E6%89%A9%E5%B1%95%E4%B8%80/</link>
      <pubDate>Wed, 13 Feb 2019 14:11:24 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/dubbo-spi%E6%89%A9%E5%B1%95%E4%B8%80/</guid>
      <description>本篇文章描述一下dubbo的扩展性实现，主要有下面几个部分：
 什么叫可扩展性 常见的扩展性的解决方案 java spi简介 为什么dubbo不采用java spi，而是自己实现一个SPI机制呢 dubbo spi基本使用 dubbo扩展点的基本概念 dubbo SPI源码阅读  本篇文章也参考了很多业界资料，详见文件结尾
什么叫可扩展性 如同罗马不是一天建成的，任何系统都一定是从小系统不断发展成为大系统的，想要从一开始就把系统设计的足够完善是不可能的，相反的，我们应该关注当下的需求，然后再不断地对系统进行迭代。在代码层面，要求我们适当的对关注点进行抽象和隔离，在软件不断添加功能和特性时，依然能保持良好的结构和可维护性，同时允许第三方开发者对其功能进行扩展。在某些时候，软件设计者对扩展性的追求甚至超过了性能。
在谈到软件设计时，可扩展性一直被谈起，那到底什么才是可扩展性，什么样的框架才算有良好的可扩展性呢？它必须要做到以下两点:
 作为框架的维护者，在添加一个新功能时，只需要添加一些新代码，而不用大量的修改现有的代码，即符合开闭原则。 作为框架的使用者，在添加一个新功能时，不需要去修改框架的源码，在自己的工程中添加代码即可。  Dubbo很好的做到了上面两点。这要得益于Dubbo的微内核+插件的机制。接下来的章节中我们会慢慢揭开Dubbo扩展机制的神秘面纱。
常见的扩展性的解决方案  Factory模式 IoC容器 OSGI容器  Dubbo作为一个框架，不希望强依赖其他的IoC容器，比如Spring，Guice。OSGI也是一个很重的实现，不适合Dubbo。最终Dubbo的实现参考了Java原生的SPI机制，但对其进行了一些扩展，以满足Dubbo的需求。
java spi简介 SPI 全称为 Service Provider Interface，是一种服务发现机制。SPI 的本质是将接口实现类的全限定名配置在文件中，并由服务加载器读取配置文件，加载实现类。这样可以在运行时，动态为接口替换实现类。正因此特性，我们可以很容易的通过 SPI 机制为我们的程序提供拓展功能。
本节通过一个示例演示 Java SPI 的使用方法。首先，我们定义一个接口，名称为 Robot。
public interface Robot { void sayHello(); }  接下来定义两个实现类，分别为 OptimusPrime 和 Bumblebee。
public class OptimusPrime implements Robot { @Override public void sayHello() { System.out.println(&amp;quot;Hello, I am Optimus Prime.</description>
    </item>
    
    <item>
      <title>dubbo自定义filter</title>
      <link>https://wenchao.ren/posts/dubbo%E8%87%AA%E5%AE%9A%E4%B9%89filter/</link>
      <pubDate>Tue, 12 Feb 2019 17:50:57 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/dubbo%E8%87%AA%E5%AE%9A%E4%B9%89filter/</guid>
      <description>dubbo的扩展性是特别的好，本篇文章通过例子来说明如何自定义dubbo的filter。为了文章完整性，贴一下官网对于filter的一些说明。
扩展说明 服务提供方和服务消费方调用过程拦截，Dubbo 本身的大多功能均基于此扩展点实现，每次远程方法执行，该拦截都会被执行，请注意对性能的影响。
约定：
 用户自定义 filter 默认在内置 filter 之后。 特殊值 default，表示缺省扩展点插入的位置。比如：filter=&amp;quot;xxx,default,yyy&amp;quot;，表示 xxx`` 在缺省 filter 之前，yyy在缺省filter` 之后。 特殊符号 -，表示剔除。比如：filter=&amp;quot;-foo1&amp;quot;，剔除添加缺省扩展点 foo1。比如：filter=&amp;quot;-default&amp;quot;，剔除添加所有缺省扩展点。 provider 和 service 同时配置的 filter 时，累加所有 filter，而不是覆盖。比如：&amp;lt;dubbo:provider filter=&amp;quot;xxx,yyy&amp;quot;/&amp;gt; 和 &amp;lt;dubbo:service filter=&amp;quot;aaa,bbb&amp;quot; /&amp;gt;，则 xxx,yyy,aaa,bbb 均会生效。如果要覆盖，需配置：&amp;lt;dubbo:service filter=&amp;quot;-xxx,-yyy,aaa,bbb&amp;quot; /&amp;gt;  扩展接口 org.apache.dubbo.rpc.Filter
扩展配置 &amp;lt;!-- 消费方调用过程拦截 --&amp;gt; &amp;lt;dubbo:reference filter=&amp;quot;xxx,yyy&amp;quot; /&amp;gt; &amp;lt;!-- 消费方调用过程缺省拦截器，将拦截所有reference --&amp;gt; &amp;lt;dubbo:consumer filter=&amp;quot;xxx,yyy&amp;quot;/&amp;gt; &amp;lt;!-- 提供方调用过程拦截 --&amp;gt; &amp;lt;dubbo:service filter=&amp;quot;xxx,yyy&amp;quot; /&amp;gt; &amp;lt;!-- 提供方调用过程缺省拦截器，将拦截所有service --&amp;gt; &amp;lt;dubbo:provider filter=&amp;quot;xxx,yyy&amp;quot;/&amp;gt;  已知扩展  org.apache.dubbo.rpc.filter.EchoFilter org.apache.dubbo.rpc.filter.GenericFilter org.apache.dubbo.rpc.filter.GenericImplFilter org.apache.dubbo.rpc.filter.TokenFilter org.</description>
    </item>
    
    <item>
      <title>terminal快捷键</title>
      <link>https://wenchao.ren/posts/terminal%E5%BF%AB%E6%8D%B7%E9%94%AE/</link>
      <pubDate>Mon, 11 Feb 2019 17:21:47 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/terminal%E5%BF%AB%E6%8D%B7%E9%94%AE/</guid>
      <description>终端跳转解说图 常用快捷键 编辑  Ctrl + a – 跳到行首 Ctrl + e – 跳到行尾 Ctrl + k – 删除当前光标至行尾内容 Ctrl + u – 删除当前光标至行首内容 Ctrl + w – 删除当前光标至词首内容 Ctrl + y – 将剪切的内容粘贴在光标后 Ctrl + xx – 在行首和当前光标处(来回)移动 Alt + b – 跳到词首 Alt + f – 跳到词尾 Alt + d – 删除自光标处起的单词内容 Alt + c – 大写光标处的字符（注：该条内容与原文不同） Alt + u – 大写自光标处起的单词内容 Alt + l – 小写自光标处起的单词内容 Alt + t – 将光标处单词与上一个词交换 Ctrl + f – 向前移动一个字符(相当于按向左箭头) Ctrl + b – 向后移动一个字符(相当于按向右箭头) Ctrl + d – 删除光标后一个字符（相当于按Delete） Ctrl + h – 删除光标前一个字符（相当于按后退键） Ctrl + t – 交换光标处的两个字符  搜索  Ctrl + r – 反向搜索历史命令 Ctrl + g – 退出历史搜索模式（相当于按Esc） Ctrl + p – 上一个历史命令（相当于按向上箭头） Ctrl + n – 下一个历史命令（相当于按向下箭头） Alt + .</description>
    </item>
    
    <item>
      <title>MAT对象之间的差异浅堆和保留堆</title>
      <link>https://wenchao.ren/posts/mat%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%E6%B5%85%E5%A0%86%E5%92%8C%E4%BF%9D%E7%95%99%E5%A0%86/</link>
      <pubDate>Mon, 11 Feb 2019 16:45:26 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/mat%E5%AF%B9%E8%B1%A1%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%E6%B5%85%E5%A0%86%E5%92%8C%E4%BF%9D%E7%95%99%E5%A0%86/</guid>
      <description>原文地址：SHALLOW HEAP, RETAINED HEAP
更贴切的标题应该是Difference between Eclipse MAT objects Shallow Heap and Retained Heap
Eclipse MAT (Memory Analyzer Tool) is a powerful tool to analyze heap dumps. It comes quite handy when you are trying to debug memory related problems. In Eclipse MAT two types of object sizes are reported:
 Shallow Heap Retained Heap  In this article lets study the difference between them. Let’s study how are they calculated?</description>
    </item>
    
    <item>
      <title>ShadowsocksX-NG 1.8.2设置kcptun</title>
      <link>https://wenchao.ren/posts/shadowsocksx-ng-1-8-2%E8%AE%BE%E7%BD%AEkcptun/</link>
      <pubDate>Fri, 01 Feb 2019 17:54:00 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/shadowsocksx-ng-1-8-2%E8%AE%BE%E7%BD%AEkcptun/</guid>
      <description>在自己的mac上升级了一下ShadowsocksX-NG到1.8.2版本。发现无法使用了，检查发现新版本中没有设置kcptun的地方了，
新版本在【服务器设置】的地方
 插件输入框中写kcptun 插件选项写：key=【此处为自己的key】;crypt=aes;mode=fast2;mtu=1350;sndwnd=2048;rcvwnd=2048;datashard=10;parityshard=3;dscp=0  检查发现kcptun会自动打开
~ » ps -ef | grep kcp 501 31273 31272 0 5:52PM ?? 0:00.37 plugins/kcptun  参考资料
 1.8.2 kcptun 自带的和1.7.1不同，如何打开kcptun #926  </description>
    </item>
    
    <item>
      <title>RuntimeException in Action for tag [rollingPolicy] java.lang.IndexOutOfBoundsException: No group 1</title>
      <link>https://wenchao.ren/posts/runtimeexception-in-action-for-tag-rollingpolicy-java-lang-indexoutofboundsexception-no-group-1/</link>
      <pubDate>Tue, 29 Jan 2019 11:09:20 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/runtimeexception-in-action-for-tag-rollingpolicy-java-lang-indexoutofboundsexception-no-group-1/</guid>
      <description>今天一个同事咨询我，他们使用logback发布的时候出现下面的异常：
20:46:27,244 |-ERROR in ch.qos.logback.core.joran.spi.Interpreter@36:25 - RuntimeException in Action for tag [rollingPolicy] java.lang.IndexOutOfBoundsException: No group 1 at java.lang.IndexOutOfBoundsException: No group 1 at at java.util.regex.Matcher.group(Matcher.java:538) at at ch.qos.logback.core.rolling.helper.FileFilterUtil.extractCounter(FileFilterUtil.java:109) at at ch.qos.logback.core.rolling.helper.FileFilterUtil.findHighestCounter(FileFilterUtil.java:93) at at ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP.computeCurrentPeriodsHighestCounterValue(SizeAndTimeBasedFNATP.java:65) at at ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP.start(SizeAndTimeBasedFNATP.java:49) at at ch.qos.logback.core.rolling.TimeBasedRollingPolicy.start(TimeBasedRollingPolicy.java:90) at at ch.qos.logback.core.joran.action.NestedComplexPropertyIA.end(NestedComplexPropertyIA.java:167) at at ch.qos.logback.core.joran.spi.Interpreter.callEndAction(Interpreter.java:317) at at ch.qos.logback.core.joran.spi.Interpreter.endElement(Interpreter.java:196) at at ch.qos.logback.core.joran.spi.Interpreter.endElement(Interpreter.java:182) at at ch.qos.logback.core.joran.spi.EventPlayer.play(EventPlayer.java:62) at at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:149) at at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:135) at at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:99) at at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:49) at at ch.qos.logback.classic.util.ContextInitializer.configureByResource(ContextInitializer.java:75) at at ch.</description>
    </item>
    
    <item>
      <title>stop using TLS-SNI-01 with Certbot</title>
      <link>https://wenchao.ren/posts/stop-using-tls-sni-01-with-certbot/</link>
      <pubDate>Mon, 28 Jan 2019 12:12:21 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/stop-using-tls-sni-01-with-certbot/</guid>
      <description>今天收到一个来自letsencrypt的邮件：Action required: Let&#39;s Encrypt certificate renewals，简单的说就是Let’s Encrypt移除了对TLS-SNI-01的支持。 所以我就按照他们的指示，修改了一下我的certbot配置。操作步骤如下:
检查certbot的版本大于0.28 使用命令certbot --version来检查命令。我检查的时候发现我的版本比0.28低，所以我需要升级一下：
[root@VM_43_49_centos workspace]# certbot --version certbot 0.26.1 sudo yum upgrade certbot  我在使用sudo yum upgrade certbot以后，测试版本出现下面的异常：
[root@VM_43_49_centos workspace]# certbot --version Traceback (most recent call last): File &amp;quot;/usr/bin/certbot&amp;quot;, line 5, in &amp;lt;module&amp;gt; from pkg_resources import load_entry_point File &amp;quot;/usr/lib/python2.7/site-packages/pkg_resources.py&amp;quot;, line 3011, in &amp;lt;module&amp;gt; parse_requirements(__requires__), Environment() File &amp;quot;/usr/lib/python2.7/site-packages/pkg_resources.py&amp;quot;, line 626, in resolve raise DistributionNotFound(req) pkg_resources.DistributionNotFound: acme&amp;gt;=0.29.0 [root@VM_43_49_centos workspace]# yum list | grep acme Repository epel is listed more than once in the configuration  所以我还需要升级一下</description>
    </item>
    
    <item>
      <title>Error:java: 无效的标记: -version 解决</title>
      <link>https://wenchao.ren/posts/error-java-%E6%97%A0%E6%95%88%E7%9A%84%E6%A0%87%E8%AE%B0-version-%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Fri, 25 Jan 2019 15:05:36 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/error-java-%E6%97%A0%E6%95%88%E7%9A%84%E6%A0%87%E8%AE%B0-version-%E8%A7%A3%E5%86%B3/</guid>
      <description>使用最新版Intellij IDEA以后，编译项目出现Error:java: 无效的标记: -version 解决
后来排查发现是因为公司的super-pom中的maven-compiler-plugin的configuration有如下的配置：
&amp;lt;compilerArgs&amp;gt; - &amp;lt;arg&amp;gt;-J-Duser.country=US&amp;lt;/arg&amp;gt; - &amp;lt;arg&amp;gt;-version&amp;lt;/arg&amp;gt; &amp;lt;/compilerArgs&amp;gt;  而这个配置会和新版本的idea冲突。把pom中的这个配置删除就好了</description>
    </item>
    
    <item>
      <title>git没有使用自己的用户</title>
      <link>https://wenchao.ren/posts/git%E6%B2%A1%E6%9C%89%E4%BD%BF%E7%94%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E7%94%A8%E6%88%B7/</link>
      <pubDate>Wed, 23 Jan 2019 00:30:57 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/git%E6%B2%A1%E6%9C%89%E4%BD%BF%E7%94%A8%E8%87%AA%E5%B7%B1%E7%9A%84%E7%94%A8%E6%88%B7/</guid>
      <description>昨晚因为一些原因删除了一波.ssh目录中的东西，导致今天在git pull 的时候出现需要我输入git@gitlab.corp.xxx.com的密码。这种问题一看就是没有识别我的gitlab用户。
一般这种问题有2种解决办法：
 走http协议 http协议需要你输入用户名和密码 走ssh协议 重新生成秘钥，然后将公钥copy到gitlab的ssh keys中  这里具体说说第二种解决办法：
 本次使用ssh-keygen -t rsa -C &amp;quot;用户名&amp;quot;，一路回车会在.ssh目录下 生成2个文件：id_rsa.pub和id_rsa文件 复制id_rsa.pub文件的内容到gitlab的profile setting -&amp;gt; SSH keys -&amp;gt; Add an SSH key  </description>
    </item>
    
    <item>
      <title>Spring的AntPathMatcher是个好东西</title>
      <link>https://wenchao.ren/posts/spring%E7%9A%84antpathmatcher%E6%98%AF%E4%B8%AA%E5%A5%BD%E4%B8%9C%E8%A5%BF/</link>
      <pubDate>Wed, 23 Jan 2019 00:30:17 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/spring%E7%9A%84antpathmatcher%E6%98%AF%E4%B8%AA%E5%A5%BD%E4%B8%9C%E8%A5%BF/</guid>
      <description>经常需要在各种中做一些模式匹配，正则表达式虽然是个好东西，但是Ant风格的匹配情况也非常的多。 这种情况下使用正则表达式不一定方便，而Spring提供的AntPathMatcher确可以帮助我们简化很多。
位于Spring-core中的org.springframework.util.AntPathMatcher使用起来非常简单：
public class AntPathMatcherTest { private AntPathMatcher pathMatcher = new AntPathMatcher(); @Test public void test() { pathMatcher.setCachePatterns(true); pathMatcher.setCaseSensitive(true); pathMatcher.setTrimTokens(true); pathMatcher.setPathSeparator(&amp;quot;/&amp;quot;); Assert.assertTrue(pathMatcher.match(&amp;quot;a&amp;quot;, &amp;quot;a&amp;quot;)); Assert.assertTrue(pathMatcher.match(&amp;quot;a*&amp;quot;, &amp;quot;ab&amp;quot;)); Assert.assertTrue(pathMatcher.match(&amp;quot;a*/**/a&amp;quot;, &amp;quot;ab/asdsa/a&amp;quot;)); Assert.assertTrue(pathMatcher.match(&amp;quot;a*/**/a&amp;quot;, &amp;quot;ab/asdsa/asdasd/a&amp;quot;)); Assert.assertTrue(pathMatcher.match(&amp;quot;*&amp;quot;, &amp;quot;a&amp;quot;)); Assert.assertTrue(pathMatcher.match(&amp;quot;*/*&amp;quot;, &amp;quot;a/a&amp;quot;)); } }  </description>
    </item>
    
    <item>
      <title>将已有的工程代码push到github或者gitlab</title>
      <link>https://wenchao.ren/posts/%E5%B0%86%E5%B7%B2%E6%9C%89%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81push%E5%88%B0github%E6%88%96%E8%80%85gitlab/</link>
      <pubDate>Wed, 23 Jan 2019 00:28:44 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%B0%86%E5%B7%B2%E6%9C%89%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%BB%A3%E7%A0%81push%E5%88%B0github%E6%88%96%E8%80%85gitlab/</guid>
      <description>执行下面的命令就好了
git init git add . git commit -m &amp;quot;Initial commit&amp;quot; git remote add origin &amp;lt;project url&amp;gt; git push -f origin master  不过有时候会在github或者gitlab上将master分支进行保护，所以可能需要先创建一个别的分支，然后merge就好了
参考资料  push-existing-project-into-github  </description>
    </item>
    
    <item>
      <title>Spark 通用数据访问</title>
      <link>https://wenchao.ren/posts/spark-%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE/</link>
      <pubDate>Wed, 23 Jan 2019 00:27:31 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/spark-%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE/</guid>
      <description>##Data abstractions RDD is the core abstraction in Apache Spark. It is an immutable, fault-tolerant distributed collection of statically typed objects that are usually stored in-memory. DataFrame abstraction is built on top of RDD and it adds “named” columns. Moreover, the Catalyst optimizer, under the hood, compiles the operations and generates JVM bytecode for efficient execution.
 However, the named columns approach gives rise to a new problem. Static type information is no longer available to the compiler, and hence we lose the advantage of compile-time type safety.</description>
    </item>
    
    <item>
      <title>理解Compressed Sparse Column Format (CSC)</title>
      <link>https://wenchao.ren/posts/%E7%90%86%E8%A7%A3compressed-sparse-column-format-csc/</link>
      <pubDate>Wed, 23 Jan 2019 00:25:21 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E7%90%86%E8%A7%A3compressed-sparse-column-format-csc/</guid>
      <description>最近在看《Spark for Data Science》这本书，阅读到《Machine Learning》这一节的时候被稀疏矩阵的存储格式CSC给弄的晕头转向的。所以专门写一篇文章记录一下我对这种格式的理解。
##目的 Compressed Sparse Column Format (CSC)的目的是为了压缩矩阵，减少矩阵存储所占用的空间。这很好理解，手法无法就是通过增加一些&amp;quot;元信息&amp;quot;来描述矩阵中的非零元素存储的位置(基于列)，然后结合非零元素的值来表示矩阵。这样在一些场景下可以减少矩阵存储的空间。
##Spark API
在Spark中我们一般创建这样的稀疏矩阵的API为：
package org.apache.spark.ml.linalg /** * Creates a column-major sparse matrix in Compressed Sparse Column (CSC) format. * * @param numRows number of rows * @param numCols number of columns * @param colPtrs the index corresponding to the start of a new column * @param rowIndices the row index of the entry * @param values non-zero matrix entries in column major */ @Since(&amp;quot;2.</description>
    </item>
    
    <item>
      <title>使用Numpy进行矩阵的基本运算</title>
      <link>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8numpy%E8%BF%9B%E8%A1%8C%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/</link>
      <pubDate>Wed, 23 Jan 2019 00:24:13 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8numpy%E8%BF%9B%E8%A1%8C%E7%9F%A9%E9%98%B5%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/</guid>
      <description>本文介绍了使用Python的Numpy库进行矩阵的基本运算 ##创建全0矩阵
# 创建3x5的全0矩阵 myZero = np.zeros([3, 5]) print myZero  输出结果：
[[ 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0.]]  ##创建全1矩阵
# 创建3x5的全1矩阵 myOnes = np.ones([3, 5]) print myOnes  输出结果：
[[ 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1.]]  ##创建0~1之间的随机矩阵
# 3x4的0~1之间的随机数矩阵 myRand = np.random.rand(3, 4) print myRand  输出结果为：</description>
    </item>
    
    <item>
      <title>促销系统设计</title>
      <link>https://wenchao.ren/posts/%E4%BF%83%E9%94%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Wed, 23 Jan 2019 00:19:49 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BF%83%E9%94%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</guid>
      <description>写在前面 首先必须得说一下，我并没有实际参与过电商系统相关的业务，我一直工作的项目组做的事情和本篇文章要讲的东西完全不同。因此本篇文章仅仅是我自己平时观察和构想的一些整理，如果有不太合理的地方，希望大家指正，先谢谢大家。
文章简介 在各大电商网站上，基本时时刻刻都可以看到促销活动。相信大家基本都参与过一些促销活动。随着业务的复杂化、运营的精细化，以及品类、平台、渠道的不断丰富，各种新的促销形式也层出不穷，贯穿从商品展示、搜索、购买、支付等整个流程。虽然促销的商品本身千差万别，但是但对于促销这个事情来说，又有很多共同的地方，本篇文章希望可以归纳总结出一套设计促销系统模型的方法论出来。
促销系统介绍 如果需要给促销一个定义的话，那么促销就是：
 在某个时间范围内，对满足某些条件的用户，给予满足某些约束的商品进行一定形式的优惠
 而促销系统就是为了支撑若干个这样的促销活动而设计出来的系统。 促销规则的生效页面是购物车页面和结算页面。在结算页面比购物车页面多出的是对运费的处理，其它的信息和购物车页面的信息是一致。只有在顾客将某个产品加入购物车后，基于购物车内的产品进行计算分析才会得出折扣后的价格、赠送或其它信息。当然在具体结算的时候，也会根据用户所选择的购物车中的项目重新计算折扣后的价格、赠送等其他信息的
常见的促销活动例子：  购买的图书满100减20，满200减50 购买某商品，赠送另外一个商品 满200元任选一个赠品 某商品特价 买A商品，在买B商品，则给予一定的折扣 满多少免运费 &amp;hellip;&amp;hellip;  促销系统模型的目标  功能强大 可扩展性好 与其他系统耦合度低  促销系统模型的设计 基本的促销模型 参考资料  当当11.11：促销系统与交易系统的重构实践 Craft6.cn 电商研发方案 - 促销规则、优惠券和活动模块分析和设计  </description>
    </item>
    
    <item>
      <title>RBAC权限系统设计</title>
      <link>https://wenchao.ren/posts/rbac%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Wed, 23 Jan 2019 00:18:31 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/rbac%E6%9D%83%E9%99%90%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</guid>
      <description>##Role Based Access Control
 Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within an enterprise. In this context, access is the ability of an individual user to perform a specific task, such as view, create, or modify a file.
 (The National Institute of Standards and Technology，美国国家标准与技术研究院)标准RBAC模型由4个部件模型组成，这4个部件模型分别是：
 基本模型RBAC0(Core RBAC） 角色分级模型RBAC1(Hierarchal RBAC) 角色限制模型RBAC2(Constraint RBAC) 统一模型RBAC3(Combines RBAC)  关于这四个区别，建议大家直接看本文参考资料中第二个链接：标准RBAC模型由4个部件模型，这篇文章说的很清楚。</description>
    </item>
    
    <item>
      <title>Spring中Enum的依赖注入</title>
      <link>https://wenchao.ren/posts/spring%E4%B8%ADenum%E7%9A%84%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</link>
      <pubDate>Wed, 23 Jan 2019 00:17:48 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/spring%E4%B8%ADenum%E7%9A%84%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5/</guid>
      <description>Spring 依赖注入很简单，没什么值得细说的。但是我之前遇到了一个场景，需要在一个Enum类中注入某一个service。 说实话之前没有遇到过这种情况。虽然我不赞同Enum类有过多的逻辑，但是没有办法，现实就是那么残酷。而且Enum确实可以通过一些手段来注入其他发service的。 比如下面的代码中，为EnumClass枚举类注入OtherService服务，代码示例如下：
public enum EnumClass { A(1), B(2); EnumClass(int id) { this.id = id; } private int id; private OtherService otherService; public int getId() { return id; } public OtherService getOtherService() { return otherService; } public void setOtherService(OtherService otherService) { this.otherService = otherService; } public ResponseType func(){ //use otherService do somethings return new ResponseType(); } @Component public static class EnumClassInner { @Autowired private OtherService otherService; @PostConstruct public void postConstruct() { for (EnumClass aEnum : EnumSet.</description>
    </item>
    
    <item>
      <title>跨域资源共享以及Spring MVC的支持</title>
      <link>https://wenchao.ren/posts/%E8%B7%A8%E5%9F%9F%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB%E4%BB%A5%E5%8F%8Aspring-mvc%E7%9A%84%E6%94%AF%E6%8C%81/</link>
      <pubDate>Wed, 23 Jan 2019 00:15:28 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E8%B7%A8%E5%9F%9F%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB%E4%BB%A5%E5%8F%8Aspring-mvc%E7%9A%84%E6%94%AF%E6%8C%81/</guid>
      <description>本来想写文章详细整理一下「跨域资源共享，以及在Spring MVC中如何实现」，但是发现网站上已经有很多文章总结了， 相信我在怎么写也和绝大多数文章的差别不大，所以我选中了5篇比较不错的文章（地址在文章参考资料中）作为引用。 所以读者直接看引用的这几篇文章就好了。
##参考资料
 浏览器同源政策及其规避方法 跨域资源共享 CORS 详解 Cross-origin resource sharing Enabling Cross Origin Requests for a RESTful Web Service CORS support in Spring Framework  </description>
    </item>
    
    <item>
      <title>使用七牛做博客图床</title>
      <link>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8%E4%B8%83%E7%89%9B%E5%81%9A%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/</link>
      <pubDate>Wed, 23 Jan 2019 00:13:07 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8%E4%B8%83%E7%89%9B%E5%81%9A%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/</guid>
      <description>经过对比了国内的一些图床以后，最后还是选择了七牛做图床。免费版的对于我这个小博客完全就够用了。 操作方式如下：
  首先注册七牛账号，然后并认证。认证方式我选择的是支付宝认证。
  然后创建一个空间
  在「对象管理」-&amp;gt;「存储资源列表」-&amp;gt;「添加」
  这块会生成一个测试域名，这个域名绝大多数情况下就够用了
  然后推荐使用极简图床，配置好自己的空间信息就可以使用了
  配置的时候域名写自己之前空间的测试域名就好了。至于空间名称就写自己之前创建的空间的名称
  AK和SK在「个人面板」-&amp;gt;「密匙管理」中可以找到
  </description>
    </item>
    
    <item>
      <title>本地搭建dubbo的运行环境</title>
      <link>https://wenchao.ren/posts/%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BAdubbo%E7%9A%84%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Wed, 23 Jan 2019 00:11:34 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BAdubbo%E7%9A%84%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83/</guid>
      <description>本篇文章主要讲述如何在本地搭建dubbo的运行环境
安装zk dubbo推荐使用zk来作为自己的注册中心，当然使用其余的实现来作为注册中心也是可以的。比如我之前就使用redis实现了一个注册中心。
Dubbo 未对 Zookeeper 服务器端做任何侵入修改，只需安装原生的 Zookeeper 服务器即可， 所有注册中心逻辑适配都在调用 Zookeeper 客户端时完成。
安装zk：
wget http://archive.apache.org/dist/zookeeper/zookeeper-3.3.3/zookeeper-3.3.3.tar.gz tar zxvf zookeeper-3.3.3.tar.gz cd zookeeper-3.3.3 cp conf/zoo_sample.cfg conf/zoo.cfg  配置:
vi conf/zoo.cfg  如果不需要集群，zoo.cfg 的内容如下 ：
tickTime=2000 initLimit=10 syncLimit=5 dataDir=/home/dubbo/zookeeper-3.3.3/data clientPort=2181  如果需要集群，zoo.cfg 的内容如下(其中 data 目录和 server 地址需改成你真实部署机器的信息)
tickTime=2000 initLimit=10 syncLimit=5 dataDir=/home/dubbo/zookeeper-3.3.3/data clientPort=2181 server.1=10.20.153.10:2555:3555 server.2=10.20.153.11:2555:3555  并在 data 目录下放置 myid 文件：
mkdir data vi myid  myid 指明自己的 id，对应上面 zoo.cfg 中 server. 后的数字，第一台的内容为 1，第二台的内容为 2，内容如下：</description>
    </item>
    
    <item>
      <title>git还原到之前版本</title>
      <link>https://wenchao.ren/posts/git%E8%BF%98%E5%8E%9F%E5%88%B0%E4%B9%8B%E5%89%8D%E7%89%88%E6%9C%AC/</link>
      <pubDate>Tue, 22 Jan 2019 10:54:41 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/git%E8%BF%98%E5%8E%9F%E5%88%B0%E4%B9%8B%E5%89%8D%E7%89%88%E6%9C%AC/</guid>
      <description>命令行操作：
 git log 查看之前的commit的id，找到想要还原的版本 git reset --hard 44bd896bb726be3d3815f1f25d738a9cd402a477 还原到之前的某个版本 git push -f origin master 强制push到远程  </description>
    </item>
    
    <item>
      <title>kafka listeners &amp; advertised.listeners</title>
      <link>https://wenchao.ren/posts/kafka-listeners-advertised-listeners/</link>
      <pubDate>Mon, 21 Jan 2019 20:15:33 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/kafka-listeners-advertised-listeners/</guid>
      <description>今天在日常使用spring-kafka消费kafka数据时发现连接不是kafka，出现下面的异常：
2019-01-21 16:55:58,675 WARN wtraceId[] wtracer[] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] c.w.f.w.k.LogConsumerConfiguration:45 - [wcollect]batch pull data from kafka error java.lang.IllegalStateException: No entry found for connection 30 at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:330) at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:134) at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:921) at org.apache.kafka.clients.NetworkClient.ready(NetworkClient.java:287) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.trySend(ConsumerNetworkClient.java:474) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:255) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236) at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1243) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1188) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1164) at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:728) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266) at java.util.concurrent.FutureTask.run(FutureTask.java) at java.lang.Thread.run(Thread.java:748)  从上面的异常中大致可以看到是初始化NetworkClient连接时出现问题，基本可以证明是连接不是kafka broker。
后来排查发现是因为我在我本地的代码中是通过ip+port的形式访问的，而我们的kafka broker配置了下面的参数：
############################# Socket Server Settings ############################# # The address the socket server listens on. It will get the value returned from # java.</description>
    </item>
    
    <item>
      <title>java中的日期pattern</title>
      <link>https://wenchao.ren/posts/java%E4%B8%AD%E7%9A%84%E6%97%A5%E6%9C%9Fpattern/</link>
      <pubDate>Mon, 21 Jan 2019 19:54:40 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java%E4%B8%AD%E7%9A%84%E6%97%A5%E6%9C%9Fpattern/</guid>
      <description>经常搞混java中的日期pattern，比如经常记混H和h的区别，所以专门整理一下，便于我以后查找
yyyy：年 MM：月 dd：日 hh：1~12小时制(1-12) HH：24小时制(0-23) mm：分 ss：秒 S：毫秒 E：星期几 D：一年中的第几天 F：一月中的第几个星期(会把这个月总共过的天数除以7) w：一年中的第几个星期 W：一月中的第几星期(会根据实际情况来算) a：上下午标识 k：和HH差不多，表示一天24小时制(1-24)。 K：和hh差不多，表示一天12小时制(0-11)。 z：表示时区 ```java 常用pattern： ```java yyyy-MM-dd HH:mm:ss.SSS yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSS&#39;Z&#39;  常用时区：
@JsonFormat(shape = JsonFormat.Shape.STRING, pattern = &amp;quot;yyyy-MM-dd&#39;T&#39;HH:mm:ss.SSS&#39;Z&#39;&amp;quot;, timezone = &amp;quot;GMT+8&amp;quot;)  日期和字符串互转：
private final static DateTimeFormatter fmt1 = DateTimeFormat.forPattern(&amp;quot;yyyy-MM-dd HH:mm:ss.SSS&amp;quot;); DateTime dateTime = DateTime.parse(date, fmt1) new DateTime().toString(&amp;quot;yyyy-MM-dd HH:mm:ss.SSS&amp;quot;) org.joda.time#Days  </description>
    </item>
    
    <item>
      <title>优化Elasticsearch的写入</title>
      <link>https://wenchao.ren/posts/%E4%BC%98%E5%8C%96elasticsearch%E7%9A%84%E5%86%99%E5%85%A5/</link>
      <pubDate>Thu, 17 Jan 2019 19:47:44 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BC%98%E5%8C%96elasticsearch%E7%9A%84%E5%86%99%E5%85%A5/</guid>
      <description>我们这边因为需要对trace系统的数据做一些高级查询，所以会将Span的可能会被用作搜索条件的信息写入elasticsearch中。 由于trace系统的数据量比较大，虽然trace系统本身的设计会有采样率这个东西来降低trace采集的数据量，但是本身还是 比较大的数据量。所以需要对es的写入做一些优化。这篇文章记录一下我们的优化项
分析我们场景的特点：
 写请求特别大 读请求很少，实时性要求低 trace系统对数据的可靠性要求低，但是要求写入及时（数据的价值会随着时间而降低）  贴一下我们优化以后的template设置：
{ &amp;quot;order&amp;quot;: 0, &amp;quot;index_patterns&amp;quot;: [ &amp;quot;trace.advanced.query-*&amp;quot; ], &amp;quot;settings&amp;quot;: { &amp;quot;index&amp;quot;: { &amp;quot;refresh_interval&amp;quot;: &amp;quot;120s&amp;quot;, &amp;quot;number_of_shards&amp;quot;: &amp;quot;10&amp;quot;, &amp;quot;translog&amp;quot;: { &amp;quot;flush_threshold_size&amp;quot;: &amp;quot;1024mb&amp;quot;, &amp;quot;sync_interval&amp;quot;: &amp;quot;120s&amp;quot;, &amp;quot;durability&amp;quot;: &amp;quot;async&amp;quot; }, &amp;quot;number_of_replicas&amp;quot;: &amp;quot;0&amp;quot; } }, &amp;quot;mappings&amp;quot;: {}, &amp;quot;aliases&amp;quot;: {} }  主要的优化步骤：
 关闭副本，设置number_of_replicas为0 在我们的场景下数据丢了是可以忍受的 调大translog.flush_threshold_size，我们设置的为1024mb 调大translog.sync_interval为120s 我们容许数据丢失，设置async模式  因为我们目前的es集群性能是足够的，所以并没有完全按照参考文章将 ELASTICSEARCH 写入速度优化到极限中的所有项目都优化。推荐大家阅读这个参考文章，比我写的好多了。
参考资料  将 ELASTICSEARCH 写入速度优化到极限  </description>
    </item>
    
    <item>
      <title>单据的设计</title>
      <link>https://wenchao.ren/posts/%E5%8D%95%E6%8D%AE%E7%9A%84%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Thu, 17 Jan 2019 19:03:55 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%8D%95%E6%8D%AE%E7%9A%84%E8%AE%BE%E8%AE%A1/</guid>
      <description>本文主要是初步记录一下关于单据零散的想法，看看是否可以形成一套可行的方法论。
单据实体 单据这个实体，其实从抽象的层面来说，它其实描述了一次行为：
 谁，在什么时间点，用了什么样的成本，对什么目标产生了一次什么样的行为
 如果让我们来对上面这句话进行解析，其实可以发现它可以拆分为下面几块：
 谁 时间信息 成本 行为描述 行为目标  下面我们来一次分析这个问题。
谁 一般订单的发起者其实都是【用户】，当然这个用户并不一定是C端用户。而用户的信息如何描述呢？一般的公司都会有【用户中心】，或者【供应商管理系统】等等的来描述【用户】的信息。做的好的公司也会有【用户画像】
时间信息 时间信息一般分为2类：
 行为动作的产生时间  比如用户的下单时间   行为动作所带来的影响发生的时间  用户购买的商品的出库时间 用户购买的商品的送货时间 用户购买的商品的预计收货时间 前者是用户主动产生的，后者其实对用户来说是被动的。    成本信息 用户在下订单的时候往往都会消耗一定的成本，这个成本并不一定是金钱。可能是：
 金钱 积分 优惠券 &amp;hellip;  ##行为描述
行为描述主要是用来解释行为，比如：
 购买行为 退货行为  不同的行为会有不同的属性信息，比如购买行为会有购买的数量，购买服务的持续时间等。
行为目标 行为的目标可以是物理实体，比如商品（一瓶可乐），也可以是虚拟实体，比如一次服务。
 商品 服务 流量 &amp;hellip;  而具体的目标也会有一大套自己的模型和属性。比如商品的sku就是一套很经典的模型。
单据本身的信息 单据本身也需要一些自己的属性信息：
 单号 单据类型 状态机  关于单据号的设计可以很有讲究，一般会有如下的考虑：
 单据号的形式，长度 是否有业务含义  无 有   业务含义的具体信息 业务含义占单据号的长度 是否连续（可遍历） 单据创建时间 单据生成的serverIP 路由信息 分库分表的依据 其他路由形式 单据数量增长对单据号的影响 单据号长度的划分 每秒最大单据数量预估 关于路由信息也有一个需要注意的点，就是要尽量做到数据分布平衡。  单据流程 单据的流程一般有正向流程和逆向流程。考虑到解耦和业务复杂度不可控导致的变数，一般情况下建议正向和逆向分开设计和实现。然后通过单号等信息进行关联。</description>
    </item>
    
    <item>
      <title>快速的熟悉陌生的系统</title>
      <link>https://wenchao.ren/posts/%E5%BF%AB%E9%80%9F%E7%9A%84%E7%86%9F%E6%82%89%E9%99%8C%E7%94%9F%E7%9A%84%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Thu, 17 Jan 2019 18:58:28 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E5%BF%AB%E9%80%9F%E7%9A%84%E7%86%9F%E6%82%89%E9%99%8C%E7%94%9F%E7%9A%84%E7%B3%BB%E7%BB%9F/</guid>
      <description>工作和学习过程中经常会遇到陌生的系统需要去熟悉，下面是我总结的一些自己的方法论，希望对你有所帮助。
了解系统Overview 首先我们需要对系统有一个Overview的了解，了解的方式可以是自己摸索，找对应系统负责人，团队leader聊，也可以自己找wiki，文档等方法。我简单的总结了一下步骤：
 熟悉系统的定位，明确系统要解决的主要问题 如果系统有对应的UI页面，比如对于web系统，可以申请系统的权限，然后登陆系统看看系统主要有哪些菜单（一般情况下菜单其实会对于系统的功能模块），每个菜单下面都有哪些菜单项，然后浏览一下每个菜单项对应的页面有哪些按钮，哪些表格，哪些表单等。这样会对系统有哪些模块、功能有一个大概的了解。 如果你幸运的话，你可以阅读系统使用手册、设计文档、需求文档等，当然大多数情况下是没有的。这个时候你其实更多的需要上一步骤对系统的大概理解，然后结合自己的工作经验，大致猜猜系统怎么设计实现的，其实对于大多数业务系统来说，看看功能模块其实是可以猜出个七七八八的系统设计和使用的情况的。当然了对于类似中间件那些系统（比如MQ，Config，Schedule）等，其实了解起来更加的迅速，因为大家基本在日常工作过程中都使用过这些系统，所以对于系统需要提供的基本功能和如何使用都比较熟悉，可能就是在系统的设计和实现上有所差异。 了解系统的上下游。上下游都是哪些团队的哪些系统？这些团队的负责人是谁？系统和上下游系统是如何交互的？交互方式是什么，交互的时机等。 了解系统的核心数据流向。清楚一个完成的数据链路是怎么样的。 如果可以的话，自己动手在系统的dev环境或者beta环境来点一点，创建一些数据，然后走走核心数据链路。 了解系统的核心监控指标。了解清楚核心监控指标，其实便于你理解系统的核心功能，并且方便日后对系统修改的时候，看看是否对核心监控指标有影响。 了解系统的部署情况。一般一个系统都会有好几个模块（module），清楚每个模块干啥的，这些模块之间的依赖关系（比如admin模块，Server模块，client模块），模块或者系统系统如何部署的，是部署在KVM上？Kubernetes上？多少个实例？多少台Server？Server啥配置？存储用的啥？MySQL？HBase？DB数据量大致多少？存储多少个G/T? 存储增长量多少？系统的QPS大致多少？ 按照上面的步骤操作完以后，相信你一定会对系统有一个明确的了解。这个时候其实你应该可以对系统的存储模型（数据模型）猜出个七七八八了，接下来可以看看系统的存储模型了，来验证我们的猜测。  阅读系统存储模型 此处说的存储模型也可以称为是数据模型。可以是MySQL表设计（有哪些表，表之间的关联关系，表的索引是哪些，唯一索引是哪些），Hbase的存储方式（rowKey是如何设计的，列簇如何弄的，有哪些列），也可以是抽象意义上的数据模型。
阅读系统的源码 基本了解系统功能以后，知道存储模型以后，大致怎么实现其实 应该也可以猜出来了。当然了很多功能其实的实现方式会有很多种，通过阅读系统的源码我们就可以知道系统是采用的哪种方式。如果遇到和我们猜测的实现方式不一样的话，其实可以思考一下为啥不一样？两种实现方式的区别在哪，孰优孰劣。
关于阅读源码，我整理了几个注意点
 首先让系统在本地/dev/beta跑起来，最好在本地，这样其实对系统的运行环境可以更加熟悉。 先从从核心数据链路的最开始进行源码阅读，第一次阅读只需要关心核心链路，分支链路第一次阅读的时候请忽略。 对核心逻辑流程进行debug。因为设计的比较好的系统，往往接口和实现是分离的，而且会有多个实现，光看接口其实有时候根本不清楚走的哪个实现，这个时候就需要进行debug了。有时候一些方法实现的可能很不清晰，这个时候通过debug，或者写main方法，单元测试等手段来了解。 开业手动画出系统核心流程的关键类的调用关系 阅读系统非核心逻辑代码 尝试讲给系统的开发人员，确认你理解的是正确的。对一些感觉设计不合理的点，咨询当时那么设计的原因  解决自己的疑惑，确认可优化点 一般情况下，我们在阅读完系统代码之后，会有自己的疑惑，也会发现一些设计不合理的点（虽然可能这个设计在当时的情况下是合理的），我们可以提出自己的优化方案，然后找对应系统的产品、开发、负责人确认。
结语 如果你完全按照上面的方式做了，还是没有对某个系统熟悉的话，那么就只能找对应系统的产品、开发、负责人具体咨询了。</description>
    </item>
    
    <item>
      <title>ApacheBench ab压测工具</title>
      <link>https://wenchao.ren/posts/apachebench-ab%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Thu, 17 Jan 2019 13:15:30 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/apachebench-ab%E5%8E%8B%E6%B5%8B%E5%B7%A5%E5%85%B7/</guid>
      <description>ab简介 ApacheBench 是 Apache服务器自带的一个web压力测试工具，简称ab。ab又是一个命令行工具，根据ab命令可以创建很多的并发访问线程，模拟多个访问者同时对某一URL地址进行访问，因此可以用来测试目标服务器的负载压力。
安装ab sudo apt-get install apache2-utils  参数列表 Usage: ab [options] [http[s]://]hostname[:port]/path Options are: -n requests Number of requests to perform //请求链接数 -c concurrency Number of multiple requests to make at a time //表示并发数 -t timelimit Seconds to max. to spend on benchmarking This implies -n 50000 -s timeout Seconds to max. wait for each response Default is 30 seconds -b windowsize Size of TCP send/receive buffer, in bytes -B address Address to bind to when making outgoing connections -p postfile File containing data to POST.</description>
    </item>
    
    <item>
      <title>org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available</title>
      <link>https://wenchao.ren/posts/org-elasticsearch-client-transport-nonodeavailableexception-none-of-the-configured-nodes-are-available/</link>
      <pubDate>Mon, 14 Jan 2019 23:18:14 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/org-elasticsearch-client-transport-nonodeavailableexception-none-of-the-configured-nodes-are-available/</guid>
      <description>问题描述 今天在操作es的过程中出现了异常:
org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available  而我创建es client的代码也很简单，核心是：
Settings settings = Settings.builder() .put(&amp;quot;cluster.name&amp;quot;, esClusterInfo.getClusterName()) .put(&amp;quot;client.transport.sniff&amp;quot;, true) //自动嗅探整个集群的状态，把集群中其他ES节点的ip添加到本地的客户端列表中 .build();  我的排查步骤 记录一下我当时的排查过程：
 看异常第一反应是集群有问题，但是排查集群的节点以后，发现集群的节点都是没问题的。 而后开始检查settings中设置的cluster.name的是否正确发现也是正确的 google发现很多人是因为将es集群的端口写错，也就是9300错写为9200，但是检查我的数据以后发现也是没问题的。 而后开始怀疑我设置的client.transport.sniff的缘故，因为这个参数的作用是：   使客户端去嗅探整个集群的状态，把集群中其它机器的ip地址加到客户端中。这样做的好处是，一般你不用手动设置集群里所有集群的ip到连接客户端，它会自动帮你添加，并且自动发现新加入集群的机器
 但是这个参数有一个问题就是：
 当ES服务器监听（publish_address ）使用内网服务器IP，而访问（bound_addresses ）使用外网IP时，不要设置client.transport.sniff为true。不设置client.transport.sniff时，默认为false(关闭客户端去嗅探整个集群的状态)。因为在自动发现时会使用内网IP进行通信，导致无法连接到ES服务器。因此此时需要直接使用addTransportAddress方法把集群中其它机器的ip地址加到客户端中。
 但是检查我的环境，发现我的环境全部是内网，所以设置不设置这个client.transport.sniff是没区别的。
最后持续google许久，也没发现问题，喝杯茶刷了一会手机突然想起印象中es client的版本和es集群的版本不一致也有可能 出问题，于是google了一波es 版本不一致这个关键字，果然是因为版本不一致导致的。
ES client和ES集群版本不一致的问题 我遇到的这个问题是es集群的版本比我使用的es client的版本低，相当于我使用高版本的client去访问低版本的集群，所以出现org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available问题。
因此我们平时需要注意es的版本问题。</description>
    </item>
    
    <item>
      <title>使用Let&#39;s Encrypt配置nginx证书</title>
      <link>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8let-s-encrypt%E9%85%8D%E7%BD%AEnginx%E8%AF%81%E4%B9%A6/</link>
      <pubDate>Thu, 27 Dec 2018 19:14:44 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E4%BD%BF%E7%94%A8let-s-encrypt%E9%85%8D%E7%BD%AEnginx%E8%AF%81%E4%B9%A6/</guid>
      <description>Automatically enable HTTPS on your website with EFF&amp;rsquo;s Certbot, deploying Let&amp;rsquo;s Encrypt certificates
主要参考：https://certbot.eff.org/lets-encrypt/centosrhel7-nginx
操作起来很简单，主要用到了下面的命令：
sudo yum install nginx sudo certbot --nginx sudo certbot renew --dry-run 0 0 1 * * certbot renew --post-hook &amp;quot;/usr/sbin/nginx -s reload&amp;quot;  </description>
    </item>
    
    <item>
      <title>Mac外接4k显示器选1080p字体发虚</title>
      <link>https://wenchao.ren/posts/mac%E5%A4%96%E6%8E%A54k%E6%98%BE%E7%A4%BA%E5%99%A8%E9%80%891080p%E5%AD%97%E4%BD%93%E5%8F%91%E8%99%9A/</link>
      <pubDate>Wed, 19 Dec 2018 22:28:30 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/mac%E5%A4%96%E6%8E%A54k%E6%98%BE%E7%A4%BA%E5%99%A8%E9%80%891080p%E5%AD%97%E4%BD%93%E5%8F%91%E8%99%9A/</guid>
      <description>MacBook外接4K显示器使用HDMI数据线选择缩放模式，然后选择1080p，会出现外接显示器字体发虚的问题，一般有下面几种的排查方式：
 检查数据线，尽量使用DP接口 hdmi数据线的话，尽量使用原厂的，亲测淘宝便宜的数据线会出现显示器内有小颗粒闪烁的问题，我之前还以为是显示器的问题。 mac系统没有开启hidpi模式，此时安装神器https://pan.baidu.com/share/link?shareid=31306&amp;amp;uk=1679854806&amp;amp;errno=0&amp;amp;errmsg=Auth%20Login%20Sucess&amp;amp;&amp;amp;bduss=&amp;amp;ssnerror=0&amp;amp;traceid=, 打开然后，选择1080P(HIDPI)就好了。  </description>
    </item>
    
    <item>
      <title>Slow startup Tomcat because of SecureRandom</title>
      <link>https://wenchao.ren/posts/slow-startup-tomcat-because-of-securerandom/</link>
      <pubDate>Tue, 04 Dec 2018 13:51:59 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/slow-startup-tomcat-because-of-securerandom/</guid>
      <description>今天在新机器上启动tomcat应用的时候，发现巨慢，检查日志发现有如下信息：
Jan 09, 2018 8:44:35 PM org.apache.catalina.util.SessionIdGenerator createSecureRandom INFO: Creation of SecureRandom instance for session ID generation using [SHA1PRNG] took [239,939] milliseconds.  这块初始化SecureRandom用了239,939毫秒，之前没遇到这个问题。查了一下发现在官方wiki https://wiki.apache.org/tomcat/HowTo/FasterStartUp#Entropy_Source
 Entropy Source Tomcat 7+ heavily relies on SecureRandom class to provide random values for its session ids and in other places. Depending on your JRE it can cause delays during startup if entropy source that is used to initialize SecureRandom is short of entropy. You will see warning in the logs when this happens, e.</description>
    </item>
    
    <item>
      <title>Spring RestTemplate parse gzip response</title>
      <link>https://wenchao.ren/posts/spring-resttemplate-parse-gzip-response/</link>
      <pubDate>Tue, 04 Dec 2018 13:45:26 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/spring-resttemplate-parse-gzip-response/</guid>
      <description>假设http://10.89.xx.xx:8080/_/metrics接口返回的数据格式是gzip格式 他的Response Headers信息如下
HTTP/1.1 200 OK Server: Apache-Coyote/1.1 Content-Encoding: gzip Content-Type: text/plain;charset=UTF-8 Transfer-Encoding: chunked Date: Thu, 28 Dec 2017 08:13:53 GMT  如果我们使用Spring RestTemplate想直接拿到String形式的返回，而不是byte[]格式，那么可以使用如下的方式：
import org.springframework.http.HttpStatus; import org.springframework.http.ResponseEntity; import org.springframework.http.client.HttpComponentsClientHttpRequestFactory; import org.springframework.web.client.RestTemplate; import org.apache.http.impl.client.HttpClientBuilder; public static void main(String[] args) { HttpComponentsClientHttpRequestFactory clientHttpRequestFactory = new HttpComponentsClientHttpRequestFactory( HttpClientBuilder.create().build()); RestTemplate restTemplate = new RestTemplate(clientHttpRequestFactory); ResponseEntity&amp;lt;String&amp;gt; responseEntity = restTemplate.getForEntity(&amp;quot;http://10.89.xx.xxx:8080/_/metrics&amp;quot;, String.class); HttpStatus statusCode = responseEntity.getStatusCode(); System.out.println(responseEntity.getBody()); }  </description>
    </item>
    
    <item>
      <title>Java RSA非对称加密</title>
      <link>https://wenchao.ren/posts/java-rsa%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86/</link>
      <pubDate>Tue, 04 Dec 2018 13:40:50 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/java-rsa%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86/</guid>
      <description>最近的一个项目中，agent和master双方需要远程通信，但是需要双方认证以及传输的信息加密，因此就选择了RSA这个非对称加密算法实现了netty的handler。
##实现思路 简要的描述一下实现思路：
 首先生成一对公钥和私钥 所有的master都使用这个私钥进行加密、解密 所有的agent都使用这个公钥进行加密和解密 master发给agent的信息，使用私钥加密，master收到agent的信息，使用私钥解密 agent发给master的信息，使用公钥加密，agent收到master的信息，使用公钥解密 无论是agent还是master，对收到的信息，只要解密失败，那么就丢弃  这样相当于实现了agent和master的认证，以及消息的加密传输。挺有意思的。 ##生成公钥私钥
###使用java代码生成：
private static final String RSA = &amp;quot;RSA&amp;quot;; public static KeyPair buildKeyPair() throws NoSuchAlgorithmException { final int keySize = 2048; KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance(RSA); keyPairGenerator.initialize(keySize); return keyPairGenerator.genKeyPair(); } KeyPair keyPair = buildKeyPair(); PublicKey pubKey = keyPair.getPublic(); PrivateKey privateKey = keyPair.getPrivate();  ###shell生成 我在这个项目实现中，是别生成了公钥文件和私钥文件，作为了工程的配置文件来用的，因此使用了shell的命令：
ssh-keygen -t rsa -b 2048 -C &amp;quot;any string&amp;quot; openssl pkcs8 -topk8 -inform PEM -outform DER -in id_rsa -out private_key.</description>
    </item>
    
    <item>
      <title>自己实现LRU Cache</title>
      <link>https://wenchao.ren/posts/%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0lru-cache/</link>
      <pubDate>Tue, 04 Dec 2018 13:24:56 +0000</pubDate>
      
      <guid>https://wenchao.ren/posts/%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0lru-cache/</guid>
      <description>今天闲的无事，写一个LRU的缓存练练手，不过提前说好哈，最好的办法是直接使用Guava中的方法：
import com.google.common.cache.CacheBuilder; import java.util.concurrent.ConcurrentMap; public class GuavaLRUCache { public static void main(String[] args) { ConcurrentMap&amp;lt;String, String&amp;gt; cache = CacheBuilder.newBuilder() .maximumSize(2L) .&amp;lt;String, String&amp;gt;build().asMap(); cache.put(&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;); cache.put(&amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;); System.out.println(cache); cache.put(&amp;quot;a&amp;quot;, &amp;quot;d&amp;quot;); System.out.println(cache); cache.put(&amp;quot;c&amp;quot;, &amp;quot;d&amp;quot;); System.out.println(cache); } }  如果自己实现的话，就比较挫了：
public class LRUCache&amp;lt;K, V&amp;gt; { private final int limit; private final Map&amp;lt;K, V&amp;gt; cache; private final Deque&amp;lt;K&amp;gt; deque; private final ReentrantLock reentrantLock = new ReentrantLock(); private static final int DEFAULT_CAPACITY = 16; private static final float DEFAULT_LOAD_FACTOR = 0.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://wenchao.ren/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://wenchao.ren/about/</guid>
      <description>欢迎访问这个被遗忘的站点
关于这个站点 如同站点标题描述的一样，这是一个被遗忘的站点，没有任何的推广宣传，没做任何的SEO优化，它就静悄悄的存在着，是不是还可能因为各种原因而无法被访问到。这个站点主要是一个可以记录我工作心得、阅读到比较有意思的东西、以及我自己的一些千奇百怪的想法的地方。站点的内容在书写的时候都假定只会有我自己会阅读到，所以很多的时候文章写的会比较随意，可能并没有什么铺垫，结论也可能非常的简洁。因此：
 如果其中某些文章解答了你的某些问题，你不用感谢我，毕竟我本意也不是为了解答你的问题，只是恰好而已。 如果某些地方你没有看懂，你的留言或者邮件的咨询，我不保证一定回复，毕竟这一切都是需要时间、精力等成本的，请谅解。  文章的内容不定期修改，如果收藏的话建议保持原链接。
关于我 我其实是一个非常平庸并且慢热的人，而且我的运气也比较一般。
我的职场履历  20年11月13日~至今 在美团工作 18年4月~20年11月6日 在便利蜂工作 13年7月本科毕业~18年4月 在Qunar工作 13年（大四）基本都在Qunar实习 12年（大三）暑假在腾讯公司(深圳)实习 11年（大二）暑假在创新工场的安全宝项目组实习  我的公众号 （这个公众号基本不怎么维护和使用）
联系方式  邮箱：me#wenchao.ren 微博: https://weibo.com/AnotherRobot 之前的博客(已经废弃): http://www.cnblogs.com/rollenholt/  关于版权 本站点所有内容，版权都归我所有，除非明确授权，否则禁止一切形式的转载。</description>
    </item>
    
  </channel>
</rss>
